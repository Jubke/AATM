{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifikation von Texten mit geteilter Autorenschaft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Abstract\" data-toc-modified-id=\"Abstract-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Abstract</a></span></li><li><span><a href=\"#Einführung\" data-toc-modified-id=\"Einführung-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Einführung</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problemstellung\" data-toc-modified-id=\"Problemstellung-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Problemstellung</a></span></li><li><span><a href=\"#Related-Research\" data-toc-modified-id=\"Related-Research-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Related Research</a></span></li><li><span><a href=\"#Theorie\" data-toc-modified-id=\"Theorie-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Theorie</a></span><ul class=\"toc-item\"><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Features</a></span></li></ul></li><li><span><a href=\"#Notebook-Vorbereitung\" data-toc-modified-id=\"Notebook-Vorbereitung-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Notebook Vorbereitung</a></span><ul class=\"toc-item\"><li><span><a href=\"#Laden-eigener-Module\" data-toc-modified-id=\"Laden-eigener-Module-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Laden eigener Module</a></span></li><li><span><a href=\"#Laden-von-externen-Bibliotheken\" data-toc-modified-id=\"Laden-von-externen-Bibliotheken-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Laden von externen Bibliotheken</a></span></li><li><span><a href=\"#Globale-Intialisierungen\" data-toc-modified-id=\"Globale-Intialisierungen-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Globale Intialisierungen</a></span></li></ul></li></ul></li><li><span><a href=\"#Verwendete-Datensätze\" data-toc-modified-id=\"Verwendete-Datensätze-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Verwendete Datensätze</a></span><ul class=\"toc-item\"><li><span><a href=\"#CMU-Book-Summary-Datensatz\" data-toc-modified-id=\"CMU-Book-Summary-Datensatz-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>CMU Book Summary Datensatz</a></span></li><li><span><a href=\"#Konstruktion-von-gelabelten-Daten\" data-toc-modified-id=\"Konstruktion-von-gelabelten-Daten-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Konstruktion von gelabelten Daten</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kombinatorische-Variablen\" data-toc-modified-id=\"Kombinatorische-Variablen-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Kombinatorische Variablen</a></span></li><li><span><a href=\"#Ähnlichkeitsmaß\" data-toc-modified-id=\"Ähnlichkeitsmaß-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Ähnlichkeitsmaß</a></span></li><li><span><a href=\"#Konstruktion-von-Datensätzen\" data-toc-modified-id=\"Konstruktion-von-Datensätzen-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Konstruktion von Datensätzen</a></span><ul class=\"toc-item\"><li><span><a href=\"#Book-Summary-1\" data-toc-modified-id=\"Book-Summary-1-3.2.3.1\"><span class=\"toc-item-num\">3.2.3.1&nbsp;&nbsp;</span>Book Summary 1</a></span></li><li><span><a href=\"#Book-Summary-2\" data-toc-modified-id=\"Book-Summary-2-3.2.3.2\"><span class=\"toc-item-num\">3.2.3.2&nbsp;&nbsp;</span>Book Summary 2</a></span></li></ul></li></ul></li><li><span><a href=\"#Alternativer-Datensatz-zur-Verifikation-der-verwendeten-Ansätze\" data-toc-modified-id=\"Alternativer-Datensatz-zur-Verifikation-der-verwendeten-Ansätze-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Alternativer Datensatz zur Verifikation der verwendeten Ansätze</a></span></li></ul></li><li><span><a href=\"#Text-Repräsentation\" data-toc-modified-id=\"Text-Repräsentation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Text Repräsentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Auswahl\" data-toc-modified-id=\"Feature-Auswahl-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Feature Auswahl</a></span></li><li><span><a href=\"#Feature-Berechnung\" data-toc-modified-id=\"Feature-Berechnung-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Feature Berechnung</a></span></li></ul></li><li><span><a href=\"#Identifikation-von-Texten-mit-geteilter-Autorenschaft\" data-toc-modified-id=\"Identifikation-von-Texten-mit-geteilter-Autorenschaft-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Identifikation von Texten mit geteilter Autorenschaft</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Baseline</a></span></li><li><span><a href=\"#Bi-direktionale-LSTM-Netze-basierend-auf-lexikalischen-und-syntaktischen-Merkmalen\" data-toc-modified-id=\"Bi-direktionale-LSTM-Netze-basierend-auf-lexikalischen-und-syntaktischen-Merkmalen-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Bi-direktionale LSTM Netze basierend auf lexikalischen und syntaktischen Merkmalen</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segmentierung-von-Texten\" data-toc-modified-id=\"Segmentierung-von-Texten-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Segmentierung von Texten</a></span></li><li><span><a href=\"#Aufbau-des-Modells\" data-toc-modified-id=\"Aufbau-des-Modells-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Aufbau des Modells</a></span></li><li><span><a href=\"#Training-mit-dem-Book-Summary-2-Datensatz\" data-toc-modified-id=\"Training-mit-dem-Book-Summary-2-Datensatz-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Training mit dem Book Summary 2 Datensatz</a></span></li><li><span><a href=\"#Training-mit-dem-PAN-Datensatz\" data-toc-modified-id=\"Training-mit-dem-PAN-Datensatz-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Training mit dem PAN Datensatz</a></span></li><li><span><a href=\"#Evaluation-der-trainierten-Modelle\" data-toc-modified-id=\"Evaluation-der-trainierten-Modelle-5.2.5\"><span class=\"toc-item-num\">5.2.5&nbsp;&nbsp;</span>Evaluation der trainierten Modelle</a></span></li></ul></li><li><span><a href=\"#Parallelen-LSTM-Netze-zum-Vergleich-temporal-gegensätzlicher-Stilentwicklungen\" data-toc-modified-id=\"Parallelen-LSTM-Netze-zum-Vergleich-temporal-gegensätzlicher-Stilentwicklungen-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Parallelen LSTM Netze zum Vergleich temporal gegensätzlicher Stilentwicklungen</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segmentierung-von-Texten\" data-toc-modified-id=\"Segmentierung-von-Texten-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Segmentierung von Texten</a></span></li><li><span><a href=\"#Aufbau-des-Modells\" data-toc-modified-id=\"Aufbau-des-Modells-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Aufbau des Modells</a></span></li><li><span><a href=\"#Training-auf-dem-Book-Summary-2-Datensatz\" data-toc-modified-id=\"Training-auf-dem-Book-Summary-2-Datensatz-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Training auf dem Book Summary 2 Datensatz</a></span></li><li><span><a href=\"#Training-auf-dem-PAN-Datensatz\" data-toc-modified-id=\"Training-auf-dem-PAN-Datensatz-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Training auf dem PAN Datensatz</a></span></li><li><span><a href=\"#Evaluierung-der-trainierten-Modelle\" data-toc-modified-id=\"Evaluierung-der-trainierten-Modelle-5.3.5\"><span class=\"toc-item-num\">5.3.5&nbsp;&nbsp;</span>Evaluierung der trainierten Modelle</a></span></li></ul></li></ul></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Comparison</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Literaturverzeichnis\" data-toc-modified-id=\"Literaturverzeichnis-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Literaturverzeichnis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur im vorherigen Abschnitt beschriebenen Aufgabenstellung finden man in der Literatur nur wenige Arbeiten. Einige aktuelle Arbeiten zur Style Change Detection findet man in der PAN CLEF 2018 Challenge **tbd Source**. \n",
    "\n",
    "**tbd Arbeiten beschreiben**\n",
    "\n",
    "Arbeiten aus PAN 2017 gehen noch einen Schritt weiter und beschäftigen sich mit einer „Style breach detection“, die die genauen Positionen identifizieren soll, an denen sich der Schreibstil ändert **tbd Source**. \n",
    "\n",
    "Eine große Verwandtschaft zur Style Change Detection weist auch das Themengebiet der Plagiatserkennung auf. Ziel ist es hierbei, Stellen in Texten zu finden, in denen auf Inhalte aus fremden Arbeiten zurückgegriffen wird, ohne dass diese als solche gekennzeichnet werden. Es wird dabei zwischen dem intrinsischen und dem extrinsischen Ansatz unterschieden.\n",
    "Beim Erstgenannten wird auf ein Referenzdatensatz zurückgegriffen, in welchem nach Quellen für mögliche Plagiate durchsucht wird(Clustering by AuthorshipWithin and Across Documents S. 3).\n",
    "Bei der zweiten Herangehensweise wird lediglich der zu untersuchende Text betrachtet. Im Vordergrund steht hierbei auch der persönliche Schreibstil des Autors. Es wird versucht mithilfe verschiedener Verfahren Stellen im Text zu identifizieren, in denen der Schreibstil signifikant von dem restlichen Text abweicht. (**tbd Intrinsic Plagiarism Detection with Feature-Rich Imbalanced Dataset Learning; Andrianna Polydouri(B), Georgios Siolas, and Andreas Stafylopatis; S.1**) \n",
    "Hierfür wurden zumeist \n",
    "Grundannahme für diese Verfahren ist es, dass ein Großteil des Textes von ein und demselben Autor stammt. (**tbd Methods for intrinsic plagiarism detection and author diarization.; Mikhail Kuznetsov, Anastasia Motrenko, Rita Kuznetsova, and Vadim Strijov;S.1**)\n",
    "Für diese Arbeit ist besonders die intrinsische Plagiatserkennung von Bedeutung. Arbeiten zu diesem Themengebiet fanden im Rahmen der PAN-Reihe statt, in der seit 2009 regelmäßig Aufgabenstellungen zu ausgewählten Themenbereichen veröffentlicht werden (**tbd https://pan.webis.de/**). In den Jahren 2009 bis 2011 (**tbd https://pan.webis.de/tasks.html**) gab es jeweils sowohl Aufgaben zur intrinsischen als auch zur extrinsischen Plagiatserkennung. 2016 wurde die Aufgabenstellung unter dem Begriff „Authorship diarization“ in einer abgewandelten Form wieder aufgenommen mit dem Ziel Textpassagen entsprechend ihren Autoren zu clustern (Clustering by AuthorshipWithin and Across Documents S. 3). \n",
    "\n",
    "Ein weiteres etwas entfernteres Themengebiet ist die „Authorship Attribution“, also die Zuordnung anonymer Texte zu den jeweiligen Autoren, von denen sie verfasst wurden. Auch wenn sich diese Aufgabenstellung stark von den in dieser Arbeit verfolgten Ziele unterscheidet, so ist die Herangehensweise doch sehr ähnlich. Denn im Gegensatz zu einer inhaltsbasierten Klassifikation, wird auch hier die Ähnlichkeit der Texte mittels stilistischer Merkmale berechnet (**tbd Text Classification for Authorship Attribution Using Naive Bayes Classifier with Limited Training Data, S.1**). Für die Identifikation einer geteilten Autorenschaft eines Textes kann dem zu folge auf dieselben Features zurückgegriffen werden.  Auch hierzu wurden im Rahmen der PAN-Serie mehrere Arbeiten veröffentlich (**tbd https://pan.webis.de/tasks.html**). Zusätzlich gibt es viele Arbeiten die sich insbesondere mit der Wahl der richtigen Merkmale auseinandergesetzt haben (**tbd Source Stamatatos, Stylistic Text Classification Using Functional Lexical Features, N-Gram Feature Selection for Authorship Identification oder Not All Character N-grams Are Created Equal: A Study in Authorship Attribution**)\n",
    "\n",
    "(**tbd Style Transformation**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Durchführung einer Klassifikation mittels mathematischer Modelle, müssen die Texte zunächst in numerische Tensoren überführt werden. Ein wichtiger Bestandteil dieser Vektorisierung ist die Zerlegung der Texte in Tokens. Dabei werden die Texte in einzelne Buchtstaben oder einzelne Wörter aufgeteilt. (**tbd Deep Learning mit Python und Keras S.232**). Anschließend können entweder festgelegte Merkmale berechnet und an das Modell übergeben werden oder man wandelt die Token in Vektoren um und übergibt diese direkt an das Modell. Im Folgenden werden beide Ansätze kurz vorgestellt.   \n",
    "\n",
    "**Vorgegebene Merkmale:**\n",
    "\n",
    "Für jeden Text werden dabei im Voraus festgelegte Merkmale berechnet, die die verschiedene Schreibstile der Autoren möglichst stark voneinander abgrenzen sollen. Stamatatos unterscheidet dabei zwischen lexikalischen, zeichenbasierten, syntaktischen, semantischen und anwendungsbezogenen Merkmalen (**tbd Stamatatos S.540 Abb.**). \n",
    " \n",
    "Beispiele für lexikalische Merkmale sind die durchschnittliche Wort-/ Satzlänge, die durchschnittliche Fehlerrate und der Wortschatz des Autors(**tbd Stamatatos S.540**).\n",
    "Weitere verbreitete lexikalische Merkmale sind die Häufigkeiten der auftretenden Wörter und Wortkombinationen, den sogenannten N-Grammen. Dieser Ansatz ist auch unter dem Begriff „Bag of words“ bzw. „Bag of n-grams“ bekannt. Dabei lassen sich die betrachteten Wöter in Funktions- und Inhaltswörter unterteilen. Im Rahmen einer stilistischen Repräsentation der Texte spielen Funktionswörter eine wichtige Rolle. Hierbei handelt es sich um kurze und sehr häufig vorkommende Worte, die für den Inhalt eines Satzes irrelevant sind und nur zur Verbindung der Inhaltswörter dienen. Während diese in inhaltsbasierten Textklassifikationen herausgefiltert werden, tragen sie viel zu einer stilistischen Abgrenzung verschiedener Autoren bei (**Stamatatos S.540**). \n",
    "\n",
    "Auch bei zeichenbasierten Merkmalen wird die Häufigkeit des Auftretens von einzelnen Zeichen und zeichenbasierter N-Gramme betrachtet. Das Auftreten von Buchstaben, Zahlen, Groß- und Kleinschreibweisen und Leerzeichen gibt Auskunft über den Schreibstil des Autors (Stamatatos S.541).\n",
    "Laut **tbd Quelle** eignen sich diese besonders gut zur Zuordnung der Texte zu ihren entsprechenden Autoren (**tbd Character N-grams Are Created Equal: A Study in Authorship Attribution S.1)**), wobei besonders jene zu guten Ergebnissen beitragen, die Wortanfänge/-enden oder Satzzeichen beinhalten (**tbd Not All Character N-grams Are Created Equal: A Study in Authorship Attribution S.8**). \n",
    "\n",
    "Desweiteren gibt es syntaktische und sematische Merkmale, die jedoch stark von der entsprechenden Sprache abhängen. Ein Beispiel für ein semantische Merkmale sind N-Gramme auf Basis von Part-of-Speech-Tags. \n",
    "\n",
    "Bei allen N-Grammen stellt sich für die Merkmalsauswahl die Frage nach der richtigen Wahl des „N“, welches die Anzahl der berücksichtigten Wörter/Zeichen festlegt.\n",
    "Generell gilt, je höher die Ordnung der N-Gramme, desto mehr Kontext wird betrachtet. Jedoch wird dadurch auch die Dimensionen des entstehenden Merkmalraums stark erhöht (**tbd Stamatatos S.542**). Verfahren wie Stemming oder Lemmatizing können genutzt werden, um die Dimensionen zu reduzieren.\n",
    "\n",
    "Die Betrachtung der Häufigkeiten ist relativ simpel und wird in vielen Arbeiten verwendet. Sie hat jedoch den Nachteil, dass die Tokens als Menge anstatt einer Sequenz wahrgenommen, wodurch jedoch die Reihenfolge, Grammatik und Struktur der Texte missachtet werden (**tbd Deep Learning, Text und sequenzielle Daten S.233f.**).\n",
    "\n",
    "\n",
    "\n",
    "**Gelernte Features:**\n",
    "\n",
    "Im Gegensatz zu den selbst berechneten Merkmalen, können die Token mittels One-Hot-Encoding auch direkt in Vektoren umgewandelt. Der Vorteil dieses Vorgehens liegt darin, dass im Vergleich zum \"Bag of Words\"-Ansatz die Struktur des Textes erhalten bleibt. Diese Repräsentationsform führt jedoch zu sehr hohen Dimensionen, da in der Regel innerhalb eine Korpus oftmals zehntausende verschiedener Wörter vorkommen. Verfahren der Worteinbettung ordnen dagegen jedem Token einen dichtbesetzteren Vektor zu. Ziel dieser Verfahren ist es mithilfe der Vektoren auch die semantischen Abhängigkeiten der Wörter abzubilden, sodass Wörter mit einer ähnlichen Bedeutung auch ähnlich Vektoren erhalten. \n",
    "Die bekanntesten Verfahren sind der Word2Vec- und der Gloveansatz. \n",
    "Deep-Learning-Verfahren, wie zum Bespiel LSTMs oder CNNs, können anhand dieser Vektoren Sequenzen von Wörtern oder Zeichen erlernen und mithilfe der identifizierten Muster Klassifizierungen durchführen. (Deep Learning mit Python S.238 ff.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laden eigener Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:22:50.729517Z",
     "start_time": "2019-02-04T18:22:50.726452Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import aatm_support\n",
    "from tensor_board_logger import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laden von externen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:22:50.738035Z",
     "start_time": "2019-02-04T18:22:50.731799Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, ConvLSTM2D, Activation, Dense, Dropout, Input, MaxPooling1D, concatenate, BatchNormalization, Bidirectional, Dot, Average, Lambda, Flatten\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globale Intialisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:22:50.742807Z",
     "start_time": "2019-02-04T18:22:50.739733Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "memory = Memory(location='./tmp', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwendete Datensätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammen mit der Aufgabenstellung wurde der CMU Book Summary Datensatz **[QUELLE]** vorgeschlagen. Teil der Aufgabenstellung war es aus diesem Datensatz einen für das Problem der SCD geeigneten gelabelten Datensatz zu erstellen, indem Texte des CMU Book Summary Datensatzes rekombiniert werden.\n",
    "\n",
    "Im folgenden wird der Datensatz kurz vorgestellt und begründet warum der CMU Book Summary Datensatz für die Aufgabenstellung nicht ideal ist. Nichtsdestotrotz wird in diesem Kapitel weiter vorgestellt wie gelabelte Daten für das SCD Problem konstruhiert wurden. Die zwei daraus resultierenden Datensätze, die im weiteren Verlauf verwendet wurden werden beschrieben.\n",
    "\n",
    "Motiviert durch die Vorbehalte gegenüber dem Datensatz wird außerdem ein alternativer Datensatz von der PAN Style Change Detection @ CELF 2018 **[QUELLE]** vorgestellt, der bei weiteren Arbeiten ebenfalls als Benchmark und zur methodischen Validierung herangezogen wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU Book Summary Datensatz\n",
    "\n",
    "Der 2013 von David Bamman and Noah Smith veröffentliche Datensatz **[QUELLE]** enthält 16.559 inhaltiche Zusammenfassungen von Büchern von Wikipedia sowie Metadaten über Autor, Titel, Publikationsatum und Genre des zusammengefassten Buches, sowie deren ID auf Wikipedia und Freebase (Metadaten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T13:13:53.718723Z",
     "start_time": "2019-02-02T13:13:53.464611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumente insgesamt:\t 16559\n",
      "Jedes Dokument hat 7 Einträge.\n",
      "Index(['wiki_id', 'firebase_id', 'title', 'author', 'pub_date', 'genres',\n",
      "       'plot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "bs_data = datasets.load_book_summary_raw()\n",
    "\n",
    "# Total entries\n",
    "print(\"Dokumente insgesamt:\\t\", bs_data.shape[0])\n",
    "print(\"Jedes Dokument hat %d Einträge.\" % (bs_data.shape[1]))\n",
    "print(bs_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T13:14:31.369281Z",
     "start_time": "2019-02-02T13:14:31.346998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einmalige Wikipedia IDs: 16559\n",
      "Einmalige Firebase IDs:\t 16559\n",
      "Einmalige Titles:\t 16277\n",
      "Einmalige Authors:\t 4715\n",
      "Einmalige Pub. Dates:\t 2640\n",
      "\n",
      "Einige Einträge fehlen:\n",
      "wiki_id           0\n",
      "firebase_id       0\n",
      "title             0\n",
      "author         2382\n",
      "pub_date       5610\n",
      "genres         3718\n",
      "plot              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Entries are unique books by ids...\n",
    "print(\"Einmalige Wikipedia IDs:\", bs_data.wiki_id.unique().size)\n",
    "print(\"Einmalige Firebase IDs:\\t\", bs_data.firebase_id.unique().size)\n",
    "# ...but not by title\n",
    "print(\"Einmalige Titles:\\t\", bs_data.title.unique().size)\n",
    "print(\"Einmalige Authors:\\t\", bs_data.author.unique().size)\n",
    "print(\"Einmalige Pub. Dates:\\t\", bs_data.pub_date.unique().size)\n",
    "\n",
    "print(\"\\nEinige Einträge fehlen:\")\n",
    "print(bs_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz wurde veröffentlicht im Zusammenhang mit Arbeiten zum Abgleich von Volltexten und Zusammenfassungen, sodass der Fokus auf Metadaten zu den zusammengefassten Büchern lag, nicht auf den Zusammenfassungen selbst. Es gilt hervorzuheben, dass die Informationen zur Autorenschaft sich auf das zusammengefasste Buch beziehen und nicht auf die Zusammenfassung selbst. Die im Datensatz enthaltenen Metainformationen sind im Kontext der intrinsischen Stilanalyse mit Ausnahme der Genre nicht von Bedeutung.\n",
    "\n",
    "Bei dem Versuch durch Kombination der Dokumente einen gelabelten Datensatz zu konstruieren muss aufgrund der fehlenden Metainformationen zu den Zusammenfassungen die Annahmen getroffen werden, dass jede Zusammenfassung von einem andere Autor stammt. Da die Zusammenfassungen von Wikipedia stammen impliziert dies jedoch zwei Risiken:\n",
    "\n",
    "1. Zusammenfassungen könnten vom selben Autor stammen.\n",
    "2. Zusammenfassungen könnten von mehreren Personen verfasst oder redigiert worden sein.\n",
    "\n",
    "Da es im Rahmen dieser Arbeit nicht möglich ist diese Risiken zu mitigieren (z.B. durch Ergänzen geigneter Metainformationen), wird stattdessen auf einen zusätzlichen Datensatz zurückgegriffen (siehe [Kapitel 3.3](#Alternativer-Datensatz-zur-Verifikation-der-verwendeten-Ansätze)). \n",
    "\n",
    "In Anwendungsscenarien der Authentizitätserkennung bei Texten ist es unwahrscheinlich, dass nicht authentische Textpassagen vollkommen andere Inhalte aufweisen als der restliche Text. Um einen möglichst realistischen Datensatz zu erhalten und eine Erkennung der Bruchstellen nach inhaltlichen Texteigenschaften zu vermeiden, sollen die Konstruktion gelabelter Texte deshalb nach Maßstäben ihrer inhaltlichen Kompatibilität stattfinden. Das gewälte Maß dazu wird auf den gegeben Genreinformationen basieren. Vorteilhaft ist an dieser Stelle, dass es sich um Zusammenfassungen und damit bereits um Abstrakte und in ihrer Art und Aufbau vermutlich ähnliche Texte handelt.\n",
    "\n",
    "Folgend wird also der Datensatz in Bezug auf die Genre-Informationen untersucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T12:26:38.755526Z",
     "start_time": "2019-02-02T12:26:38.750493Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_genre_book_relations(data):\n",
    "    # Subset original data\n",
    "    temp = data.loc[:, ['wiki_id', 'genres']]\n",
    "    # drop rows without genres\n",
    "    temp = temp.dropna()\n",
    "    # extract `id: genre` pairs to lists\n",
    "    temp.genres = temp.genres.str.replace('[{}\"]', '', regex=True).str.split(', ')\n",
    "    # map each genre <=> book relation to a seperate row\n",
    "    genre_tags = []\n",
    "    for key, row in temp.iterrows():\n",
    "        book_id = row[0]\n",
    "        tags = pd.Series(row[1]).str.split(': ')\n",
    "        for genre_id, genre_name in tags:\n",
    "            genre_tags += [[book_id, genre_id, genre_name]]\n",
    "\n",
    "    genre_tags = pd.DataFrame(genre_tags)\n",
    "    genre_tags.columns = ['wiki_id', 'genre_id', 'genre_name']\n",
    "    \n",
    "    return genre_tags\n",
    "    \n",
    "def extract_genres(genre_tags):\n",
    "    # Extract unique genres\n",
    "    genres = genre_tags.groupby(['genre_id', 'genre_name']).agg('count')\n",
    "    genres.columns = ['count']\n",
    "    \n",
    "    print(\"Genre Data\")\n",
    "    print(genres.describe())\n",
    "    print(\"\\nNote that there are more unique ids than names ('Mystery' genre)\")\n",
    "    \n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T12:26:44.213400Z",
     "start_time": "2019-02-02T12:26:39.526544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre Data\n",
      "             count\n",
      "count   228.000000\n",
      "mean    131.596491\n",
      "std     545.391873\n",
      "min       1.000000\n",
      "25%       2.000000\n",
      "50%       6.500000\n",
      "75%      29.000000\n",
      "max    4747.000000\n",
      "\n",
      "Note that there are more unique ids than names ('Mystery' genre)\n",
      "\n",
      "Top 10 Genres\n",
      "                                  count\n",
      "genre_id  genre_name                   \n",
      "/m/02xlf  Fiction                  4747\n",
      "/m/014dfn Speculative fiction      4314\n",
      "/m/06n90  Science Fiction          2870\n",
      "/m/05hgj  Novel                    2463\n",
      "/m/01hmnh Fantasy                  2413\n",
      "/m/0dwly  Children's literature    2122\n",
      "/m/02n4kr Mystery                  1395\n",
      "/m/03mfnf Young adult literature    825\n",
      "/m/0c3351 Suspense                  765\n",
      "/m/0lsxr  Crime Fiction             753\n"
     ]
    }
   ],
   "source": [
    "genre_tags = extract_genre_book_relations(bs_data)\n",
    "genres = extract_genres(genre_tags)\n",
    "print(\"\\nTop 10 Genres\")\n",
    "print(genres.sort_values('count', ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T12:46:31.636275Z",
     "start_time": "2019-02-02T12:46:31.523342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         genre_name\n",
      "count  12841.000000\n",
      "mean       2.336578\n",
      "std        1.401971\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        3.000000\n",
      "max       11.000000\n"
     ]
    }
   ],
   "source": [
    "# count genres per book\n",
    "n_genres_per_book = genre_tags.loc[:,['wiki_id', 'genre_name']].groupby('wiki_id').count()\n",
    "# draw histogram\n",
    "n_genres_per_book.hist()\n",
    "plt.title(\"Verteilung der Anzahl von Genres pro Buch\")\n",
    "plt.ylabel(\"Anzahl Bücher mit x Genres\")\n",
    "plt.xlabel('Anzahl Genres')\n",
    "plt.show()\n",
    "print(n_genres_per_book.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T12:49:25.439748Z",
     "start_time": "2019-02-02T12:49:25.435840Z"
    }
   },
   "source": [
    "Es gibt 228 Genres nach ID, jedoch nur 227 eindeutige Genres nach Bezeichnung. Das Genre 'Mystery' wird unter zwei IDs geführt. Bereits unter den Top 10 Genres lässt sich gut erkennen, dass es sich bei vielen Genres auch um Sub-Genres handelt, z.B. 'Fiction' und 'Crime Fiction'. Durchschnittlich ist ein Genre 132 mal vergeben. 75% der Genres sind nur 29 mal oder weniger vertreten. Das häufigste Genre mit 4747 Dokumenten ist 'Fiction'. Im Durchschnitt werden jedem Buch 2,34 Genre zugeordnet. Es haben 50% aller Bücher 2 oder weniger, 75% haben 3 oder weniger Genre zugeordnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstruktion von gelabelten Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kombinatorische Variablen\n",
    "\n",
    "Bei der Kombination von Texten gibt es mehrere Faktoren, die berücksichtigt und variiert werden können, um verschiedene Datensätze zu erstellen. Es liegt nahe, dass das Vorgehen bei der Erstellung des Datensatzes später Einfluss auf die Ergebnisse bei der Identifizierung der selben Texte hat.\n",
    "\n",
    "**Ähnlichkeit der Texte:**\n",
    "Durch Variation eines Schwellenwerts in Bezug auf die minimale Ähnlichkeit - gegeben eines geeigneten Ähnlichkeitmaßes - kann die Schwierigkeit der resultierenden Kombinationen beeinflusst werden kann. Die Bestimmung der geteilten Urheberschaft von Texten, die sich nach einem geeigneten Maß im Voraus ähnlicher sind, könnte schwieriger zu erkennen sein als bei einem kombinierten Text, bei dem Originaltexte im Vorfeld als weniger ähnlich eingestuft wurden.\n",
    "\n",
    "**Wiederverwendung von Texten:**\n",
    "Soll bei der Kombination von Texten mit oder ohne Zurücklegen kombiniert werden? Ein Text, der für die Kombination verwendet wurde, kann entweder für die Kombination mit zusätzlichen Texten wiederverwendet werden oder eben nicht. Dies kann ebenso für die Wiederverwendung als einzelner Text im resultierenden Datensatz variiert werden.\n",
    "\n",
    "**Verfahren zur Kombination:**\n",
    "Es gibt verschiedene Möglichkeiten, Texte zu kombinieren. Die triviale Variante ist zwei Texte aneinander zu hängen. Ein anderer Ansatz könnte darin bestehen, einzelne Texte in Absätze oder Sätze aufzuteilen und diese dann zu verschachteln oder zufällig zu mischen.\n",
    "\n",
    "**Anzahl der zu kombinierenden Texte:**\n",
    "Beliebig viele Texte können zu Texten gemeinsamer Autorenschaft mit unterschiedlichem Grad kombiniert werden, also bspw. 2, 3 oder mehr Autoren.\n",
    "\n",
    "**Anteil der kombinierten Texte am resultierenden Datensatz:**\n",
    "Soll ein ausgeglichener oder ein unausgeglichener Datensatz erstellt werden? Letzterer ist dabei wahrscheinlich realistischer da unter viele authentischen Dokumenten wenige unauthentische sein werden. Andererseits ist ein ausgeglichener Datensatz in den meisten Fälle besser um Algorithmen effizient und ohne Bias zu trainieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ähnlichkeitsmaß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ähnlichkeit zu kombinierender Texte wird als primärer Faktor für Schwierigkeit und Qualität des resultierenden Datensatzes betrachtet. Die Autoren des Datensatz für die PAN Style Change Detection **[QUELLE]** haben dies bspw. dadurch sichergestellt, dass Fragen und Antworten aus dem technisch Forum Stack Overflow verwendet wurden, für Metainformationen zu den enthaltenen Themen und Subthemen verfügbar waren. Analog verwenden wir bei dem Book Summary Datensatz die vergebenen Genre und Subgenre.\n",
    "\n",
    "Dazu wird für jedes Dokument ein Vektor erstellt der binär die Zugehörigkeit zu jedem Genre kodiert (\"k-hot encoding\"). Jede Dimension des Vektorraums repräsentiert also die Zugehörigkeit zu einem Genre, die entweder mit 1 (\"zugehörig\") oder 0 (\"nicht zugehörig\") belegt wird. \n",
    "\n",
    "Da vor allem Texte mit hoher Ähnlichkeit von Interesse sind, wurden bei der Kodierung der Genres ausschließlich die Genres berücksichtigt, die mindestens zwei Texten zugeordnet werden. Dadurch reduziert sich die Dimension des Vektorraums von 228 auf 183."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T14:19:47.517394Z",
     "start_time": "2019-02-02T14:19:47.510092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract k-hot-encoding of genres\n",
    "@memory.cache\n",
    "def extract_genre_vectors(data):\n",
    "    # parse genre strings\n",
    "    result = data.genres.str.replace('[{}\"]', '', regex=True) \\\n",
    "                        .str.replace('/m/.+?: ', '', regex=True) \\\n",
    "                        .str.get_dummies(', ')\n",
    "    # Prefix and normalize genre columns\n",
    "    result.columns = ['genre_' + str(col) for col in result.columns.str.lower().str.replace('[^a-z]', '_')]\n",
    "    # Select genres that have been assigned at least twice.\n",
    "    # \n",
    "    # Assuming we use genres only to match text an additional dimensions \n",
    "    # that is never shared will only make a text more 'different' from \n",
    "    # all other text. No information gain in that.\n",
    "    # \n",
    "    # This reduces genres from 227 to 183. \n",
    "    result = result.loc[:, result.sum(axis=0) >= 2]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ähnlichkeitsmaß für die berechneten Vektoren wird die Kosinus-Ähnlichkeit verwendet. Die Kosinus-Ähnlichkeit zeigt die Ähnlichkeit der Ausrichtung zweier Vektoren an, indem sie den Cosinus des Winkels zwischen diesen Vektoren misst. Die Größe der Vektoren ist nicht relevant. Die Kosinus-Ähnlichkeit ist 0 für orthogonale Vektoren, 1 für Vektoren gleicher Ausrichtung oder -1 für diametral entgegengesetzte Vektoren.\n",
    "\n",
    "$$ similarity: S_C(x,y) = cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} $$\n",
    "\n",
    "Da jeder Text mit jedem anderen verglichen wird erhalten wir eine entsprechende Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T14:26:13.609152Z",
     "start_time": "2019-02-02T14:26:13.605202Z"
    }
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def calc_cosine_sim_matrix(genres):\n",
    "    # calculate cosine similarities\n",
    "    result = cosine_similarity(genres)\n",
    "    # override lower diagonal with nan to drop duplicates and self combination\n",
    "    result[np.tril_indices(result.shape[0])] = np.nan\n",
    "    # downcast precision to reduce memory footprint\n",
    "    result = pd.DataFrame(result).apply(pd.to_numeric, downcast='float')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konstruktion von Datensätzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit den berechneten Ähnlichkeiten der Texte und unter berücksichtigung der weiteren Faktoren als Variablen wurden zwei Datensätze konstruiert. Implementiert wurde die einfache Kombination durch aneinanderhängen von zwei Texten mit und ohne Zurücklegen unter Berücksichtigung des erwünschten Anteils gemischter oder einfacher Texte im resultierenden Datensatz und eines Grenzwertes für die Ähnlichkeit der zu kombinierenden Texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T14:35:06.436235Z",
     "start_time": "2019-02-02T14:35:06.425562Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    # the dataset \n",
    "    data,\n",
    "    # the similarity matrix\n",
    "    sim_matrix,\n",
    "    # the minimum similarity between the combined texts\n",
    "    sim_threshold = 0.5,\n",
    "    # flag to reuse texts\n",
    "    reuse_texts = False,\n",
    "    # target share of combined texts in the resulting dataset\n",
    "    target_share = 0.2,\n",
    "    # name of the columns containing the text\n",
    "    text_col = 'plot',\n",
    "    # number of texts to combine, only 2 is supported\n",
    "    comb_degree = 2,\n",
    "    # Method of combination, only 'append' is implemented\n",
    "    # combination = 'append'\n",
    "):  \n",
    "    # We focus on the combination of two texts only.\n",
    "    if comb_degree > 2:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Total number of available texts\n",
    "    n_total = data.shape[0]\n",
    "    # Instantiate number of texts that will be combined\n",
    "    n_combine = 0\n",
    "    # Calculate number of combined texts\n",
    "    if (reuse_texts):\n",
    "        n_combine = (n_total / ( 1 / target_share - 1)) * comb_degree\n",
    "    else:\n",
    "        # Number of texts that will be combined\n",
    "        n_combine = n_total * comb_degree / (1/target_share + comb_degree - 1)\n",
    "    \n",
    "    # `n_combine` should be an integer value and dividable by `comb_degree` and\n",
    "    # we will interpret target_share as minimum.\n",
    "    n_combine = math.ceil(n_combine)\n",
    "    n_combine += (n_combine % comb_degree)\n",
    "    \n",
    "    # Instantiate number of resulting combined texts\n",
    "    n_combined = int(n_combine / comb_degree)\n",
    "    # Number of texts that will directly be taken into the in the final dataset without combination\n",
    "    if reuse_texts:\n",
    "        n_direct = n_total\n",
    "    else:\n",
    "        n_direct = n_total - n_combine\n",
    "    \n",
    "    # Number of resulting texts\n",
    "    n_result = n_direct + n_combined\n",
    "    actual_share = n_combined/n_result\n",
    "    \n",
    "    print('Total # of input texts: ', n_total)\n",
    "    print('# of texts to combine: ', n_combine)\n",
    "    print('# of resulting texts with shared authorship: ', n_combined)\n",
    "    print('# of resulting texts without shared authorship: ', n_direct)\n",
    "    print('Total # of resulting texts: ', n_result)\n",
    "    print('Actual share of texts with shared authorship: ', actual_share)\n",
    "    \n",
    "    # dropping nan\n",
    "    sim_df = sim_matrix.stack()\n",
    "    # drop combinations below threshold and return left viable choices\n",
    "    choices = sim_df[sim_df >= sim_threshold]\n",
    "    \n",
    "    to_combine = np.ndarray((n_combined), dtype=tuple)\n",
    "    if reuse_texts:\n",
    "        choices = choices.index.values\n",
    "        # random selection of viable indice combinations\n",
    "        to_combine = np.random.choice(choices, n_combined, replace=False)\n",
    "    else:\n",
    "        for i in range(to_combine.shape[0]):\n",
    "            # choose random tuple\n",
    "            choice = np.random.choice(choices.loc[choices.notna()].index.values)\n",
    "            to_combine[i] = choice\n",
    "            # drop all tuples including either choice\n",
    "            choices.loc[[choice[0], choice[1]]] = np.nan\n",
    "            choices.loc[:, [choice[0], choice[1]]] = np.nan\n",
    "\n",
    "    tc_values = list(sum(to_combine,()))\n",
    "    all_values = pd.Series(data.index.values)\n",
    "    if reuse_texts:\n",
    "        to_direct = all_values\n",
    "    else:\n",
    "        to_direct = all_values[~all_values.isin(tc_values)]\n",
    "    \n",
    "    # combine texts\n",
    "    combined = pd.DataFrame([[append_texts(data, ids, text_col), 1] for ids in to_combine])\n",
    "    direct = pd.DataFrame([[data[text_col][i], 0] for i in to_direct])\n",
    "    \n",
    "    result = pd.concat(\n",
    "        [combined, direct],\n",
    "        axis=0,\n",
    "        ignore_index=True, \n",
    "        copy=False\n",
    "    )\n",
    "    \n",
    "    result.columns = ['text', 'label']\n",
    "    \n",
    "    return result\n",
    "\n",
    "def append_texts(data, ids, text_col = 'plot'):\n",
    "    t = \"\"\n",
    "    for i in range(len(ids)):\n",
    "        t += data[text_col][ids[i]]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Book Summary 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der erste Datensatz _Book Summary 1_ verwendet jeden Text nur ein einziges Mal. Der Ähnlichkeitsschwellwert liegt bei 0,8 und der Datensatz ist ausgeglichen. Es gibt 5.520 Texte mit geteilter Autorenschaft und 5.519 Texte ohne geteilte Autorenschaft. Insgesamt enthält der Datensatz 11.039 Dokumente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calc_cosine_sim_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b062dbd3402c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msim_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_cosine_sim_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_genre_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m book_summary_1 = build_dataset(\n\u001b[0;32m      3\u001b[0m     \u001b[0mbs_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msim_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msim_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calc_cosine_sim_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "sim_matrix = calc_cosine_sim_matrix(extract_genre_vectors(bs_data))\n",
    "book_summary_1 = build_dataset(\n",
    "    bs_data,\n",
    "    sim_matrix,\n",
    "    sim_threshold = 0.8,\n",
    "    target_share = 0.5\n",
    ")\n",
    "\n",
    "dataset.to_csv(aatm_support.next_file('./datasets/constructed', '.csv'), encoding='utf-8', doublequote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Book Summary 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der zweite Datensatz _Book Summary 2_ legt Texte nach dem ziehen zurück, woraus ein entsprechend größerer Datensatz resultiert. Der Ähnlichkeitsschwellwert liegt bei 0,95 und der Datensatz ist ausgeglichen. Es gibt 14.794 Texte mit geteilter Autorenschaft und 14.794 Texte ohne geteilte Autorenschaft. Insgesamt enthält der Datensatz 29.588 Dokumente. Vor der Kombination wurden Außenseiter im Sinne von besonders langen und besonders kurzen Texten entfernt, wodurch der Ausgangsdatensatz auf 14.794 reduziert wurde. Alle diese Texte sind als nicht kombinierte Texte im resultierenden Datensatz enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of input texts:  14794\n",
      "# of texts to combine:  29588\n",
      "# of resulting texts with shared authorship:  14794\n",
      "# of resulting texts without shared authorship:  14794\n",
      "Total # of resulting texts:  29588\n",
      "Actual share of texts with shared authorship:  0.5\n",
      "Next available file: ./datasets/constructed_2.csv\n"
     ]
    }
   ],
   "source": [
    "# filter very short and very long texts\n",
    "plot_len = bs_data['plot'].str.len()\n",
    "bs_data_filtered = bs_data.loc[(plot_len - plot_len.mean() < 2 * plot_len.std()) & (plot_len >= 300)]\n",
    "bs_data_filtered.reset_index(inplace=True)\n",
    "\n",
    "# calculate similarity matrix\n",
    "sim_matrix = calc_cosine_sim_matrix(extract_genres(data))\n",
    "\n",
    "dataset = build_dataset(\n",
    "    data,\n",
    "    sim_matrix,\n",
    "    sim_threshold = 0.95,\n",
    "    target_share = 0.5,\n",
    "    reuse_texts = True\n",
    ")\n",
    "\n",
    "dataset.to_csv(aatm_support.next_file('./datasets/constructed', '.csv'), encoding='utf-8', doublequote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativer Datensatz zur Verifikation der verwendeten Ansätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Bedenken an der Tauglichkeit des CMU Book Summary Datensatzes für das SCD Problem (siehe [Kapitel 3.1](#CMU-Book-Summary-Datensatz)), wurde der Datensatz der PAN Style Change Detection Aufgabe als Alternativer Datensatz herangezogen. Der Datensatz wurde für ein im Rahmen der CLEF Conference **QUELLE** veranstaltetes \"evaluabtion lab\" herausgegeben. \n",
    "\n",
    "Er beinhaltet insgesamt 5.824 Dokumente, die aus Fragen und Antworten des StackExchange**QUELLE** Forum-Netzwerks zusammengestellt wurden. Der Texte enthalten zwischen 300 und 1000 Token und wurden bereinigt um Texte die von anderen Autoren editiert wurden. Bei der Erstellung wurde auf eine ausgewogene Verteilung von Autoren, Themen und Subthemen, sowie Anzahl an Autorenwechseln und der Länge der Texte geachtet. Dies gilt sowohl bei Dokumenten von mehreren als auch bei Dokumenten von einem Autor. \n",
    "\n",
    "Insgesamt stellt der Datensatz damit ein wesentlich qualitativere Grundlage zur Entwicklung von Algorithmen zur Erkennung von Texten mit mehreren Autoren da. Mit rund 5.824 Dokumenten ist er jedoch vergleichweise klein, was grade für die Verarbeitung in tiefen Neuronalen Netzen Herausforderungen mit sich bringt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Repräsentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Auswahl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für diese Arbeit werden neben verschiedenen von Hand ausgewählten Merkmalen N-grams auf Wort, \n",
    "Zeichen und POS mit jeweils variablen Längen betrachtet. Für die Umsetzung wird dabei die Open-Source-Library „NLTK“ (**tbd Quelle**) verwendet, da diese sehr modular aufgebaut und weit verbreitet ist.\n",
    "Da die Dimensionen des Merkmalraums abhängig von der Anzahl der Wörter innerhalb eines N-Grammes stark ansteigen, soll im Feature-Selektion-Schritt im Voraus über den vollständigen Korpus algorithmisch eine Auswahl getroffen, welche N-Gramme für eine Klassifikation hinsichtlich des Schreibstils relevant sind. \n",
    "\n",
    "Wie bereits in [Kapitel 2.2](#Theorie) beschrieben, werden zunächst die Textdaten in Tokens zerlegt. Bereits bei diesem Vorgang gibt es eine Vielzahl von Möglichkeiten, die zu unterschiedlichen Ergebnisse führen können und die Dimension des Merkmalraums stark beeinflussen. Um im späteren Verlauf der Arbeit etwas Spielraum zu haben, wird versucht diesen Vorgang möglichst flexibel zu gestalten. Beispielsweise kann über Parameter gesteuert werden, ob die Wörter in Originalform vorliegen oder mittels Stemming oder Lemmatizing auf den Wortstamm zurückgeführt werden sollen. Außerdem können ausgewählte Satzzeichen entfernt, Groß- und Kleinschreibung angepasst und Wörter, sowohl nach Länge als auch nach der Zugehörigkeit zu einem Korpus mit Funktionswörtern, gefiltert werden. \n",
    "\n",
    "Für die Erstellung der N-Gramme wird auf den CountVectorizer von sklearn (**tbd https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer**) zurückgegriffen, der für eine festgelegte Menge aller gewünschten N-Grams und den Tokens die N-Gramme und ihre Häufigkeiten innerhalb jedes Textes bestimmt. \n",
    "Dieser Vorgang wird für die Wort-, Zeichen- und die POS-basierten N-Gramme durchgeführt. Für die POS-Identifizierung wird auf den POS-Tagger von „NLTK“ zurückgegriffen (**tbd Quelle**). \n",
    "Innerhalb jeder dieser drei Kategorien wird für jedes unterschiedliche 'N' eine Selektion der Merkmale getroffen. \n",
    "\n",
    "Um einen Vergleich zu ermöglichen, müssen die Häufigkeiten skaliert werden. Dazu werden jeweils die Häufigkeiten der N-Gramme eines Textes ins Verhältnis der Gesamtzahl aller Token des Textes (**tbd Stylistic Text Classification Using Functional Lexical Features 2007 S.6**) gesetzt. Alternativ könnte zur Normalisierung auch der TFIDF-Ansatz verwendet werden. Dieser wird im folgenden jedoch nicht genutzt, da dieser die Gewichte der relevanten Funktionswörter auf null setzt.\n",
    "\n",
    "Die Relevanz der Features kann anschließend sowohl auf Basis der Häufigkeiten (**tbd N-Gram Feature Selection for Authorship Identification S.2**) als auch der Varianzen (**tbd https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold**) über alle Texte des Korpus hinweg bestimmt werden, wobei auch eine Kombination möglich ist.  Hohe Häufigkeiten der N-Gramme lassen auf ein Funktionswort schließen, die als äußert aussagekräftig für den Schreibstil von Autoren gelten (**tbd The Linguometric Approach for Co-authoring Author’s Style Definition S.1**). \n",
    "Eine hohe Varianz dagegen deutet auf ein besonders diskriminatives Merkmal hin. Neben dem Filtern über relativen und absoluten Grenzwerten für die Anzahl der Features, könne auch Quantile angegeben werden, sodass ein gewisser Anteil der Gesamthäufigkeiten oder der Gesamtvarianz abgedeckt ist (**tbd Quelle**).\n",
    "\n",
    "**Tbd Verwendete Parameter**\n",
    "\n",
    "**Tbd Ergebnis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Berechnung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Feature Berechnung werden die Ergebnisse der Feature Selektion aufgegriffen. Für jeden Text des Training- und Testdatensatzes werden die Häufigkeiten der ausgewählten N-Gramme bestimmt. Dazu werden dem CountVectorizer die ermittelten Merkmale als Dictionary mit den selektierten Merkmalen übergeben, sodass bei lediglich diese bei der Erzeugung der Häufigkeitsmatrix berücksichtigt werden. Zusätzlich werden auch die manuell selektierten Merkmale berechnet. Dazu gehören die die durchschnittliche Satzlänge, die durchschnittliche Anzahl an Zeichen pro Wort, die durchschnittliche Anzahl an Token pro Satz und die Größe des verwendeten Vokabulars. \n",
    "**tbd Ergebnis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifikation von Texten mit geteilter Autorenschaft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basierend auf einer reihe von Beobachtungen unter den Einreichungen der PAN 2018 SCD Aufgabe werden zwei Ansätze zur Lösung des SCD Problems verfolgt und verglichen. \n",
    "\n",
    "Die erste Beobachtung ist, dass der von Schaetti**[QUELLE]** zur PAN SCD 2018 vorgestellte Ansatz eines Character-basierten Convolutional Neural Network seiner Einschätzung nach unter anderem aufgrund des relativ kleinen Datensatzes mit nur 62% Genauigkeit den letzten Platz unter den eingereichten Lösungen belegt. Um dieses Ergebnis zu erreichen musst Schaetti bereits den Datensatz durch erneute rekobination der gegebenen Probleme auf ca. 18.000 Probleme erweitern. Die Vermuting liegt also nahe, dass ein Netz das komplett eigenständig Features lernen soll die zur Diskriminierung zweier Autorenstile geeignet sind deutlich mehr Daten benötigt.\n",
    "\n",
    "Das parallele hierarchische Attention Network von M. Hosseinia and A. Mukherjee **[QUELLE]** erreichte hingegen mit 82% die zweitbeste Leistung. Sie gaben ihrem Neuronalen Netz als Eingabe Parse Tree Feature (PTF) Embeddings. Dies entspricht der Eingabe des Textes als deren kodierte Satzstruktur und vermeidet so direkt jeglichen Einfluss des Inhalts auf das Ergebnis. Ebenso wird dem Netz nicht die Featureextraktion selbst überlassen, sondern es wird auf das Erkennen von Mustern beschränkt.\n",
    "\n",
    "Die dritte Beobachtung ist das weitere erfolgreiche Lösungen **[QUELLE],[QUELLE]** sich gemeinhin auf vorselektierte statistische syntaktische Features gestützt haben, damit jedoch ein Ensemble von Klassifkatoren trainiert haben.\n",
    "\n",
    "Im ersten Ansatz sollen weitere Möglichkeiten geprüft werden, vorab extrahierte Features als Eingabe für Neuronale Netze zu verwenden und so für die relativ kleinen Datenmengen zu kompensieren. Es werden statistischen und syntaktischen Features verwendet, um ein Neuronales Netz zu trainieren.\n",
    "\n",
    "Im zweiten Ansatz werden Word-Embbedings verwendet ein Neuronales Netz zu trainieren. **BEGRÜNDUNG**\n",
    "\n",
    "Zu Beginn werden verschiedene Standard-Algorithmen als Baseline für die Klassifizierung angewandt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Vergleich von späteren Ergebnissen werden eine SVM und eine Random Forest Implementierung von Sklearn**[QUELLE]**, sowie der LightGBM Algorithmus**[Quelle]** verwendet. Sie werden jeweils über die gesamte Menge extrahierter Features für jeden Datensatz trainiert und evaluiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T12:59:27.344348Z",
     "start_time": "2019-02-04T12:59:27.340314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trains and scores three machines for a single dataset and saves the score to a result table\n",
    "def train_and_score(name, results, X_train, X_test, y_train, y_test):\n",
    "    # SVM training and score\n",
    "    svm_clf = SVC(gamma='auto')\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    results['SVM'][name] = svm_clf.score(X_test, y_test)\n",
    "\n",
    "    # SVM training and score\n",
    "    rf_clf = RandomForestClassifier(n_estimators=300)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    results['RF'][name] = rf_clf.score(X_test, y_test)\n",
    "    \n",
    "    # SVM training and score\n",
    "    lgb_clf = lgb.LGBMClassifier(n_estimators=300)\n",
    "    lgb_clf.fit(X_train, y_train)\n",
    "    results['LightGBM'][name] = lgb_clf.score(X_test, y_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T15:54:03.755388Z",
     "start_time": "2019-02-04T15:39:14.800540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     SVM        RF  LightGBM\n",
      "Book Summary 1  0.583333  0.740217  0.776087\n",
      "Book Summary 2  0.574557  0.728268  0.813438\n",
      "PAN             0.568365  0.682976  0.695710\n"
     ]
    }
   ],
   "source": [
    "# prepare results table\n",
    "results = pd.DataFrame(\n",
    "    np.zeros((3,3)), \n",
    "    index=['Book Summary 1', 'Book Summary 2', 'PAN'], \n",
    "    columns=['SVM', 'RF', 'LightGBM']\n",
    ")\n",
    "\n",
    "# Baseline for Book Summary 1\n",
    "train_and_score('Book Summary 1', results, *train_test_split(\n",
    "    datasets.load_extracted_features('bs_1'), \n",
    "    datasets.load_book_summary_1().label,\n",
    "    test_size=0.25\n",
    "))\n",
    "\n",
    "# Baseline for Book Summary 2\n",
    "train_and_score('Book Summary 2', results, *train_test_split(\n",
    "    datasets.load_extracted_features('bs_2'), \n",
    "    datasets.load_book_summary_2().label,\n",
    "    test_size=0.25\n",
    "))\n",
    "\n",
    "# Baseline for PAN\n",
    "train_and_score('PAN', results, \n",
    "    datasets.load_extracted_features('pan_training_1'), \n",
    "    datasets.load_extracted_features('pan_validation_1'),\n",
    "    datasets.load_pan_data('training').label,\n",
    "    datasets.load_pan_data('validation').label\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-direktionale LSTM Netze basierend auf lexikalischen und syntaktischen Merkmalen  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem ersten Ansatz verwenden wir ein Netz aus vier bi-dirktionalen LSTM Schichten (eines für jede Feature Gruppe) gefolgt von zwei vollständig verknüpften Schichten zur Klassifizierung. Die Zeitschritte der LSTM Schicht sind dabei jeweils eine Sequence des Textes, sodass deren temporale Abhängigkeit eine Veränderung der Merkmale im Laufe des Textes kodieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentierung von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:01:11.640009Z",
     "start_time": "2019-02-04T16:01:11.635226Z"
    }
   },
   "source": [
    "Um zeitliche Veränderungen von Features zu erfassen werden diese jeweils über gleichlange Segmente des Textes berechnet. Die Anzahl der Segmente ist dabei immer gleich. Die Länge varriert also zwsichen Segmenten unterschiedlicher Dokumente nicht aber zwischen den Segmenten eines Dokumentes. Die auf jedem Segment berechneten Features werden durch die Länge der Segmente normiert, sodass diese über Dokumente hinweg vergleichbar sind**[VERWEIS FEATUREBERECHNUNG]**.\n",
    "\n",
    "Die Anzahl der verwendeten Segmente wurde auf vier festgesetzt. Die Wahl basiert auf der Anzahl der vorkommenden Autorenwechsel im PAN Datensatz. Sie macht aber auch Sinn für die Book Summary Datensätze, die zwar nur einen Wechsel haben, der durch Kombination unterschiedlich langer Texte prinzipiell jedoch auch im vorderen oder hinteren Teil des  Gesamttextes liegen kann. Eine höhere Zahl an Abschnitten würde zwar feinere Granularität erlauben aber auch das resultierende Netz ungleich komplexer machen. Die vier Abschnitte scheinen deshlab ein guter Kompromis. Weitere Experimente mit feingranularerer Segmentierung könnten bei der Identifizierung genauer Positionen von Stilbrüchen interessant werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:18:10.782715Z",
     "start_time": "2019-02-04T17:18:10.775750Z"
    }
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def split_to_windows(series, windows):\n",
    "    # split into characters\n",
    "    result = series.str.split()\n",
    "    # split char sequences into windows of equal length\n",
    "    result = pd.DataFrame(result.apply(np.array_split, indices_or_sections=windows).tolist())\n",
    "    # join characters per window \n",
    "    result = result.applymap(lambda s: \" \".join(s) )\n",
    "    return result\n",
    "\n",
    "@memory.cache\n",
    "def extract_features_for_windows(series, windows=4, save=True, selected_features=None):\n",
    "    # split into windows\n",
    "    result = split_to_windows(series, windows)\n",
    "    \n",
    "    # run feature extraction per window\n",
    "    columns = result.columns\n",
    "    for col in columns:\n",
    "        features = run_feature_extraction(result[col], False, selected_features=selected_features)\n",
    "        result = pd.concat([result, features], axis=1)\n",
    "        print(result.info())\n",
    "    \n",
    "    # drop text columns\n",
    "    result.drop(columns=columns, axis=1, inplace=True)\n",
    "\n",
    "    # Save the calculated features\n",
    "    if save:\n",
    "        result.to_csv(\n",
    "            path_or_buf = aatm_support.next_file('.//Features//calc_features_with_windows'),\n",
    "            sep = ',', \n",
    "            header = True,\n",
    "            index = True\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Helper to reshape the feature data loaded from file to fit one window per timestep\n",
    "def reshape_by_windows(data, windows = 4):\n",
    "    return data.values.reshape((data.shape[0], windows, int(data.shape[1] / windows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:49:29.751155Z",
     "start_time": "2019-02-04T16:49:29.735899Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_multi_input_lstm_model(\n",
    "    input_dims,\n",
    "    settings,\n",
    "    windows = 4\n",
    "):\n",
    "    # We have four inputs. One for each type of feature: \n",
    "    # Lexical Features, Word N-Grams, Character N-Grams, POS N-Grams\n",
    "    lex_input = Input(shape=[windows, input_dims[0]], name='lex_input')   # 4 lexical features\n",
    "    word_input = Input(shape=[windows, input_dims[1]], name='word_input') # most freq. word (1,2,3)-grams\n",
    "    char_input = Input(shape=[windows, input_dims[2]], name='char_input') # most freq. char (1,2,3,4)-grams\n",
    "    pos_input = Input(shape=[windows, input_dims[3]], name='pos_input')   # most freq. pos (1,2,3)-grams\n",
    "    \n",
    "    lex_norm = BatchNormalization(axis=2)(lex_input) \n",
    "    word_norm = BatchNormalization(axis=2)(word_input)\n",
    "    char_norm = BatchNormalization(axis=2)(char_input)\n",
    "    pos_norm = BatchNormalization(axis=2)(pos_input) \n",
    "    \n",
    "    # A LSTM will transform the window sequence into a single vector,\n",
    "    # containing information about the entire sequence of windows\n",
    "    lex_output = Bidirectional(LSTM(\n",
    "        settings['lex.lstm.units'],\n",
    "        dropout=settings['lex.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(settings['lex.lstm.reg_recurrent']),\n",
    "        kernel_regularizer=regularizers.l2(settings['lex.lstm.re_kernel']),\n",
    "        activation='elu'\n",
    "    ))(lex_norm)\n",
    "    word_output = Bidirectional(LSTM(\n",
    "        settings['word.lstm.units'], \n",
    "        dropout=settings['word.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(settings['word.lstm.reg_recurrent']),\n",
    "        kernel_regularizer=regularizers.l2(settings['word.lstm.re_kernel']),\n",
    "        activation='elu'\n",
    "    ))(word_norm)\n",
    "    char_output = Bidirectional(LSTM(\n",
    "        settings['char.lstm.units'], \n",
    "        dropout=settings['char.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(settings['char.lstm.reg_recurrent']),\n",
    "        kernel_regularizer=regularizers.l2(settings['char.lstm.re_kernel']),\n",
    "        activation='elu'\n",
    "    ))(char_norm)\n",
    "    pos_output = Bidirectional(LSTM(\n",
    "        settings['pos.lstm.units'],\n",
    "        dropout=settings['pos.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(settings['pos.lstm.reg_recurrent']),\n",
    "        kernel_regularizer=regularizers.l2(settings['pos.lstm.re_kernel']),\n",
    "        activation='elu'\n",
    "    ))(pos_norm)\n",
    "    \n",
    "    # Next we join the LSTM outputs into a densely connected network\n",
    "    x = concatenate([lex_output, word_output, char_output, pos_output])\n",
    "    \n",
    "    x = Dense(\n",
    "        settings['dense.1.units'],\n",
    "        activation=settings['dense.1.acti'],\n",
    "        kernel_regularizer=regularizers.l2(settings['dense.1.reg_kernel'])\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(settings['dense.1.dropout'])(x)\n",
    "    x = Dense(\n",
    "        settings['dense.2.units'],\n",
    "        activation=settings['dense.2.acti'],\n",
    "        kernel_regularizer=regularizers.l2(settings['dense.2.reg_kernel'])\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(settings['dense.2.dropout'])(x)\n",
    "    \n",
    "    # And finally we add layer for the output\n",
    "    main_output = Dense(1, activation=settings['out.acti'], name='main_output')(x)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[\n",
    "            lex_input,\n",
    "            word_input,\n",
    "            char_input,\n",
    "            pos_input\n",
    "        ], \n",
    "        outputs=[\n",
    "            main_output\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=RMSprop(lr=settings['lr.initial']),\n",
    "        loss={\n",
    "            'main_output': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'main_output': 1.\n",
    "        },\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper to split window data into correct input shape for the network\n",
    "def get_inputs_for_model(data, input_dims, windows = 4):\n",
    "    # split for inputs\n",
    "    return [\n",
    "        data[:,:,0:input_dims[0]], \n",
    "        data[:,:,input_dims[0]:sum(input_dims[0:2])], \n",
    "        data[:,:,sum(input_dims[0:2]):sum(input_dims[0:3])], \n",
    "        data[:,:,sum(input_dims[0:3]):]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training mit dem Book Summary 2 Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einige Parameter werden angepasst, die Struktur des Netzes bleibt erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:20:22.627941Z",
     "start_time": "2019-02-04T18:13:03.401112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = datasets.load_book_summary_2()\n",
    "features = datasets.load_extracted_features('with_windows_bs_2')\n",
    "# 821 cols per window\n",
    "# 0:4\n",
    "# 4:216 => 212\n",
    "# 216:589 => 373\n",
    "# 616: => 232\n",
    "input_dims = (4, 212, 373, 232)\n",
    "\n",
    "features = reshape_by_windows(features)\n",
    "\n",
    "# split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data.label, test_size=0.25)\n",
    "\n",
    "settings = {\n",
    "    'folder': aatm_support.next_file('./logs/4-lstms-merged/run', ''),\n",
    "    'lex.lstm.units': 16,\n",
    "    'lex.lstm.dropout': 0.2,\n",
    "    'lex.lstm.reg_recurrent': 0.02,\n",
    "    'lex.lstm.reg_kernel': 0.01,\n",
    "    'word.lstm.units': 64,\n",
    "    'word.lstm.dropout': 0.2,\n",
    "    'word.lstm.reg_recurrent': 0.02,\n",
    "    'word.lstm.reg_kernel': 0.01,\n",
    "    'char.lstm.units': 64,\n",
    "    'char.lstm.dropout': 0.2,\n",
    "    'char.lstm.reg_recurrent': 0.02,\n",
    "    'char.lstm.reg_kernel': 0.01,\n",
    "    'pos.lstm.units': 64,\n",
    "    'pos.lstm.dropout': 0.2,\n",
    "    'pos.lstm.reg_recurrent': 0.02,\n",
    "    'pos.lstm.reg_kernel': 0.01,\n",
    "    'dense.1.units': 128,\n",
    "    'dense.1.acti': 'elu',\n",
    "    'dense.1.reg_kernel': 0.1,\n",
    "    'dense.1.dropout': 0.3,\n",
    "    'dense.2.units': 64,\n",
    "    'dense.2.acti': 'elu',\n",
    "    'dense.2.reg_kernel': 0.1,\n",
    "    'dense.2.dropout': 0.3,\n",
    "    'out.acti': 'sigmoid',\n",
    "    'lr.initial': 0.001,\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'early_stop.monitor': 'val_acc',\n",
    "    'early_stop.min_delta': 0,\n",
    "    'early_stop.patience': 15\n",
    "}\n",
    "\n",
    "model = build_multi_input_lstm_model(input_dims, settings)\n",
    "\n",
    "model.fit(\n",
    "    get_inputs_for_model(X_train, input_dims),\n",
    "    y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=settings['batch_size'],\n",
    "    epochs=settings['epochs'], \n",
    "    validation_data=(\n",
    "        get_inputs_for_model(X_test, input_dims), \n",
    "        y_test\n",
    "    ),\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5, \n",
    "            min_lr=0.0005\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=settings['early_stop.monitor'], \n",
    "            min_delta=settings['early_stop.min_delta'],\n",
    "            patience=settings['early_stop.patience'],\n",
    "            restore_best_weights=True\n",
    "        ), \n",
    "        TensorBoardLogger(\n",
    "            log_dir=settings['folder'], \n",
    "            histogram_freq=0,\n",
    "            batch_size=settings['batch_size'], \n",
    "            write_graph=False,\n",
    "            settings_str_to_log=json.dumps(settings, ensure_ascii=False)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training mit dem PAN Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.load_pan_data('training')\n",
    "data_val = datasets.load_pan_data('validation')\n",
    "features_train = datasets.load_extracted_features('with_windows_pan_training_2')\n",
    "features_val = datasets.load_extracted_features('with_windows_pan_valiation_2')\n",
    "# 848 cols per window\n",
    "# lex: 0:4 => 4\n",
    "# word: 4:231 => 227\n",
    "# chars: 231:616 => 385\n",
    "# pos: 616: => 232\n",
    "input_dims = (4, 227, 385, 232)\n",
    "\n",
    "features_train = reshape_by_windows(features_train)\n",
    "features_val = reshape_by_windows(features_val)\n",
    "\n",
    "# split into test and training data\n",
    "X_train = features_train \n",
    "X_test = features_val\n",
    "y_train = data_train.label\n",
    "y_test = data_val.label\n",
    "\n",
    "settings = {\n",
    "    'folder': aatm_support.next_file('./logs/4-lstms-merged-pan/run', ''),\n",
    "    'lex.lstm.units': 16,\n",
    "    'lex.lstm.dropout': 0.5,\n",
    "    'lex.lstm.reg_recurrent': 0.3,\n",
    "    'lex.lstm.reg_kernel': 0.2,\n",
    "    'word.lstm.units': 64,\n",
    "    'word.lstm.dropout': 0.5,\n",
    "    'word.lstm.reg_recurrent': 0.3,\n",
    "    'word.lstm.reg_kernel': 0.2,\n",
    "    'char.lstm.units': 64,\n",
    "    'char.lstm.dropout': 0.5,\n",
    "    'char.lstm.reg_recurrent': 0.3,\n",
    "    'char.lstm.reg_kernel': 0.2,\n",
    "    'pos.lstm.units': 64,\n",
    "    'pos.lstm.dropout': 0.5,\n",
    "    'pos.lstm.reg_recurrent': 0.3,\n",
    "    'pos.lstm.reg_kernel': 0.2,\n",
    "    'dense.1.units': 128,\n",
    "    'dense.1.acti': 'elu',\n",
    "    'dense.1.reg_kernel': 0.3,\n",
    "    'dense.1.dropout': 0.5,\n",
    "    'dense.2.units': 64,\n",
    "    'dense.2.acti': 'elu',\n",
    "    'dense.2.reg_kernel': 0.3,\n",
    "    'dense.2.dropout': 0.5,\n",
    "    'out.acti': 'sigmoid',\n",
    "    'lr.initial': 0.005,\n",
    "    'epochs': 50,\n",
    "    'batch_size': 50,\n",
    "    'early_stop.monitor': 'val_acc',\n",
    "    'early_stop.min_delta': 0,\n",
    "    'early_stop.patience': 15\n",
    "}\n",
    "\n",
    "model = build_multi_input_lstm_model(input_dims, settings)\n",
    "\n",
    "model.fit(\n",
    "    get_inputs_for_model(X_train, input_dims),\n",
    "    y_train,\n",
    "    batch_size=settings['batch_size'],\n",
    "    shuffle=True,\n",
    "    epochs=settings['epochs'], \n",
    "    validation_data=(\n",
    "        get_inputs_for_model(X_test, input_dims),\n",
    "        y_test,\n",
    "    ),\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5, \n",
    "            min_lr=0.0005\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=settings['early_stop.monitor'], \n",
    "            min_delta=settings['early_stop.min_delta'],\n",
    "            patience=settings['early_stop.patience'],\n",
    "            restore_best_weights=True\n",
    "        ), \n",
    "        TensorBoardLogger(\n",
    "            log_dir=settings['folder'], \n",
    "            histogram_freq=0,\n",
    "            batch_size=settings['batch_size'], \n",
    "            write_graph=False,\n",
    "            settings_str_to_log=json.dumps(settings, ensure_ascii=False)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation der trainierten Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T19:01:34.571986Z",
     "start_time": "2019-02-04T19:01:34.566572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalulate_farlstm(model, data, features, input_dims):\n",
    "    result = model.evaluate(\n",
    "        get_inputs_for_model(reshape_by_windows(features), input_dims), \n",
    "        data.label\n",
    "    )\n",
    "\n",
    "    print('Loss: {:f}'.format(result[0]))\n",
    "    print('Accuracy: {:.2f}%'.format(result[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T19:06:46.499348Z",
     "start_time": "2019-02-04T19:04:06.339150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Summary 1\n",
      "11039/11039 [==============================] - 7s 676us/step\n",
      "Loss: 0.694690\n",
      "Accuracy: 73.84%\n",
      "Book Summary 2\n",
      "29588/29588 [==============================] - 14s 483us/step\n",
      "Loss: 0.423544\n",
      "Accuracy: 82.10%\n",
      "PAN\n",
      "1352/1352 [==============================] - 5s 4ms/step\n",
      "Loss: 1.039965\n",
      "Accuracy: 60.80%\n"
     ]
    }
   ],
   "source": [
    "print('Book Summary 1')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/4-lstms-merged/model_0.h5'),\n",
    "    datasets.load_book_summary_1(),\n",
    "    datasets.load_extracted_features('with_windows_bs_1.2'),\n",
    "    input_dims = (4, 212, 373, 232)\n",
    ")\n",
    "print('Book Summary 2')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/4-lstms-merged/model_0.h5'),\n",
    "    datasets.load_book_summary_2(),\n",
    "    datasets.load_extracted_features('with_windows_bs_2'),\n",
    "    input_dims = (4, 212, 373, 232)\n",
    ")\n",
    "print('PAN')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/4-lstms-merged-pan/model_1.h5'),\n",
    "    datasets.load_pan_data('test'),\n",
    "    datasets.load_extracted_features('with_windows_pan_test_2'),\n",
    "    input_dims = (4, 227, 385, 232)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelen LSTM Netze zum Vergleich temporal gegensätzlicher Stilentwicklungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:52:46.152369Z",
     "start_time": "2019-02-04T18:52:46.148695Z"
    }
   },
   "source": [
    "Basierend auf dem Prinzip von M. Hosseinia and A. Mukherjee **[QUELLE]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentierung von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:01:14.685338Z",
     "start_time": "2019-02-04T17:56:11.644Z"
    }
   },
   "source": [
    "Die Segmentierung entspricht der aus [Kapitel 5.2.1](#Segmentierung-von-Texten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:01:14.687508Z",
     "start_time": "2019-02-04T17:57:18.784Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFaRLSTM(\n",
    "    n_features,\n",
    "    settings,\n",
    "    windows = 4\n",
    "):\n",
    "    # We have four inputs. One for each type of feature: \n",
    "    # Lexical Features, Word N-Grams, Character N-Grams, POS N-Grams\n",
    "    i = Input(shape=[windows, n_features], name='input')\n",
    "    \n",
    "    x = BatchNormalization(axis=2)(i)\n",
    "    \n",
    "    # A LSTM will transform the window sequence into a single vector,\n",
    "    # containing information about the entire sequence of windows\n",
    "    x1 = LSTM(\n",
    "        settings['forward.lstm.units'],\n",
    "        dropout=settings['forward.lstm.dropout'],\n",
    "        recurrent_dropout=settings['forward.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(0.055),\n",
    "        kernel_regularizer=regularizers.l2(0.05),\n",
    "        return_sequences=True,\n",
    "        activation='tanh'\n",
    "    )(x)\n",
    "    \n",
    "    x2 = LSTM(\n",
    "        settings['backward.lstm.units'],\n",
    "        dropout=settings['backward.lstm.dropout'],\n",
    "        recurrent_dropout=settings['backward.lstm.dropout'],\n",
    "        recurrent_regularizer=regularizers.l2(0.055),\n",
    "        kernel_regularizer=regularizers.l2(0.05),\n",
    "        return_sequences=True,\n",
    "        activation='tanh',\n",
    "        go_backwards=True\n",
    "    )(x)\n",
    "    \n",
    "    # Next we join the LSTM outputs into a densely connected network\n",
    "    dot = Dot(2, normalize=True)([x1, x2])\n",
    "    avg = Average()([x1,x2])\n",
    "    x = concatenate([dot, avg])\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(\n",
    "        settings['dense.1.units'],\n",
    "        activation=settings['dense.1.acti'],\n",
    "        kernel_regularizer=regularizers.l2(0.1)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(settings['dense.1.dropout'])(x)\n",
    "    x = Dense(\n",
    "        settings['dense.2.units'],\n",
    "        activation=settings['dense.2.acti'],\n",
    "        kernel_regularizer=regularizers.l2(0.1)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(settings['dense.2.dropout'])(x)\n",
    "    \n",
    "    # And finally we add a layer for the output\n",
    "    o = Dense(1, activation=settings['out.acti'], name='main_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[i], outputs=[o])\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=RMSprop(lr=settings['lr.initial']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training auf dem Book Summary 2 Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('2')\n",
    "features = load_features(3, base='./Features/calc_features_with_windows')\n",
    "\n",
    "features[features==np.inf]=np.nan\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "features = reshape_by_windows(features)\n",
    "\n",
    "# split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data.label, test_size=0.2)\n",
    "\n",
    "settings = {\n",
    "    'folder': aatm_support.next_file('./logs/FaRLSTM-constructed-2/run', ''),\n",
    "    'forward.lstm.units': 32,\n",
    "    'forward.lstm.dropout': 0.2,\n",
    "    'backward.lstm.units': 32,\n",
    "    'backward.lstm.dropout': 0.2,\n",
    "    'dense.1.units': 16,\n",
    "    'dense.1.acti': 'elu',\n",
    "    'dense.1.dropout': 0.2,\n",
    "    'dense.2.units': 8,\n",
    "    'dense.2.acti': 'relu',\n",
    "    'dense.2.dropout': 0.2,\n",
    "    'out.acti': 'sigmoid',\n",
    "    'lr.initial': 0.002,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 50,\n",
    "    'early_stop.monitor': 'val_loss',\n",
    "    'early_stop.min_delta': 0,\n",
    "    'early_stop.patience': 8\n",
    "}\n",
    "\n",
    "\n",
    "# Build the model\n",
    "n_features = features.shape[2]\n",
    "model = getFaRLSTM(n_features, settings)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=settings['batch_size'],\n",
    "    shuffle=True,\n",
    "    epochs=settings['epochs'], \n",
    "    validation_data=(\n",
    "        X_test,\n",
    "        y_test,\n",
    "    ),\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2, \n",
    "            min_lr=0.0005\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=settings['early_stop.monitor'], \n",
    "            min_delta=settings['early_stop.min_delta'],\n",
    "            patience=settings['early_stop.patience'],\n",
    "            restore_best_weights=True\n",
    "        ), \n",
    "        TensorBoardLogger(\n",
    "            log_dir=settings['folder'], \n",
    "            histogram_freq=0,\n",
    "            batch_size=settings['batch_size'], \n",
    "            write_graph=False,\n",
    "            settings_str_to_log=json.dumps(settings, ensure_ascii=False)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training auf dem PAN Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_pan_data('training')\n",
    "data_val = load_pan_data('validation')\n",
    "features_train = load_features(2, base='./Features/calc_features_with_windows')\n",
    "features_val = load_features(4, base='./Features/calc_features_with_windows')\n",
    "\n",
    "features_train[features_train==np.inf]=np.nan\n",
    "features_train.fillna(0, inplace=True)\n",
    "features_val[features_val==np.inf]=np.nan\n",
    "features_val.fillna(0, inplace=True)\n",
    "\n",
    "features_train = reshape_by_windows(features_train)\n",
    "features_val = reshape_by_windows(features_val)\n",
    "\n",
    "# split into test and training data\n",
    "X_train = np.concatenate((features_train, features_val))\n",
    "#X_test = features_val\n",
    "y_train = np.concatenate((data_train.label,data_val.label))\n",
    "#y_test = data_val.label\n",
    "\n",
    "settings = {\n",
    "    'folds': 5,\n",
    "    'folder': aatm_support.next_file('./logs/FaRLSTM/run', ''),\n",
    "    'forward.lstm.units': 32,\n",
    "    'forward.lstm.dropout': 0.4,\n",
    "    'backward.lstm.units': 32,\n",
    "    'backward.lstm.dropout': 0.4,\n",
    "    'dense.1.units': 16,\n",
    "    'dense.1.acti': 'elu',\n",
    "    'dense.1.dropout': 0.3,\n",
    "    'dense.2.units': 8,\n",
    "    'dense.2.acti': 'relu',\n",
    "    'dense.2.dropout': 0.3,\n",
    "    'out.acti': 'sigmoid',\n",
    "    'lr.initial': 0.001,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 20,\n",
    "    'early_stop.monitor': 'val_loss',\n",
    "    'early_stop.min_delta': 0,\n",
    "    'early_stop.patience': 8\n",
    "}\n",
    "\n",
    "\n",
    "# Build the model\n",
    "n_features = features_train.shape[2]\n",
    "model = getFaRLSTM(n_features, settings)\n",
    "\n",
    "folds = list(StratifiedKFold(n_splits=settings['folds'], shuffle=True).split(X_train, y_train))\n",
    "\n",
    "# train for each fold\n",
    "for j, (train_idx, test_idx) in enumerate(folds):\n",
    "    print('\\nFold ', j)\n",
    "    X_train_cv = X_train[train_idx]\n",
    "    y_train_cv = y_train[train_idx]\n",
    "    X_test_cv = X_train[test_idx]\n",
    "    y_test_cv = y_train[test_idx]\n",
    "\n",
    "    model.fit(\n",
    "        X_train_cv,\n",
    "        y_train_cv,\n",
    "        batch_size=settings['batch_size'],\n",
    "        shuffle=True,\n",
    "        epochs=settings['epochs'], \n",
    "        validation_data=(\n",
    "            X_test_cv,\n",
    "            y_test_cv,\n",
    "        ),\n",
    "        callbacks=[\n",
    "            # ReduceLROnPlateau(\n",
    "            #     monitor='val_loss',\n",
    "            #     factor=0.5,\n",
    "            #     patience=2, \n",
    "            #     min_lr=0.0005\n",
    "            # ),\n",
    "            EarlyStopping(\n",
    "                monitor=settings['early_stop.monitor'], \n",
    "                min_delta=settings['early_stop.min_delta'],\n",
    "                patience=settings['early_stop.patience'],\n",
    "                restore_best_weights=(j + 1 == settings['folds'])\n",
    "            ), \n",
    "            TensorBoardLogger(\n",
    "                log_dir=settings['folder'], \n",
    "                histogram_freq=0,\n",
    "                batch_size=settings['batch_size'], \n",
    "                write_graph=False,\n",
    "                settings_str_to_log=json.dumps(settings, ensure_ascii=False)\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:01:14.690067Z",
     "start_time": "2019-02-04T17:58:23.520Z"
    }
   },
   "source": [
    "#### Evaluierung der trainierten Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:45:33.427392Z",
     "start_time": "2019-02-04T18:45:33.421828Z"
    }
   },
   "outputs": [],
   "source": [
    "def evalulate_farlstm(model, data, features):\n",
    "    result = model.evaluate(\n",
    "        reshape_by_windows(features), \n",
    "        data.label\n",
    "    )\n",
    "\n",
    "    print('Loss: {:f}'.format(result[0]))\n",
    "    print('Accuracy: {:.2f}%'.format(result[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T18:56:39.955678Z",
     "start_time": "2019-02-04T18:54:27.693267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Summary 1\n",
      "11039/11039 [==============================] - 6s 502us/step\n",
      "Loss: 0.599818\n",
      "Accuracy: 75.52%\n",
      "Book Summary 2\n",
      "29588/29588 [==============================] - 9s 311us/step\n",
      "Loss: 0.510331\n",
      "Accuracy: 80.82%\n",
      "PAN\n",
      "1352/1352 [==============================] - 4s 3ms/step\n",
      "Loss: 1.132577\n",
      "Accuracy: 65.16%\n"
     ]
    }
   ],
   "source": [
    "print('Book Summary 1')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/FaRLSTM-constructed-2/model_0.h5'),\n",
    "    datasets.load_book_summary_1(),\n",
    "    datasets.load_extracted_features('with_windows_bs_1.2')\n",
    ")\n",
    "print('Book Summary 2')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/FaRLSTM-constructed-2/model_0.h5'),\n",
    "    datasets.load_book_summary_2(),\n",
    "    datasets.load_extracted_features('with_windows_bs_2')\n",
    ")\n",
    "print('PAN')\n",
    "evalulate_farlstm(\n",
    "    load_model('./models/FaRLSTM/model_4.h5'),\n",
    "    datasets.load_pan_data('test'),\n",
    "    datasets.load_extracted_features('with_windows_pan_test_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T09:32:50.704074Z",
     "start_time": "2019-02-02T09:32:50.567883Z"
    }
   },
   "source": [
    "## Literaturverzeichnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "562px",
    "left": "1567px",
    "right": "4px",
    "top": "114px",
    "width": "349px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
