{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifikation von Texten mit geteilter Autorenschaft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dependencies</a></span></li><li><span><a href=\"#Data-Exploration:-booksummaries\" data-toc-modified-id=\"Data-Exploration:-booksummaries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Exploration: booksummaries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracting-Genres\" data-toc-modified-id=\"Extracting-Genres-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracting Genres</a></span></li><li><span><a href=\"#Wieviele-Autoren-haben-in-wievielen-unterschiedlichen-Genres-publiziert?\" data-toc-modified-id=\"Wieviele-Autoren-haben-in-wievielen-unterschiedlichen-Genres-publiziert?-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Wieviele Autoren haben in wievielen unterschiedlichen Genres publiziert?</a></span></li><li><span><a href=\"#Wieviele-Autoren-haben-wieviele-unterschiedliche-Bücher-publiziert?\" data-toc-modified-id=\"Wieviele-Autoren-haben-wieviele-unterschiedliche-Bücher-publiziert?-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Wieviele Autoren haben wieviele unterschiedliche Bücher publiziert?</a></span></li><li><span><a href=\"#Doppelbelegung-von-Titeln\" data-toc-modified-id=\"Doppelbelegung-von-Titeln-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Doppelbelegung von Titeln</a></span></li></ul></li><li><span><a href=\"#Mixing-Plots\" data-toc-modified-id=\"Mixing-Plots-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Mixing Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-Binary-Genre-Vectors-per-book\" data-toc-modified-id=\"Generate-Binary-Genre-Vectors-per-book-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Generate Binary Genre Vectors per book</a></span></li><li><span><a href=\"#Similarity-Measures\" data-toc-modified-id=\"Similarity-Measures-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Similarity Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Cosine-Similarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Similarity-Matrix\" data-toc-modified-id=\"Similarity-Matrix-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>Similarity Matrix</a></span></li></ul></li></ul></li><li><span><a href=\"#Construction-of-artificial-texts-with-shared-authorship\" data-toc-modified-id=\"Construction-of-artificial-texts-with-shared-authorship-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Construction of artificial texts with shared authorship</a></span><ul class=\"toc-item\"><li><span><a href=\"#Combinatoric-considerations\" data-toc-modified-id=\"Combinatoric-considerations-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Combinatoric considerations</a></span></li><li><span><a href=\"#Building-a-dataset\" data-toc-modified-id=\"Building-a-dataset-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Building a dataset</a></span></li><li><span><a href=\"#The-simple-case\" data-toc-modified-id=\"The-simple-case-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>The simple case</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Configuration\" data-toc-modified-id=\"Configuration-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Configuration</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Feature Selection</a></span></li></ul></li><li><span><a href=\"#Feature-Extraction\" data-toc-modified-id=\"Feature-Extraction-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-features-on-the-entire-text-as-a-single-sequence\" data-toc-modified-id=\"Calculating-features-on-the-entire-text-as-a-single-sequence-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Calculating features on the entire text as a single sequence</a></span></li><li><span><a href=\"#Calculating-features-on-sequences-of-each-text\" data-toc-modified-id=\"Calculating-features-on-sequences-of-each-text-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Calculating features on sequences of each text</a></span></li></ul></li><li><span><a href=\"#Training-Iterations\" data-toc-modified-id=\"Training-Iterations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training Iterations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Baseline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Data\" data-toc-modified-id=\"Loading-Data-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Loading Data</a></span></li><li><span><a href=\"#Splitting-into-training-and-test-data\" data-toc-modified-id=\"Splitting-into-training-and-test-data-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Splitting into training and test data</a></span></li><li><span><a href=\"#SVM-baseline-classifier\" data-toc-modified-id=\"SVM-baseline-classifier-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>SVM baseline classifier</a></span></li><li><span><a href=\"#Random-Forest-baseline-classifier\" data-toc-modified-id=\"Random-Forest-baseline-classifier-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>Random Forest baseline classifier</a></span></li></ul></li><li><span><a href=\"#Added-Window-approach\" data-toc-modified-id=\"Added-Window-approach-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Added Window approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-features-for-windows\" data-toc-modified-id=\"Extract-features-for-windows-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Extract features for windows</a></span></li><li><span><a href=\"#Run-Baseline-classifiers-with-window-approach\" data-toc-modified-id=\"Run-Baseline-classifiers-with-window-approach-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Run Baseline classifiers with window approach</a></span></li><li><span><a href=\"#Run-Baseline-classifiers-with-windows-with-basic-features-only\" data-toc-modified-id=\"Run-Baseline-classifiers-with-windows-with-basic-features-only-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Run Baseline classifiers with windows with basic features only</a></span></li><li><span><a href=\"#Using-windows-in-an-LSTM-Neural-Network\" data-toc-modified-id=\"Using-windows-in-an-LSTM-Neural-Network-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Using windows in an LSTM Neural Network</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:48:18.313674Z",
     "start_time": "2019-01-25T13:48:18.309216Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import datetime\n",
    "import FEATURE_SELECTOR_v4\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import aatm_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration: booksummaries\n",
    "\n",
    "Basic data exploration of the booksummaries dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.066339Z",
     "start_time": "2019-01-25T13:22:49.063716Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "memory = Memory(location='./tmp', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.073174Z",
     "start_time": "2019-01-25T13:22:49.068383Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    return pd.read_csv(\n",
    "        './datasets/booksummaries/booksummaries.txt', \n",
    "        header=None,\n",
    "        sep='\\t',\n",
    "        names=['wiki_id', 'firebase_id', 'title', 'author', 'pub_date', 'genres', 'plot'],\n",
    "        dtype={\n",
    "            'wiki_id': 'uint32',\n",
    "            'author': 'category'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T10:38:28.665729Z",
     "start_time": "2018-12-10T10:38:27.961759Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = load_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:09.158075Z",
     "start_time": "2018-11-10T17:30:09.154422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16559, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entries\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:10.351625Z",
     "start_time": "2018-11-10T17:30:10.338886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Wikipedia IDs:\t 16559\n",
      "Unique Firebase IDs:\t 16559\n",
      "Unique Titles:\t\t 16277\n",
      "Unique Authors:\t\t 4715\n",
      "Unique Pub. Dates:\t 2640\n"
     ]
    }
   ],
   "source": [
    "# Entries are unique books by ids...\n",
    "print(\"Unique Wikipedia IDs:\\t\", raw_data.wiki_id.unique().size)\n",
    "print(\"Unique Firebase IDs:\\t\", raw_data.firebase_id.unique().size)\n",
    "# ...but not by title\n",
    "print(\"Unique Titles:\\t\\t\", raw_data.title.unique().size)\n",
    "print(\"Unique Authors:\\t\\t\", raw_data.author.unique().size)\n",
    "print(\"Unique Pub. Dates:\\t\", raw_data.pub_date.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:13.022434Z",
     "start_time": "2018-11-10T17:30:13.008452Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wiki_id           0\n",
       "firebase_id       0\n",
       "title             0\n",
       "author         2382\n",
       "pub_date       5610\n",
       "genres         3718\n",
       "plot              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have some missing values\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T08:45:36.165532Z",
     "start_time": "2018-12-07T08:45:34.967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset original data\n",
    "temp = data.loc[:, ['wiki_id', 'genres']]\n",
    "# drop rows without genres\n",
    "temp = temp.dropna()\n",
    "# extract `id: genre` pairs to lists\n",
    "temp.genres = temp.genres.str.replace('[{}\"]', '', regex=True).str.split(', ')\n",
    "# map each genre <=> book relation to a seperate row\n",
    "genre_tags = []\n",
    "for key, row in temp.iterrows():\n",
    "    book_id = row[0]\n",
    "    tags = pd.Series(row[1]).str.split(': ')\n",
    "    for genre_id, genre_name in tags:\n",
    "        genre_tags += [[book_id, genre_id, genre_name]]\n",
    "\n",
    "\n",
    "genre_tags = pd.DataFrame(genre_tags)\n",
    "genre_tags.columns = ['wiki_id', 'genre_id', 'genre_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T08:45:36.168265Z",
     "start_time": "2018-12-07T08:45:34.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract unique genres\n",
    "genres = genre_tags.groupby(['genre_id', 'genre_name']).agg('count')\n",
    "genres.columns = ['count']\n",
    "genres.describe() # Note that there are more unique ids than names (\"Mystery\" genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T18:05:59.641621Z",
     "start_time": "2018-11-10T18:05:59.630953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/m/02xlf</th>\n",
       "      <th>Fiction</th>\n",
       "      <td>4747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/014dfn</th>\n",
       "      <th>Speculative fiction</th>\n",
       "      <td>4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/06n90</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <td>2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/05hgj</th>\n",
       "      <th>Novel</th>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/01hmnh</th>\n",
       "      <th>Fantasy</th>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/0dwly</th>\n",
       "      <th>Children's literature</th>\n",
       "      <td>2122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/02n4kr</th>\n",
       "      <th>Mystery</th>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/03mfnf</th>\n",
       "      <th>Young adult literature</th>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/0c3351</th>\n",
       "      <th>Suspense</th>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/m/0lsxr</th>\n",
       "      <th>Crime Fiction</th>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  count\n",
       "genre_id  genre_name                   \n",
       "/m/02xlf  Fiction                  4747\n",
       "/m/014dfn Speculative fiction      4314\n",
       "/m/06n90  Science Fiction          2870\n",
       "/m/05hgj  Novel                    2463\n",
       "/m/01hmnh Fantasy                  2413\n",
       "/m/0dwly  Children's literature    2122\n",
       "/m/02n4kr Mystery                  1395\n",
       "/m/03mfnf Young adult literature    825\n",
       "/m/0c3351 Suspense                  765\n",
       "/m/0lsxr  Crime Fiction             753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 Genres\n",
    "genres.sort_values('count', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:39.771474Z",
     "start_time": "2018-11-10T17:30:39.457859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7effcb9ebc18>]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEo5JREFUeJzt3X+wX3V95/HnSyKK0RYQvbUJ9bJr1opLK0wKbJ1p70gXozgNuwOdOJYGh212Z7Brd9JpY/9htpUZnKnFOq22WYGkritStAMFt24WvdvZ2YKCuCKkDEFZCETRDUTiD9pL3/vH94T9Em64v773e7z383zM3Pme8zmfc877k9y5r+/58T3fVBWSpPa8qO8CJEn9MAAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASANSbKm7xqkcTEAtGIkOSvJ3UmeSvIXST6V5P3dsnck+UqSJ5P8ryQ/M7TeQ0l+K8lXkxzq1ntpt2wqyf4kv5Pkm8B1c23vBep7of2clOSWJN9O8kQ3vX5o3ekk7+/2dTjJXyV5ZZJPJPluki8lmRzq/9NJ9iQ5mOT+JL8yon9mNcQA0IqQ5HjgL4FdwMnAJ4F/1S07C7gW+LfAK4E/A25O8pKhTfwKsAk4DfgZ4NKhZT/RbfO1wLZ5bu9YjrWfFzEIl9cCPwX8APjjo9bdAlwCrAP+KfC33TonA3uBK7rxrgX2AP8FeDXwTuAjSd44j/qkZxkAWinOBdYAH66qf6iqzwBf7Jb9OvBnVXVHVT1TVbuBp7t1jvhwVT1WVQeBvwLeNLTsH4ErqurpqvrBPLd3LLPup6r+b1V9uqq+X1VPAVcCv3jUutdV1YNVdQj4r8CDVfXfq2oG+AvgzK7fO4CHquq6qpqpqi8DnwYumkd90rM836mV4ieBR+u5Ty98pHt9LbA1yW8MLTu+W+eIbw5Nf/+oZd+uqh8Ozc9ne8cy636SvAy4msHRwUnd8lckOa6qnunmvzW07g9mmX/5UH3nJHlyaPka4OPzqE96lgGgleIAsC5JhkLgVOBBBkFwZVVduchtH/1I3KVubzbbgdcD51TVN5O8CbgbyCK29QjwP6rqX46wPjXIU0BaKf4WeAZ4T5I1STYDZ3fL/hPw75Kck4G1SS5I8opF7mvU2wN4BYN38U8mOZnufP4i3QL8sySXJHlx9/NzSd6whG2qQQaAVoSq+nvgXwOXAU8Cv8rgD+HTVXUng/P2fww8AezjuRd5F7qvkW6v8yHgBOA7wO3AXy+hvqeA8xlcNH6MwWmnDwDzuUgtPSt+IYxWqiR3AH9aVdf1XYu0EnkEoBUjyS8m+YnuFNBWBrdZLvqdtNQ6LwJrJXk9cAODu2EeBC6qqgPj2nmSnwLuO8bi06vq4XHVIo2Cp4AkqVGeApKkRv1InwI65ZRTanJysu8yFux73/sea9eu7buMsXLMbWhtzCt1vHfdddd3qupVc/X7kQ6AyclJ7rzzzr7LWLDp6Wmmpqb6LmOsHHMbWhvzSh1vkv8zn36eApKkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEb9SH8SeKkmd9zay353bVp5Hx2X1B6PACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaNe8ASHJckruT3NLNn5bkjiQPJPlUkuO79pd08/u65ZND23hf135/kreOejCSpPlbyBHAe4G9Q/MfAK6uqg3AE8BlXftlwBNV9Trg6q4fSU4HtgBvBDYBH0ly3NLKlyQt1rwCIMl64ALgY918gLcAN3ZddgMXdtObu3m65ed1/TcD11fV01X1DWAfcPYoBiFJWrg18+z3IeC3gVd0868EnqyqmW5+P7Cum14HPAJQVTNJDnX91wG3D21zeJ1nJdkGbAOYmJhgenp6vmN5nu1nzMzdaRkcPnx4SXWvRI65Da2NebWPd84ASPIO4PGquivJ1JHmWbrWHMteaJ3/31C1E9gJsHHjxpqamjq6y7xduuPWRa+7FLs2rWUpda9E09PTjrkBrY15tY93PkcAbwZ+OcnbgZcCP8bgiODEJGu6o4D1wGNd//3AqcD+JGuAHwcODrUfMbyOJGnM5rwGUFXvq6r1VTXJ4CLu56vqXcAXgIu6bluBm7rpm7t5uuWfr6rq2rd0dwmdBmwAvjiykUiSFmS+1wBm8zvA9UneD9wNXNO1XwN8PMk+Bu/8twBU1b1JbgDuA2aAy6vqmSXsX5K0BAsKgKqaBqa76a8zy108VfVD4OJjrH8lcOVCi5QkjZ6fBJakRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRS/lCGB3DPY8e6uX7iB+66oKx71PSyuURgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGjVnACR5aZIvJvnfSe5N8h+79tOS3JHkgSSfSnJ81/6Sbn5ft3xyaFvv69rvT/LW5RqUJGlu8zkCeBp4S1X9LPAmYFOSc4EPAFdX1QbgCeCyrv9lwBNV9Trg6q4fSU4HtgBvBDYBH0ly3CgHI0mavzkDoAYOd7Mv7n4KeAtwY9e+G7iwm97czdMtPy9Juvbrq+rpqvoGsA84eySjkCQt2Jr5dOreqd8FvA74E+BB4Mmqmum67AfWddPrgEcAqmomySHglV377UObHV5neF/bgG0AExMTTE9PL2xEQ7afMTN3p2UwcUI/+17Kv9VSHT58uNf998Exr36rfbzzCoCqegZ4U5ITgb8E3jBbt+41x1h2rPaj97UT2AmwcePGmpqamk+Js7p0x62LXncptp8xwwfvmdc/7Ug99K6pse/ziOnpaZbyf7USOebVb7WPd0F3AVXVk8A0cC5wYpIjf+XWA4910/uBUwG65T8OHBxun2UdSdKYzecuoFd17/xJcgLwS8Be4AvARV23rcBN3fTN3Tzd8s9XVXXtW7q7hE4DNgBfHNVAJEkLM5/zFK8BdnfXAV4E3FBVtyS5D7g+yfuBu4Fruv7XAB9Pso/BO/8tAFV1b5IbgPuAGeDy7tSSJKkHcwZAVX0VOHOW9q8zy108VfVD4OJjbOtK4MqFlylJGjU/CSxJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNWpN3wVodCZ33NrbvndtWtvbviUtjkcAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUqDkDIMmpSb6QZG+Se5O8t2s/OcmeJA90ryd17Uny4ST7knw1yVlD29ra9X8gydblG5YkaS7zOQKYAbZX1RuAc4HLk5wO7ABuq6oNwG3dPMDbgA3dzzbgozAIDOAK4BzgbOCKI6EhSRq/OQOgqg5U1Ze76aeAvcA6YDOwu+u2G7iwm94M/HkN3A6cmOQ1wFuBPVV1sKqeAPYAm0Y6GknSvC3oYXBJJoEzgTuAiao6AIOQSPLqrts64JGh1fZ3bcdqP3of2xgcOTAxMcH09PRCSnyO7WfMLHrdpZg4ob999+Xw4cNL+r9aiRzz6rfaxzvvAEjycuDTwG9W1XeTHLPrLG31Au3PbajaCewE2LhxY01NTc23xOe5tKenY24/Y4YP3tPWg1Z3bVrLUv6vVqLp6WnHvMqt9vHO6y6gJC9m8Mf/E1X1ma75W92pHbrXx7v2/cCpQ6uvBx57gXZJUg/mcxdQgGuAvVX1h0OLbgaO3MmzFbhpqP3XuruBzgUOdaeKPgecn+Sk7uLv+V2bJKkH8zlP8WbgEuCeJF/p2n4XuAq4IcllwMPAxd2yzwJvB/YB3wfeDVBVB5P8PvClrt/vVdXBkYxCkrRgcwZAVf1PZj9/D3DeLP0LuPwY27oWuHYhBUqSloefBJakRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqPm/FJ4aT7uefQQl+64dez7feiqC8a+T2m18AhAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRs0ZAEmuTfJ4kq8NtZ2cZE+SB7rXk7r2JPlwkn1JvprkrKF1tnb9H0iydXmGI0mar/kcAewCNh3VtgO4rao2ALd18wBvAzZ0P9uAj8IgMIArgHOAs4ErjoSGJKkfcwZAVf0NcPCo5s3A7m56N3DhUPuf18DtwIlJXgO8FdhTVQer6glgD88PFUnSGC32KyEnquoAQFUdSPLqrn0d8MhQv/1d27HanyfJNgZHD0xMTDA9Pb3IEmH7GTOLXncpJk7ob9996WvMS/n9WKrDhw/3uv8+tDbm1T7eUX8ncGZpqxdof35j1U5gJ8DGjRtrampq0cX08R21MPhD+MF72vq65b7G/NC7psa+zyOmp6dZyu/nStTamFf7eBd7F9C3ulM7dK+Pd+37gVOH+q0HHnuBdklSTxYbADcDR+7k2QrcNNT+a93dQOcCh7pTRZ8Dzk9yUnfx9/yuTZLUkzmP2ZN8EpgCTkmyn8HdPFcBNyS5DHgYuLjr/lng7cA+4PvAuwGq6mCS3we+1PX7vao6+sKyJGmM5gyAqnrnMRadN0vfAi4/xnauBa5dUHWSpGXjJ4ElqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNaqtZxZr1Zns6ZHfALs2re1t39IoeAQgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjfIbwaRFuufRQ1zawzeSPXTVBWPfp1YnjwAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSo7wNVFphJnu49fSIXZvW9rZvjZ5HAJLUqLEHQJJNSe5Psi/JjnHvX5I0MNYASHIc8CfA24DTgXcmOX2cNUiSBsZ9DeBsYF9VfR0gyfXAZuC+MdchaRH6evxFX1b7NY9U1fh2llwEbKqqf9PNXwKcU1XvGeqzDdjWzb4euH9sBY7OKcB3+i5izBxzG1ob80od72ur6lVzdRr3EUBmaXtOAlXVTmDneMpZHknurKqNfdcxTo65Da2NebWPd9wXgfcDpw7NrwceG3MNkiTGHwBfAjYkOS3J8cAW4OYx1yBJYsyngKpqJsl7gM8BxwHXVtW946xhTFb0KaxFcsxtaG3Mq3q8Y70ILEn60eEngSWpUQaAJDXKABihJKcm+UKSvUnuTfLevmsahyTHJbk7yS191zIOSU5McmOSv+v+r/9F3zUttyT/ofud/lqSTyZ5ad81jVqSa5M8nuRrQ20nJ9mT5IHu9aQ+axw1A2C0ZoDtVfUG4Fzg8kYedfFeYG/fRYzRHwF/XVU/Dfwsq3zsSdYB/x7YWFX/nMENHFv6rWpZ7AI2HdW2A7itqjYAt3Xzq4YBMEJVdaCqvtxNP8XgD8O6fqtaXknWAxcAH+u7lnFI8mPALwDXAFTV31fVk/1WNRZrgBOSrAFexir8/E5V/Q1w8KjmzcDubno3cOFYi1pmBsAySTIJnAnc0W8ly+5DwG8D/9h3IWPyT4BvA9d1p70+lmRVPzCmqh4F/gB4GDgAHKqq/9ZvVWMzUVUHYPAGD3h1z/WMlAGwDJK8HPg08JtV9d2+61kuSd4BPF5Vd/VdyxitAc4CPlpVZwLfY5WdFjhad957M3Aa8JPA2iS/2m9VGgUDYMSSvJjBH/9PVNVn+q5nmb0Z+OUkDwHXA29J8p/7LWnZ7Qf2V9WRI7sbGQTCavZLwDeq6ttV9Q/AZ4Cf77mmcflWktcAdK+P91zPSBkAI5QkDM4N762qP+y7nuVWVe+rqvVVNcngouDnq2pVvzOsqm8CjyR5fdd0Hqv/ceYPA+cmeVn3O34eq/zC95Cbga3d9Fbgph5rGTm/E3i03gxcAtyT5Ctd2+9W1Wd7rEmj9xvAJ7rnWX0deHfP9SyrqrojyY3Alxnc6XY3q/ARCUk+CUwBpyTZD1wBXAXckOQyBkF4cX8Vjp6PgpCkRnkKSJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRv0//2hCBS9IThgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of genres per book\n",
    "genre_tags.loc[:,['wiki_id', 'genre_name']].groupby('wiki_id').count().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.078221Z",
     "start_time": "2019-01-25T13:22:49.075557Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def genres_for_books(wiki_ids):\n",
    "    return genre_tags.loc[genre_tags['wiki_id'].isin(wiki_ids)]\n",
    "\n",
    "# Look up multiple books\n",
    "# genres_for_books(data.wiki_id.iloc[0:3])\n",
    "# To look up single book wrap in an array\n",
    "# genres_for_books([data.wiki_id[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.083443Z",
     "start_time": "2019-01-25T13:22:49.080592Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def genres_for_author(author):\n",
    "    books_for_author = data.loc[data['author'] == author].loc[:, 'wiki_id']\n",
    "    return genres_for_books(books_for_author)\n",
    "\n",
    "# Look up genres for single author\n",
    "# genres_for_author('George Orwell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wieviele Autoren haben in wievielen unterschiedlichen Genres publiziert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:50.716330Z",
     "start_time": "2018-11-10T17:30:46.012049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4714.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.441451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.489543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1\n",
       "count  4714.000000\n",
       "mean      2.441451\n",
       "std       2.489543\n",
       "min       0.000000\n",
       "25%       1.000000\n",
       "50%       2.000000\n",
       "75%       3.000000\n",
       "max      22.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = data.groupby('author')\n",
    "\n",
    "author_genres = []\n",
    "for name, group in authors:\n",
    "    author_genres += [\n",
    "        [name, genres_for_books(group.wiki_id).genre_name.unique().size]\n",
    "    ]\n",
    "\n",
    "author_genres = pd.DataFrame(author_genres)\n",
    "author_genres.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:30:50.893146Z",
     "start_time": "2018-11-10T17:30:50.760854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7effcb9fa828>]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "author_genres.hist( bins=22 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wieviele Autoren haben wieviele unterschiedliche Bücher publiziert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:31:04.932409Z",
     "start_time": "2018-11-10T17:31:04.724303Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_titles_per_author=authors.title.nunique()\n",
    "plt=num_titles_per_author.hist(bins=num_titles_per_author.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:31:05.702545Z",
     "start_time": "2018-11-10T17:31:05.696255Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4714.000000\n",
      "mean        3.006364\n",
      "std         5.098350\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         3.000000\n",
      "max        74.000000\n",
      "Name: title, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(num_titles_per_author.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Doppelbelegung von Titeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:31:09.121318Z",
     "start_time": "2018-11-10T17:31:08.731932Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der doppelten Titeln: (246, 7)\n"
     ]
    }
   ],
   "source": [
    "titles=data.groupby(\"title\")\n",
    "print('Anzahl der doppelten Titeln:',titles.nunique().loc[titles.nunique().wiki_id > 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T17:31:10.189600Z",
     "start_time": "2018-11-10T17:31:09.811324Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_id</th>\n",
       "      <th>firebase_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>genres</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Casa</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A Taste for Death</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abduction</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Beauty</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   wiki_id  firebase_id  title  author  pub_date  genres  plot\n",
       "title                                                                         \n",
       "1945                     2            2      1       2         2       2     2\n",
       "A Casa                   2            2      1       2         0       2     2\n",
       "A Taste for Death        2            2      1       2         1       2     2\n",
       "Abduction                2            2      1       2         2       2     2\n",
       "American Beauty          2            2      1       2         2       1     2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.nunique().loc[titles.nunique().wiki_id > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f24982aaf28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles.nunique().wiki_id.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Binary Genre Vectors per book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.090229Z",
     "start_time": "2019-01-25T13:22:49.085715Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Extract k-hot-encoding of genres\n",
    "@memory.cache\n",
    "def extract_genres(data):\n",
    "    result = data.genres.str.replace('[{}\"]', '', regex=True) \\\n",
    "                         .str.replace('/m/.+?: ', '', regex=True) \\\n",
    "                         .str.get_dummies(', ')\n",
    "    # Prefix and normalize genre columns\n",
    "    result.columns = ['genre_' + str(col) for col in result.columns.str.lower().str.replace('[^a-z]', '_')]\n",
    "    # Select genres that have been assigned at least twice.\n",
    "    # \n",
    "    # Assuming we use genres only to match text an additional dimensions \n",
    "    # that is never shared will only make a text more 'different' from \n",
    "    # all other text. No information gain in that.\n",
    "    # \n",
    "    # This reduces genres from 227 to 183. \n",
    "    result = result.loc[:, result.sum(axis=0) >= 2]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine-Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity indicates similarity of orientation of two vectors by measuring the cosine of the angle between those vectors. Magnitude of the vectors is not of relevance. The cosine similarity is 0 for orthogonal vectors, 1 for vectors of the same orientation or -1  for diametrically opposed vectors.\n",
    "\n",
    "$$ similarity: S_C(x,y) = cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} $$\n",
    "\n",
    "As we are using the cosine similarity to compare attribute vectors in strictly positive space we can use the cosine distance, which is the complement of the cosine similarity in positive space. A distance of 0 indicates same orientation, a distance of 1 indicates no similarity or decorrelation and values in between indicate similarity or dissimilarity.\n",
    "\n",
    "$$ D_C(x,y) = 1 - S_C(x,y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate pairwise cosine similar for our books binary genre vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.095908Z",
     "start_time": "2019-01-25T13:22:49.092167Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def calc_cosine_sim_matrix(genres):\n",
    "    result = cosine_similarity(genres)\n",
    "    result[np.tril_indices(result.shape[0])] = np.nan\n",
    "    result = pd.DataFrame(result).apply(pd.to_numeric, downcast='float')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.101447Z",
     "start_time": "2019-01-25T13:22:49.098254Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def calc_cosine_dist_matrix(genres):\n",
    "    return (calc_cosine_sim_matrix(genres) * -1.0) + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T10:39:22.224454Z",
     "start_time": "2018-12-10T10:39:18.024828Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_matrix = calc_cosine_sim_matrix(extract_genres(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of artificial texts with shared authorship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no labeled dataset of texts with shared and non-shared authorship (or at least non we know of), we have to construct a labeled dataset by artificially combining texts, then treating them as text of shared authorship and mix them with texts that have not been altered and are therefore considered to be of a non-shared authorship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinatoric considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While combining texts for mixed authorship there are several factors that can be considered and varied to create different datasets that might effect the results later.\n",
    "\n",
    "__Degree of similarity__  \n",
    "By varying a threshold of similarity we assume that the difficulty of resulting combinations can be effected. Determining shared authorship of texts that are more alike by any given measure upfront, might be more difficult to detect than with a combined text of which original texts were considered less alike upfront.\n",
    "\n",
    "__Reuse of texts__  \n",
    "A text that was used for combination could be or not be reused for combination with additional texts or it could also be not be used as a text of non-shared authorship in the same dataset.\n",
    "\n",
    "__Method of combination__  \n",
    "There are different ways to combine texts. The simplest being to append on text to another. Another approach could be to split individual texts into paragraphs or sentences and interleave or randomly mix those tokens.\n",
    "  \n",
    "__Number of texts being combined__  \n",
    "Any number of texts could be combined to create texts of shared authorship with different degrees, e.g. 1 to n authors.\n",
    "  \n",
    "__Share of combined texts in the resulting dataset__  \n",
    "Depending on used features and techniques the percentage of shared texts vs texts of single authorship might effect the algorithms performance for new unrelated datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.113259Z",
     "start_time": "2019-01-25T13:22:49.103619Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    # The dataset \n",
    "    data,\n",
    "    sim_matrix,\n",
    "    sim_threshold = 0.5,\n",
    "    reuse_texts = False,\n",
    "    combination = 'append',\n",
    "    comb_degree = 2,\n",
    "    target_share = 0.2,\n",
    "    text_col = 'plot'\n",
    "):  \n",
    "    if comb_degree > 2:\n",
    "        raise NotImplementedError\n",
    "    # Total number of available texts\n",
    "    n_total = data.shape[0]\n",
    "    # Instantiate number of texts that will be combined\n",
    "    n_combine = 0\n",
    "    # Calculate number of combined texts\n",
    "    if (reuse_texts):\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        # Number of texts that will be combined\n",
    "        n_combine = n_total * comb_degree / (1/target_share + comb_degree - 1)\n",
    "    \n",
    "    # `n_combine` should be an integer value and dividable by `comb_degree` and\n",
    "    # we will interpret target_share as minimum.\n",
    "    n_combine = math.ceil(n_combine)\n",
    "    n_combine += (n_combine % comb_degree)\n",
    "    \n",
    "    \n",
    "    # Instantiate number of resulting combined texts\n",
    "    n_combined = int(n_combine / comb_degree)\n",
    "    # Number of texts that will directly be taken into the in the final dataset without combination\n",
    "    n_direct = n_total - n_combine\n",
    "    \n",
    "    # Number of resulting texts\n",
    "    n_result = n_direct + n_combined\n",
    "    actual_share = n_combined/n_result\n",
    "    \n",
    "    print('Total # of input texts: ', n_total)\n",
    "    print('# of texts to combine: ', n_combine)\n",
    "    print('# of resulting texts with shared authorship: ', n_combined)\n",
    "    print('# of resulting texts without shared authorship: ', n_direct)\n",
    "    print('Total # of resulting texts: ', n_result)\n",
    "    print('Actual share of texts with shared authorship: ', actual_share)\n",
    "    \n",
    "    # override lower diagonal with nan to drop duplicates and self combination.\n",
    "    # SHOULD BE DONE BEFORE PASSED TO THIS METHOD\n",
    "    # sim_matrix[np.tril_indices(sim_matrix.shape[0])] = np.nan\n",
    "    # dropping nan\n",
    "    sim_df = sim_matrix.stack()\n",
    "    # drop combinations below threshold and return left viable choices\n",
    "    choices = sim_df[sim_df >= sim_threshold]\n",
    "    \n",
    "    to_combine = np.ndarray((n_combined), dtype=tuple)\n",
    "    if reuse_texts:\n",
    "        choices = choices.index.values\n",
    "        to_combine = np.random.choice(choices, n_combined, replace=False)\n",
    "    else:\n",
    "        for i in range(to_combine.shape[0]):\n",
    "            # choose random tuple\n",
    "            choice = np.random.choice(choices.loc[choices.notna()].index.values)\n",
    "            to_combine[i] = choice\n",
    "            # drop all tuples including either choice\n",
    "            choices.loc[[choice[0], choice[1]]] = np.nan\n",
    "            choices.loc[:, [choice[0], choice[1]]] = np.nan\n",
    "\n",
    "    tc_values = list(sum(to_combine,()))\n",
    "    all_values = pd.Series(data.index.values)\n",
    "    to_direct = all_values[~all_values.isin(tc_values)]\n",
    "    \n",
    "    # combine texts\n",
    "    combined = pd.DataFrame([[append_texts(data, ids, text_col), 1] for ids in to_combine])\n",
    "    direct = pd.DataFrame([[data[text_col][i], 0] for i in to_direct])\n",
    "    \n",
    "    result = pd.concat(\n",
    "        [combined, direct],\n",
    "        axis=0,\n",
    "        ignore_index=True, \n",
    "        copy=False\n",
    "    )\n",
    "    \n",
    "    result.columns = ['text', 'label']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.117928Z",
     "start_time": "2019-01-25T13:22:49.115051Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def append_texts(data, ids, text_col = 'plot'):\n",
    "    t = \"\"\n",
    "    for i in range(len(ids)):\n",
    "        t += data[text_col][ids[i]]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The simple case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with we will go with a simple case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T13:31:08.130875Z",
     "start_time": "2018-12-10T10:40:33.643796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of input texts:  16559\n",
      "# of texts to combine:  11040\n",
      "# of resulting texts with shared authorship:  5520\n",
      "# of resulting texts without shared authorship:  5519\n",
      "Total # of resulting texts:  11039\n",
      "Actual share of texts with shared authorship:  0.5000452939577861\n"
     ]
    }
   ],
   "source": [
    "# sample_size = 100 # data.shape[0]\n",
    "# dataset = build_dataset(data.iloc[0:sample_size], sim_matrix.iloc[0:sample_size, 0:sample_size])\n",
    "dataset = build_dataset(\n",
    "    data,\n",
    "    sim_matrix,\n",
    "    sim_threshold = 0.8,\n",
    "    target_share = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T13:32:52.380104Z",
     "start_time": "2018-12-10T13:32:51.409970Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('./datasets/constructed_2.csv', encoding='utf-8', doublequote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.126168Z",
     "start_time": "2019-01-25T13:22:49.119820Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def set_filter_params(\n",
    "    var_word_n_grams  = (True, 'absolute', [5000, 500, 500]),\n",
    "    var_char_n_grams  = (True, 'absolute', [500, 500, 500, 500]),\n",
    "    var_pos_n_grams   = (True, 'absolute', [500, 500, 500]),\n",
    "    freq_word_n_grams = (True, 'absolute', [2000, 2000, 2000]),\n",
    "    freq_char_n_grams = (True, 'absolute', [2000, 2000, 2000, 2000]),\n",
    "    freq_pos_n_grams  = (True, 'absolute', [2000, 2000, 2000])):\n",
    "\n",
    "    return {\n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # word_n_grams])\n",
    "        'var_word_n_grams': var_word_n_grams,      \n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # char_n_grams])\n",
    "        'var_char_n_grams': var_char_n_grams,   \n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # pos_n_grams])\n",
    "        'var_pos_n_grams': var_pos_n_grams,       \n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in word_n_grams])\n",
    "        'freq_word_n_grams': freq_word_n_grams,\n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in char_n_grams])\n",
    "        'freq_char_n_grams': freq_char_n_grams,           \n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in pos_n_grams])\n",
    "        'freq_pos_n_grams': freq_pos_n_grams\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.133467Z",
     "start_time": "2019-01-25T13:22:49.128211Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def set_features_to_calc(char_n_grams= [1,2,3,4],        \n",
    "    word_n_grams = [1,2,3],                                          \n",
    "    pos_n_grams=[1,2,3],                      \n",
    "    avg_sent_len=True,                         \n",
    "    avg_word_len=True,                         \n",
    "    token_per_sent=True,                      \n",
    "    vocabulary_richness=True):\n",
    "\n",
    "    return{\n",
    "        # list with n for char_n_grams\n",
    "        'char_n_grams': char_n_grams,        \n",
    "        # list with n for word_n_grams\n",
    "        'word_n_grams': word_n_grams,                                          \n",
    "        # list with n for pos_n_grams\n",
    "        'pos_n_grams': pos_n_grams,                      \n",
    "        # flag if average sentence length should be calculated\n",
    "        'avg_sent_len': avg_sent_len,                         \n",
    "        # flag if average token length should be calculated\n",
    "        'avg_word_len': avg_word_len,                         \n",
    "        # flag if token per sentences should be calculated\n",
    "        'token_per_sent':token_per_sent,                      \n",
    "        # flag if vocabulary_richness should be calculated\n",
    "        'vocabulary_richness': vocabulary_richness             \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.150197Z",
     "start_time": "2019-01-25T13:22:49.135472Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def set_token_params_1 (\n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+'),      \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle'), \n",
    "    get_tokens = 'sentence',                                                    \n",
    "    word_strain = 'stem',\n",
    "    filter_length = ('long', 0),                                                \n",
    "    handle_stopwords = '',                                                      \n",
    "    get_sentences = False,                                                      \n",
    "    punctuation = \"[,;.!—]\",                                                   \n",
    "    lower_stop_words = set(stopwords.words('english')),                         \n",
    "    uncapitalized = True):    \n",
    "    \n",
    "    return {\n",
    "        # Word tokenizer\n",
    "        'word_tokenizer': word_tokenizer,\n",
    "        # Sentence tokenizer\n",
    "        'sent_tokenizer': sent_tokenizer,\n",
    "        # Group tokens: 'text'|'sentence'\n",
    "        'get_tokens': get_tokens,\n",
    "        # 'lemma'|'stem'\n",
    "        'word_strain': word_strain,\n",
    "        # Get tokens with length >= <int> or <= <int>: ('long',<int>)|('short',<int>)\n",
    "        'filter_length': filter_length,\n",
    "        # 'get'|'remove'\n",
    "        'handle_stopwords': handle_stopwords, \n",
    "        # Sentences tokens\n",
    "        'get_sentences': get_sentences,\n",
    "        # Remove punctuation\n",
    "        'punctuation': punctuation,\n",
    "        # Stopwords\n",
    "        'lower_stop_words': lower_stop_words,\n",
    "        # Original or uncapitalized stemms/lemmas\n",
    "        'uncapitalized': uncapitalized \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.159567Z",
     "start_time": "2019-01-25T13:22:49.151900Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def run_feature_selection(\n",
    "        serie_texts,\n",
    "        features_to_calc = set_features_to_calc(), \n",
    "        token_params_1 = set_token_params_1(),\n",
    "        filter_params = set_filter_params(\n",
    "            var_word_n_grams=(True, 'threshold', [0.001, 0.001, 0.001]),\n",
    "            var_char_n_grams=(True, 'threshold', [0.001, 0.001, 0.001, 0.001]),\n",
    "            var_pos_n_grams=(True, 'threshold', [0.01, 0.01, 0.01]),\n",
    "            freq_word_n_grams=(False),\n",
    "            freq_char_n_grams=(False),\n",
    "            freq_pos_n_grams=(False)\n",
    "        ),\n",
    "        cv_min_df = 0.1\n",
    "    ):\n",
    "\n",
    "    features = FEATURE_SELECTOR_v4.select_features(\n",
    "        serie_texts,                          \n",
    "        features_to_calc, \n",
    "        token_params_1,              \n",
    "        filter_params,\n",
    "        path_to_features = aatm_support.last_file('.//Features//selected_features'),\n",
    "        flag_extract_features = False,\n",
    "        cv_min_df = cv_min_df,         \n",
    "        normalization_type = ''\n",
    "    )\n",
    "    \n",
    "    with open(aatm_support.next_file('.//Features//selected_features'), 'w') as f:\n",
    "        json.dump(selected_features, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features on the entire text as a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.168079Z",
     "start_time": "2019-01-25T13:22:49.161488Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def run_feature_extraction(series, save=True):\n",
    "    extracted_features = FEATURE_SELECTOR_v4.select_features(\n",
    "        serie_texts=series,                          \n",
    "        features_to_calc = set_features_to_calc(), \n",
    "        token_params_1 = set_token_params_1(),\n",
    "        filter_params= set_filter_params(\n",
    "            (False,'',[]),(False,'',[]),(False,'',[]),(False,'',[]),(False,'',[]),(False,'',[])\n",
    "        ),\n",
    "        path_to_features= aatm_support.last_file('.//Features//selected_features'),    \n",
    "        flag_extract_features= True,\n",
    "        cv_min_df = 0,                           \n",
    "        normalization_type= ''\n",
    "    )\n",
    "    \n",
    "    print(extracted_features.info())\n",
    "        \n",
    "    # Save the calculated features\n",
    "    if save:\n",
    "        extracted_features.to_csv(\n",
    "            path_or_buf = aatm_support.next_file('.//Features//calc_features'),\n",
    "            sep = ',', \n",
    "            header = True,\n",
    "            index = True\n",
    "        )\n",
    "    \n",
    "    return extracted_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features on sequences of each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:22:49.175948Z",
     "start_time": "2019-01-25T13:22:49.170010Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def split_to_windows(series, windows):\n",
    "    # split into characters\n",
    "    result = series.str.split()\n",
    "    # split char sequences into windows of equal length\n",
    "    result = pd.DataFrame(result.apply(np.array_split, indices_or_sections=windows).tolist())\n",
    "    # join characters per window \n",
    "    result = result.applymap(lambda s: \" \".join(s) )\n",
    "    return result\n",
    "\n",
    "@memory.cache\n",
    "def extract_features_for_windows(series, windows=4, save=True):\n",
    "    # split into windows\n",
    "    result = split_to_windows(series, windows)\n",
    "    \n",
    "    # run feature extraction per window\n",
    "    columns = result.columns\n",
    "    for col in columns:\n",
    "        features = run_feature_extraction(result[col], False)\n",
    "        result = pd.concat([result, features], axis=1)\n",
    "        print(result.info())\n",
    "    \n",
    "    # drop text columns\n",
    "    result.drop(columns=columns, axis=1, inplace=True)\n",
    "\n",
    "    # Save the calculated features\n",
    "    if save:\n",
    "        result.to_csv(\n",
    "            path_or_buf = aatm_support.next_file('.//Features//calc_features_with_windows'),\n",
    "            sep = ',', \n",
    "            header = True,\n",
    "            index = True\n",
    "        )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T15:28:33.507890Z",
     "start_time": "2018-12-22T15:28:33.505447Z"
    }
   },
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T15:33:42.159178Z",
     "start_time": "2018-12-22T15:33:41.802934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to data\n",
    "extracted_features = pd.read_csv(\n",
    "    './/Features/constructed_1.csv',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T13:59:02.692500Z",
     "start_time": "2018-12-10T13:59:02.671431Z"
    }
   },
   "outputs": [],
   "source": [
    "# REVIEW: we shouldn't get NaN values in the first place\n",
    "# Fill NaN values in features\n",
    "extracted_features.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T13:59:06.712590Z",
     "start_time": "2018-12-10T13:59:06.645246Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(extracted_features, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T13:24:52.608502Z",
     "start_time": "2018-12-09T13:24:52.543283Z"
    }
   },
   "source": [
    "#### SVM baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:00:40.329476Z",
     "start_time": "2018-12-10T13:59:52.034888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC(gamma='auto')\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:03:34.738496Z",
     "start_time": "2018-12-10T14:03:14.697255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5907219324732363\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Random Forest baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:04:21.728051Z",
     "start_time": "2018-12-10T14:03:40.986814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=300)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:04:22.244653Z",
     "start_time": "2018-12-10T14:04:22.003372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7587153444962943\n"
     ]
    }
   ],
   "source": [
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added Window approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features for windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:27:50.941456Z",
     "start_time": "2019-01-21T17:26:50.425660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11039 entries, 0 to 11038\n",
      "Columns: 1003 entries, 1 to TO VB NN\n",
      "dtypes: float64(1000), object(3)\n",
      "memory usage: 84.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11039 entries, 0 to 11038\n",
      "Columns: 2002 entries, 2 to TO VB NN\n",
      "dtypes: float64(2000), object(2)\n",
      "memory usage: 168.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11039 entries, 0 to 11038\n",
      "Columns: 3001 entries, 3 to TO VB NN\n",
      "dtypes: float64(3000), object(1)\n",
      "memory usage: 252.7+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11039 entries, 0 to 11038\n",
      "Columns: 4000 entries, avg_sent_len to TO VB NN\n",
      "dtypes: float64(4000)\n",
      "memory usage: 336.9 MB\n",
      "None\n",
      "Next available file: .//Features//calc_features_with_windows_0.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>token_per_sent</th>\n",
       "      <th>vocabulary_richness</th>\n",
       "      <th>a</th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>...</th>\n",
       "      <th>NN CC NN</th>\n",
       "      <th>NN DT NN</th>\n",
       "      <th>NN IN DT</th>\n",
       "      <th>NN IN JJ</th>\n",
       "      <th>NN IN NN</th>\n",
       "      <th>NN MD VB</th>\n",
       "      <th>NN NN IN</th>\n",
       "      <th>NN NN NN</th>\n",
       "      <th>NN TO VB</th>\n",
       "      <th>TO VB NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.833333</td>\n",
       "      <td>5.793269</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>0.629808</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133.250000</td>\n",
       "      <td>5.700535</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145.428571</td>\n",
       "      <td>5.502703</td>\n",
       "      <td>26.428571</td>\n",
       "      <td>0.524324</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130.888889</td>\n",
       "      <td>5.690821</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.714286</td>\n",
       "      <td>5.934247</td>\n",
       "      <td>26.071429</td>\n",
       "      <td>0.528767</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132.111111</td>\n",
       "      <td>5.661905</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71.500000</td>\n",
       "      <td>4.820225</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162.500000</td>\n",
       "      <td>6.310680</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>0.766990</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>151.200000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88.727273</td>\n",
       "      <td>6.216561</td>\n",
       "      <td>14.272727</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>247.950000</td>\n",
       "      <td>6.229899</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>0.476131</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>161.714286</td>\n",
       "      <td>5.989418</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>119.000000</td>\n",
       "      <td>5.727273</td>\n",
       "      <td>20.777778</td>\n",
       "      <td>0.537433</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>159.666667</td>\n",
       "      <td>5.702381</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116.833333</td>\n",
       "      <td>5.965957</td>\n",
       "      <td>19.583333</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>93.875000</td>\n",
       "      <td>5.913386</td>\n",
       "      <td>15.875000</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>95.708333</td>\n",
       "      <td>5.713930</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>0.487562</td>\n",
       "      <td>0.061947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100.833333</td>\n",
       "      <td>5.084034</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>5.513514</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>115.250000</td>\n",
       "      <td>5.910256</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>94.333333</td>\n",
       "      <td>5.660000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>6.818182</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>98.600000</td>\n",
       "      <td>5.699422</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>0.653179</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>143.714286</td>\n",
       "      <td>6.134146</td>\n",
       "      <td>23.428571</td>\n",
       "      <td>0.689024</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>147.523810</td>\n",
       "      <td>5.823308</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.044983</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>122.185185</td>\n",
       "      <td>5.717504</td>\n",
       "      <td>21.370370</td>\n",
       "      <td>0.461005</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>110.500000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>0.594872</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>125.533333</td>\n",
       "      <td>5.958861</td>\n",
       "      <td>21.066667</td>\n",
       "      <td>0.547468</td>\n",
       "      <td>0.050955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>146.761905</td>\n",
       "      <td>5.904215</td>\n",
       "      <td>24.857143</td>\n",
       "      <td>0.467433</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>125.400000</td>\n",
       "      <td>5.859813</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11009</th>\n",
       "      <td>252.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11010</th>\n",
       "      <td>116.571429</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>20.571429</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11011</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>6.042254</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>133.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11013</th>\n",
       "      <td>127.576923</td>\n",
       "      <td>5.944444</td>\n",
       "      <td>21.461538</td>\n",
       "      <td>0.478495</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.014493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014</th>\n",
       "      <td>125.800000</td>\n",
       "      <td>5.285714</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11015</th>\n",
       "      <td>168.000000</td>\n",
       "      <td>6.588235</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>46.333333</td>\n",
       "      <td>5.791667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>171.428571</td>\n",
       "      <td>6.091371</td>\n",
       "      <td>28.142857</td>\n",
       "      <td>0.538071</td>\n",
       "      <td>0.073684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11018</th>\n",
       "      <td>107.357143</td>\n",
       "      <td>6.036145</td>\n",
       "      <td>17.785714</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11019</th>\n",
       "      <td>140.750000</td>\n",
       "      <td>6.186813</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11021</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11022</th>\n",
       "      <td>180.500000</td>\n",
       "      <td>5.822581</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11023</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11024</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>5.052632</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11025</th>\n",
       "      <td>81.333333</td>\n",
       "      <td>5.674419</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11026</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>6.055556</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11027</th>\n",
       "      <td>116.125000</td>\n",
       "      <td>5.546269</td>\n",
       "      <td>20.937500</td>\n",
       "      <td>0.549254</td>\n",
       "      <td>0.062147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11028</th>\n",
       "      <td>115.428571</td>\n",
       "      <td>5.771429</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11029</th>\n",
       "      <td>129.000000</td>\n",
       "      <td>6.142857</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>5.937500</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>91.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11032</th>\n",
       "      <td>54.500000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11033</th>\n",
       "      <td>113.857143</td>\n",
       "      <td>6.178295</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11034</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>6.941176</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11035</th>\n",
       "      <td>112.000000</td>\n",
       "      <td>5.894737</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>99.666667</td>\n",
       "      <td>5.537037</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11037</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11038</th>\n",
       "      <td>125.500000</td>\n",
       "      <td>6.197531</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11039 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_sent_len  avg_word_len  token_per_sent  vocabulary_richness  \\\n",
       "0        200.833333      5.793269       34.666667             0.629808   \n",
       "1        133.250000      5.700535       23.375000             0.620321   \n",
       "2        145.428571      5.502703       26.428571             0.524324   \n",
       "3        130.888889      5.690821       23.000000             0.565217   \n",
       "4        154.714286      5.934247       26.071429             0.528767   \n",
       "5        132.111111      5.661905       23.333333             0.647619   \n",
       "6         71.500000      4.820225       14.833333             0.606742   \n",
       "7        162.500000      6.310680       25.750000             0.766990   \n",
       "8        151.200000      5.600000       27.000000             0.540741   \n",
       "9         88.727273      6.216561       14.272727             0.738854   \n",
       "10       247.950000      6.229899       39.800000             0.476131   \n",
       "11       161.714286      5.989418       27.000000             0.693122   \n",
       "12       119.000000      5.727273       20.777778             0.537433   \n",
       "13       159.666667      5.702381       28.000000             0.773810   \n",
       "14       116.833333      5.965957       19.583333             0.553191   \n",
       "15        93.875000      5.913386       15.875000             0.653543   \n",
       "16        95.708333      5.713930       16.750000             0.487562   \n",
       "17       100.833333      5.084034       19.833333             0.613445   \n",
       "18        68.000000      5.513514       12.333333             0.810811   \n",
       "19       115.250000      5.910256       19.500000             0.634615   \n",
       "20        94.333333      5.660000       16.666667             0.880000   \n",
       "21        75.000000      6.818182       11.000000             1.000000   \n",
       "22        98.600000      5.699422       17.300000             0.653179   \n",
       "23       143.714286      6.134146       23.428571             0.689024   \n",
       "24       147.523810      5.823308       25.333333             0.511278   \n",
       "25       122.185185      5.717504       21.370370             0.461005   \n",
       "26       110.500000      5.666667       19.500000             0.594872   \n",
       "27       125.533333      5.958861       21.066667             0.547468   \n",
       "28       146.761905      5.904215       24.857143             0.467433   \n",
       "29       125.400000      5.859813       21.400000             0.644860   \n",
       "...             ...           ...             ...                  ...   \n",
       "11009    252.000000      6.000000       42.000000             0.809524   \n",
       "11010    116.571429      5.666667       20.571429             0.645833   \n",
       "11011    143.000000      6.042254       23.666667             0.788732   \n",
       "11012    133.200000      6.000000       22.200000             0.639640   \n",
       "11013    127.576923      5.944444       21.461538             0.478495   \n",
       "11014    125.800000      5.285714       23.800000             0.705882   \n",
       "11015    168.000000      6.588235       25.500000             0.666667   \n",
       "11016     46.333333      5.791667        8.000000             0.916667   \n",
       "11017    171.428571      6.091371       28.142857             0.538071   \n",
       "11018    107.357143      6.036145       17.785714             0.570281   \n",
       "11019    140.750000      6.186813       22.750000             0.725275   \n",
       "11020     78.000000      5.571429       14.000000             0.928571   \n",
       "11021     36.000000      4.500000        8.000000             0.875000   \n",
       "11022    180.500000      5.822581       31.000000             0.725806   \n",
       "11023     65.000000      5.909091       11.000000             0.818182   \n",
       "11024     96.000000      5.052632       19.000000             0.947368   \n",
       "11025     81.333333      5.674419       14.333333             0.767442   \n",
       "11026    109.000000      6.055556       18.000000             0.833333   \n",
       "11027    116.125000      5.546269       20.937500             0.549254   \n",
       "11028    115.428571      5.771429       20.000000             0.525000   \n",
       "11029    129.000000      6.142857       21.000000             0.857143   \n",
       "11030     95.000000      5.937500       16.000000             0.875000   \n",
       "11031     91.000000      7.000000       13.000000             0.923077   \n",
       "11032     54.500000      5.450000       10.000000             0.850000   \n",
       "11033    113.857143      6.178295       18.428571             0.767442   \n",
       "11034    118.000000      6.941176       17.000000             1.000000   \n",
       "11035    112.000000      5.894737       19.000000             0.815789   \n",
       "11036     99.666667      5.537037       18.000000             0.814815   \n",
       "11037     39.000000      7.800000        5.000000             1.000000   \n",
       "11038    125.500000      6.197531       20.250000             0.666667   \n",
       "\n",
       "              a     about     after       all      also        an    ...     \\\n",
       "0      0.061856  0.010309  0.000000  0.020619  0.000000  0.020619    ...      \n",
       "1      0.044444  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "2      0.090909  0.000000  0.010101  0.000000  0.010101  0.000000    ...      \n",
       "3      0.075472  0.009434  0.009434  0.000000  0.000000  0.009434    ...      \n",
       "4      0.067797  0.005650  0.000000  0.000000  0.005650  0.000000    ...      \n",
       "5      0.074074  0.027778  0.009259  0.000000  0.018519  0.009259    ...      \n",
       "6      0.090909  0.000000  0.000000  0.000000  0.000000  0.018182    ...      \n",
       "7      0.057692  0.000000  0.000000  0.000000  0.000000  0.057692    ...      \n",
       "8      0.043478  0.000000  0.000000  0.007246  0.000000  0.007246    ...      \n",
       "9      0.012821  0.000000  0.000000  0.000000  0.000000  0.025641    ...      \n",
       "10     0.054348  0.002717  0.000000  0.002717  0.008152  0.008152    ...      \n",
       "11     0.049383  0.000000  0.012346  0.000000  0.000000  0.012346    ...      \n",
       "12     0.038095  0.000000  0.000000  0.004762  0.009524  0.004762    ...      \n",
       "13     0.127660  0.000000  0.021277  0.000000  0.000000  0.000000    ...      \n",
       "14     0.080000  0.008000  0.000000  0.000000  0.000000  0.008000    ...      \n",
       "15     0.106061  0.000000  0.015152  0.000000  0.000000  0.000000    ...      \n",
       "16     0.061947  0.000000  0.004425  0.013274  0.004425  0.008850    ...      \n",
       "17     0.089552  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "18     0.055556  0.000000  0.055556  0.000000  0.000000  0.000000    ...      \n",
       "19     0.012658  0.000000  0.012658  0.012658  0.000000  0.012658    ...      \n",
       "20     0.080000  0.000000  0.000000  0.000000  0.000000  0.040000    ...      \n",
       "21     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "22     0.048780  0.000000  0.012195  0.000000  0.000000  0.012195    ...      \n",
       "23     0.050633  0.000000  0.000000  0.000000  0.000000  0.037975    ...      \n",
       "24     0.044983  0.010381  0.000000  0.003460  0.006920  0.000000    ...      \n",
       "25     0.061290  0.006452  0.000000  0.003226  0.003226  0.016129    ...      \n",
       "26     0.065421  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "27     0.050955  0.000000  0.006369  0.000000  0.000000  0.012739    ...      \n",
       "28     0.036630  0.000000  0.003663  0.003663  0.003663  0.000000    ...      \n",
       "29     0.047619  0.000000  0.000000  0.000000  0.000000  0.031746    ...      \n",
       "...         ...       ...       ...       ...       ...       ...    ...      \n",
       "11009  0.166667  0.000000  0.000000  0.000000  0.055556  0.000000    ...      \n",
       "11010  0.074074  0.000000  0.012346  0.000000  0.000000  0.000000    ...      \n",
       "11011  0.055556  0.000000  0.000000  0.000000  0.000000  0.027778    ...      \n",
       "11012  0.109375  0.015625  0.000000  0.000000  0.000000  0.031250    ...      \n",
       "11013  0.059859  0.003521  0.003521  0.000000  0.000000  0.007042    ...      \n",
       "11014  0.063492  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11015  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11016  0.142857  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11017  0.073684  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11018  0.033058  0.016529  0.000000  0.000000  0.016529  0.008264    ...      \n",
       "11019  0.022222  0.000000  0.022222  0.000000  0.000000  0.022222    ...      \n",
       "11020  0.000000  0.000000  0.000000  0.000000  0.000000  0.100000    ...      \n",
       "11021  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11022  0.025641  0.025641  0.000000  0.000000  0.000000  0.025641    ...      \n",
       "11023  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11024  0.076923  0.000000  0.000000  0.000000  0.000000  0.076923    ...      \n",
       "11025  0.142857  0.000000  0.035714  0.000000  0.000000  0.000000    ...      \n",
       "11026  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11027  0.062147  0.000000  0.005650  0.000000  0.000000  0.000000    ...      \n",
       "11028  0.040000  0.006667  0.013333  0.006667  0.000000  0.000000    ...      \n",
       "11029  0.111111  0.111111  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11030  0.000000  0.000000  0.111111  0.000000  0.000000  0.000000    ...      \n",
       "11031  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11032  0.000000  0.000000  0.000000  0.000000  0.000000  0.090909    ...      \n",
       "11033  0.098039  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11034  0.200000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11035  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11036  0.107143  0.000000  0.000000  0.000000  0.035714  0.000000    ...      \n",
       "11037  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    ...      \n",
       "11038  0.038462  0.000000  0.012821  0.000000  0.000000  0.000000    ...      \n",
       "\n",
       "       NN CC NN  NN DT NN  NN IN DT  NN IN JJ  NN IN NN  NN MD VB  NN NN IN  \\\n",
       "0      0.030303  0.030303  0.181818  0.000000  0.030303  0.000000  0.030303   \n",
       "1      0.000000  0.052632  0.105263  0.000000  0.000000  0.052632  0.052632   \n",
       "2      0.083333  0.041667  0.208333  0.000000  0.041667  0.000000  0.083333   \n",
       "3      0.000000  0.045455  0.136364  0.000000  0.045455  0.000000  0.045455   \n",
       "4      0.000000  0.000000  0.222222  0.000000  0.000000  0.022222  0.022222   \n",
       "5      0.000000  0.000000  0.148148  0.037037  0.074074  0.000000  0.000000   \n",
       "6      0.166667  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7      0.000000  0.100000  0.050000  0.000000  0.050000  0.000000  0.000000   \n",
       "8      0.027027  0.054054  0.054054  0.000000  0.000000  0.000000  0.000000   \n",
       "9      0.000000  0.058824  0.058824  0.000000  0.000000  0.000000  0.058824   \n",
       "10     0.010753  0.032258  0.118280  0.043011  0.064516  0.010753  0.010753   \n",
       "11     0.047619  0.000000  0.095238  0.000000  0.000000  0.000000  0.095238   \n",
       "12     0.040000  0.000000  0.040000  0.000000  0.000000  0.040000  0.000000   \n",
       "13     0.090909  0.000000  0.090909  0.000000  0.090909  0.000000  0.000000   \n",
       "14     0.060606  0.030303  0.060606  0.000000  0.121212  0.000000  0.060606   \n",
       "15     0.000000  0.000000  0.187500  0.000000  0.062500  0.000000  0.000000   \n",
       "16     0.000000  0.000000  0.150000  0.000000  0.000000  0.000000  0.016667   \n",
       "17     0.000000  0.000000  0.176471  0.000000  0.058824  0.000000  0.058824   \n",
       "18     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19     0.000000  0.090909  0.136364  0.000000  0.045455  0.045455  0.000000   \n",
       "20     0.125000  0.000000  0.125000  0.000000  0.125000  0.000000  0.000000   \n",
       "21     0.000000  0.000000  0.500000  0.000000  0.000000  0.000000  0.000000   \n",
       "22     0.000000  0.000000  0.137931  0.034483  0.034483  0.034483  0.034483   \n",
       "23     0.040000  0.080000  0.120000  0.000000  0.000000  0.000000  0.040000   \n",
       "24     0.000000  0.071429  0.166667  0.000000  0.000000  0.000000  0.000000   \n",
       "25     0.029851  0.014925  0.134328  0.014925  0.044776  0.000000  0.029851   \n",
       "26     0.000000  0.000000  0.047619  0.000000  0.000000  0.047619  0.047619   \n",
       "27     0.019231  0.057692  0.134615  0.000000  0.019231  0.000000  0.076923   \n",
       "28     0.000000  0.043478  0.130435  0.043478  0.014493  0.028986  0.057971   \n",
       "29     0.000000  0.000000  0.176471  0.000000  0.058824  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "11009  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11010  0.000000  0.000000  0.227273  0.000000  0.000000  0.000000  0.000000   \n",
       "11011  0.000000  0.000000  0.090909  0.000000  0.090909  0.000000  0.000000   \n",
       "11012  0.000000  0.076923  0.076923  0.000000  0.000000  0.000000  0.000000   \n",
       "11013  0.043478  0.028986  0.115942  0.028986  0.014493  0.000000  0.014493   \n",
       "11014  0.000000  0.000000  0.133333  0.000000  0.000000  0.000000  0.000000   \n",
       "11015  0.000000  0.000000  0.100000  0.100000  0.000000  0.000000  0.000000   \n",
       "11016       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11017  0.000000  0.000000  0.222222  0.037037  0.000000  0.000000  0.000000   \n",
       "11018  0.000000  0.000000  0.064516  0.064516  0.000000  0.000000  0.032258   \n",
       "11019  0.000000  0.000000  0.176471  0.000000  0.000000  0.000000  0.000000   \n",
       "11020  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11021       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11022  0.000000  0.000000  0.200000  0.000000  0.000000  0.000000  0.000000   \n",
       "11023  0.000000  0.000000  0.166667  0.000000  0.166667  0.000000  0.000000   \n",
       "11024  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11025  0.000000  0.000000  0.142857  0.000000  0.000000  0.000000  0.000000   \n",
       "11026       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11027  0.117647  0.000000  0.176471  0.000000  0.000000  0.000000  0.000000   \n",
       "11028  0.000000  0.000000  0.166667  0.000000  0.000000  0.000000  0.000000   \n",
       "11029  0.000000  0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
       "11030       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11031  0.000000  0.000000  0.000000  0.250000  0.000000  0.000000  0.000000   \n",
       "11032  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11033  0.000000  0.000000  0.117647  0.058824  0.000000  0.000000  0.058824   \n",
       "11034       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11035  0.000000  0.200000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11036  0.000000  0.166667  0.166667  0.166667  0.000000  0.000000  0.000000   \n",
       "11037       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "11038  0.000000  0.000000  0.190476  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       NN NN NN  NN TO VB  TO VB NN  \n",
       "0      0.000000  0.000000  0.000000  \n",
       "1      0.000000  0.000000  0.000000  \n",
       "2      0.000000  0.000000  0.000000  \n",
       "3      0.000000  0.045455  0.000000  \n",
       "4      0.000000  0.000000  0.044444  \n",
       "5      0.000000  0.074074  0.000000  \n",
       "6      0.000000  0.166667  0.000000  \n",
       "7      0.050000  0.100000  0.000000  \n",
       "8      0.027027  0.108108  0.000000  \n",
       "9      0.058824  0.058824  0.000000  \n",
       "10     0.000000  0.053763  0.021505  \n",
       "11     0.047619  0.000000  0.000000  \n",
       "12     0.000000  0.120000  0.040000  \n",
       "13     0.000000  0.090909  0.090909  \n",
       "14     0.000000  0.030303  0.000000  \n",
       "15     0.000000  0.000000  0.000000  \n",
       "16     0.033333  0.016667  0.033333  \n",
       "17     0.000000  0.058824  0.000000  \n",
       "18     0.000000  0.000000  0.000000  \n",
       "19     0.000000  0.136364  0.000000  \n",
       "20     0.000000  0.000000  0.125000  \n",
       "21     0.000000  0.000000  0.000000  \n",
       "22     0.034483  0.034483  0.000000  \n",
       "23     0.040000  0.040000  0.000000  \n",
       "24     0.000000  0.047619  0.023810  \n",
       "25     0.014925  0.044776  0.000000  \n",
       "26     0.000000  0.000000  0.000000  \n",
       "27     0.019231  0.057692  0.000000  \n",
       "28     0.014493  0.043478  0.000000  \n",
       "29     0.058824  0.058824  0.000000  \n",
       "...         ...       ...       ...  \n",
       "11009  0.000000  0.000000  0.000000  \n",
       "11010  0.000000  0.090909  0.000000  \n",
       "11011  0.000000  0.090909  0.000000  \n",
       "11012  0.076923  0.076923  0.000000  \n",
       "11013  0.000000  0.086957  0.014493  \n",
       "11014  0.200000  0.000000  0.000000  \n",
       "11015  0.000000  0.000000  0.000000  \n",
       "11016       NaN       NaN       NaN  \n",
       "11017  0.000000  0.037037  0.000000  \n",
       "11018  0.000000  0.032258  0.000000  \n",
       "11019  0.000000  0.117647  0.000000  \n",
       "11020  0.000000  0.000000  0.000000  \n",
       "11021       NaN       NaN       NaN  \n",
       "11022  0.000000  0.000000  0.000000  \n",
       "11023  0.000000  0.333333  0.000000  \n",
       "11024  0.000000  1.000000  0.000000  \n",
       "11025  0.000000  0.000000  0.000000  \n",
       "11026       NaN       NaN       NaN  \n",
       "11027  0.000000  0.058824  0.000000  \n",
       "11028  0.000000  0.111111  0.000000  \n",
       "11029  0.000000  0.000000  0.000000  \n",
       "11030       NaN       NaN       NaN  \n",
       "11031  0.000000  0.250000  0.000000  \n",
       "11032  0.000000  0.000000  0.000000  \n",
       "11033  0.000000  0.000000  0.000000  \n",
       "11034       NaN       NaN       NaN  \n",
       "11035  0.000000  0.200000  0.200000  \n",
       "11036  0.000000  0.000000  0.000000  \n",
       "11037       NaN       NaN       NaN  \n",
       "11038  0.047619  0.000000  0.000000  \n",
       "\n",
       "[11039 rows x 4000 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    './/datasets//constructed_1.csv',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "extract_features_for_windows(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Baseline classifiers with window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T15:16:55.948744Z",
     "start_time": "2019-01-23T15:11:31.360900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM CLassifier\n",
      "0.6681306615426846\n",
      "RF CLassifier\n",
      "0.7024430414493549\n"
     ]
    }
   ],
   "source": [
    "# Loading feature data\n",
    "features = pd.read_csv(\n",
    "    './/Features/calc_features_with_windows_0.txt',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "# REVIEW: we shouldn't get NaN or infinite values in the first place\n",
    "# Fill NaN, infinite values in features\n",
    "features[features==np.inf]=np.nan\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "# Reshape for LSTM steps\n",
    "features\n",
    "\n",
    "# split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data.label, test_size=0.33, random_state=42)\n",
    "\n",
    "# run through default SVM Classifier\n",
    "print('SVM CLassifier')\n",
    "svm_clf = SVC(gamma='auto')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "print(svm_clf.score(X_test, y_test))\n",
    "\n",
    "# run through default Random Forest Classifier\n",
    "print('RF CLassifier')\n",
    "rf_clf = RandomForestClassifier(n_estimators=300)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T15:06:22.726431Z",
     "start_time": "2019-01-23T15:06:11.714344Z"
    }
   },
   "source": [
    "#### Run Baseline classifiers with windows with basic features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T15:11:22.130327Z",
     "start_time": "2019-01-23T15:11:00.364393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11039, 16)\n",
      "SVM CLassifier\n",
      "0.5492725775459786\n",
      "RF CLassifier\n",
      "0.6958550645072742\n"
     ]
    }
   ],
   "source": [
    "# Loading feature data\n",
    "features = pd.read_csv(\n",
    "    './/Features/calc_features_with_windows_0.txt',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "features = features.iloc[:, [0,1,2,3,1000,1001,1002,1003,2000,2001,2002,2003,3000,3001,3002,3003]]\n",
    "print(features.shape)\n",
    "\n",
    "# REVIEW: we shouldn't get NaN or infinite values in the first place\n",
    "# Fill NaN, infinite values in features\n",
    "features[features==np.inf]=np.nan\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "# split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data.label, test_size=0.33, random_state=42)\n",
    "\n",
    "# run through default SVM Classifier\n",
    "print('SVM CLassifier')\n",
    "svm_clf = SVC(gamma='auto')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "print(svm_clf.score(X_test, y_test))\n",
    "\n",
    "# run through default Random Forest Classifier\n",
    "print('RF CLassifier')\n",
    "rf_clf = RandomForestClassifier(n_estimators=300)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using windows in an LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T13:53:50.424245Z",
     "start_time": "2019-01-25T13:52:55.630451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianluebke/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 4, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 10)                40440     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 265)               2915      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 265)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 265)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 266       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 43,621\n",
      "Trainable params: 43,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7396 samples, validate on 3643 samples\n",
      "Epoch 1/100\n",
      "7396/7396 [==============================] - 3s 439us/step - loss: 0.6767 - acc: 0.5710 - val_loss: 0.6647 - val_acc: 0.5819\n",
      "Epoch 2/100\n",
      "7396/7396 [==============================] - 1s 99us/step - loss: 0.6489 - acc: 0.6136 - val_loss: 0.6411 - val_acc: 0.6423\n",
      "Epoch 3/100\n",
      "7396/7396 [==============================] - 1s 110us/step - loss: 0.6365 - acc: 0.6307 - val_loss: 0.6344 - val_acc: 0.6481\n",
      "Epoch 4/100\n",
      "7396/7396 [==============================] - 1s 97us/step - loss: 0.6333 - acc: 0.6326 - val_loss: 0.6312 - val_acc: 0.6470\n",
      "Epoch 5/100\n",
      "7396/7396 [==============================] - 1s 91us/step - loss: 0.6277 - acc: 0.6408 - val_loss: 0.6306 - val_acc: 0.6467\n",
      "Epoch 6/100\n",
      "7396/7396 [==============================] - 1s 93us/step - loss: 0.6249 - acc: 0.6470 - val_loss: 0.6208 - val_acc: 0.6550\n",
      "Epoch 7/100\n",
      "7396/7396 [==============================] - 1s 98us/step - loss: 0.6194 - acc: 0.6463 - val_loss: 0.6259 - val_acc: 0.6506\n",
      "Epoch 8/100\n",
      "7396/7396 [==============================] - 1s 112us/step - loss: 0.6197 - acc: 0.6489 - val_loss: 0.6219 - val_acc: 0.6541\n",
      "Epoch 9/100\n",
      "7396/7396 [==============================] - 1s 118us/step - loss: 0.6167 - acc: 0.6558 - val_loss: 0.6149 - val_acc: 0.6593\n",
      "Epoch 10/100\n",
      "7396/7396 [==============================] - 1s 125us/step - loss: 0.6178 - acc: 0.6545 - val_loss: 0.6144 - val_acc: 0.6593\n",
      "Epoch 11/100\n",
      "7396/7396 [==============================] - 1s 132us/step - loss: 0.6108 - acc: 0.6627 - val_loss: 0.6878 - val_acc: 0.5218\n",
      "Epoch 12/100\n",
      "7396/7396 [==============================] - 1s 116us/step - loss: 0.6150 - acc: 0.6509 - val_loss: 0.6073 - val_acc: 0.6651\n",
      "Epoch 13/100\n",
      "7396/7396 [==============================] - 1s 106us/step - loss: 0.6084 - acc: 0.6597 - val_loss: 0.6347 - val_acc: 0.6404\n",
      "Epoch 14/100\n",
      "7396/7396 [==============================] - 1s 124us/step - loss: 0.6098 - acc: 0.6624 - val_loss: 0.6059 - val_acc: 0.6695\n",
      "Epoch 15/100\n",
      "7396/7396 [==============================] - 1s 130us/step - loss: 0.6071 - acc: 0.6631 - val_loss: 0.6085 - val_acc: 0.6648\n",
      "Epoch 16/100\n",
      "7396/7396 [==============================] - 1s 125us/step - loss: 0.6046 - acc: 0.6662 - val_loss: 0.6039 - val_acc: 0.6703\n",
      "Epoch 17/100\n",
      "7396/7396 [==============================] - 1s 122us/step - loss: 0.6035 - acc: 0.6696 - val_loss: 0.6167 - val_acc: 0.6552\n",
      "Epoch 18/100\n",
      "7396/7396 [==============================] - 1s 121us/step - loss: 0.6059 - acc: 0.6608 - val_loss: 0.6005 - val_acc: 0.6805\n",
      "Epoch 19/100\n",
      "7396/7396 [==============================] - 1s 118us/step - loss: 0.6038 - acc: 0.6690 - val_loss: 0.5993 - val_acc: 0.6747\n",
      "Epoch 20/100\n",
      "7396/7396 [==============================] - 1s 114us/step - loss: 0.5951 - acc: 0.6731 - val_loss: 0.6817 - val_acc: 0.5954\n",
      "Epoch 21/100\n",
      "7396/7396 [==============================] - 1s 98us/step - loss: 0.6008 - acc: 0.6662 - val_loss: 0.5950 - val_acc: 0.6827\n",
      "Epoch 22/100\n",
      "7396/7396 [==============================] - 1s 100us/step - loss: 0.5945 - acc: 0.6764 - val_loss: 0.5958 - val_acc: 0.6816\n",
      "Epoch 23/100\n",
      "7396/7396 [==============================] - 1s 104us/step - loss: 0.5976 - acc: 0.6720 - val_loss: 0.6105 - val_acc: 0.6635\n",
      "Epoch 24/100\n",
      "7396/7396 [==============================] - 1s 99us/step - loss: 0.5962 - acc: 0.6701 - val_loss: 0.5917 - val_acc: 0.6862\n",
      "Epoch 25/100\n",
      "7396/7396 [==============================] - 1s 121us/step - loss: 0.5913 - acc: 0.6774 - val_loss: 0.5995 - val_acc: 0.6736\n",
      "Epoch 26/100\n",
      "7396/7396 [==============================] - 1s 104us/step - loss: 0.5924 - acc: 0.6705 - val_loss: 0.6402 - val_acc: 0.6333\n",
      "Epoch 27/100\n",
      "7396/7396 [==============================] - 1s 108us/step - loss: 0.5932 - acc: 0.6751 - val_loss: 0.6011 - val_acc: 0.6838\n",
      "Epoch 28/100\n",
      "7396/7396 [==============================] - 1s 105us/step - loss: 0.5894 - acc: 0.6821 - val_loss: 0.6524 - val_acc: 0.6195\n",
      "Epoch 29/100\n",
      "7396/7396 [==============================] - 1s 108us/step - loss: 0.5883 - acc: 0.6785 - val_loss: 0.6085 - val_acc: 0.6635\n",
      "Epoch 30/100\n",
      "7396/7396 [==============================] - 1s 103us/step - loss: 0.5967 - acc: 0.6702 - val_loss: 0.5954 - val_acc: 0.6799\n",
      "Epoch 31/100\n",
      "7396/7396 [==============================] - 1s 108us/step - loss: 0.5856 - acc: 0.6781 - val_loss: 0.6012 - val_acc: 0.6835\n",
      "Epoch 32/100\n",
      "7396/7396 [==============================] - 1s 98us/step - loss: 0.5865 - acc: 0.6756 - val_loss: 0.5975 - val_acc: 0.6758\n",
      "Epoch 33/100\n",
      "7396/7396 [==============================] - 1s 105us/step - loss: 0.5830 - acc: 0.6817 - val_loss: 0.5829 - val_acc: 0.6975\n",
      "Epoch 34/100\n",
      "7396/7396 [==============================] - 1s 113us/step - loss: 0.5916 - acc: 0.6714 - val_loss: 0.5911 - val_acc: 0.6862\n",
      "Epoch 35/100\n",
      "7396/7396 [==============================] - 1s 105us/step - loss: 0.5789 - acc: 0.6848 - val_loss: 0.5838 - val_acc: 0.6978\n",
      "Epoch 36/100\n",
      "7396/7396 [==============================] - 1s 101us/step - loss: 0.5798 - acc: 0.6837 - val_loss: 0.5974 - val_acc: 0.6846\n",
      "Epoch 37/100\n",
      "7396/7396 [==============================] - 1s 94us/step - loss: 0.5869 - acc: 0.6833 - val_loss: 0.5887 - val_acc: 0.6945\n",
      "Epoch 38/100\n",
      "7396/7396 [==============================] - 1s 100us/step - loss: 0.5754 - acc: 0.6906 - val_loss: 0.5889 - val_acc: 0.6868\n",
      "Epoch 39/100\n",
      "7396/7396 [==============================] - 1s 98us/step - loss: 0.5748 - acc: 0.6912 - val_loss: 0.6058 - val_acc: 0.6753\n",
      "Epoch 40/100\n",
      "7396/7396 [==============================] - 1s 100us/step - loss: 0.5859 - acc: 0.6766 - val_loss: 0.5921 - val_acc: 0.6854\n",
      "Epoch 41/100\n",
      "7396/7396 [==============================] - 1s 97us/step - loss: 0.5725 - acc: 0.6939 - val_loss: 0.5856 - val_acc: 0.6991\n",
      "Epoch 42/100\n",
      "7396/7396 [==============================] - 1s 110us/step - loss: 0.5735 - acc: 0.6975 - val_loss: 0.5830 - val_acc: 0.6983\n",
      "Epoch 43/100\n",
      "7396/7396 [==============================] - 1s 108us/step - loss: 0.5685 - acc: 0.6981 - val_loss: 0.6408 - val_acc: 0.6333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    './/datasets//constructed_1.csv',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")\n",
    "# Loading feature data\n",
    "features = pd.read_csv(\n",
    "    './/Features/calc_features_with_windows_0.txt',\n",
    "    sep = ',',\n",
    "    header = 0, \n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "# REVIEW: we shouldn't get NaN or infinite values in the first place\n",
    "# Fill NaN, infinite values in features\n",
    "features[features==np.inf]=np.nan\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Build the model\n",
    "timesteps = 4   # number of windows\n",
    "data_dim = 1000 # number of features\n",
    "\n",
    "# reshape the feature data to fit the time steps\n",
    "features = features.as_matrix().reshape((features.shape[0], timesteps, data_dim))\n",
    "\n",
    "# split into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data.label, test_size=0.33, random_state=42)\n",
    "\n",
    "inputs = Input(shape=[timesteps, data_dim])\n",
    "layer = LSTM(10, unroll=True)(inputs)\n",
    "layer = Dense(512)(layer)\n",
    "layer = Activation('relu')(layer)\n",
    "layer = Dropout(0.1)(layer)\n",
    "layer = Dense(1)(layer)\n",
    "layer = Activation('sigmoid')(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=500,\n",
    "    epochs=100, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)]\n",
    ")\n",
    "\n",
    "aatm_support.draw_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "537px",
    "left": "1552px",
    "right": "20px",
    "top": "114px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
