Since the epistemological and methodological issues are similar with simulations, which are computer extended analogs of thought experiments, and computer assisted proofs I will only focus on the latter. Computer assisted proofs came into prominence with the Appel-Haken-Koch's (supposed) proof of the Four-Color Theorem. Tymoczko in his 1980 paper Computers, Proofs and Mathematicians wrote:"Computer-assisted proofs illustrate the need for a more realistic philosophy of mathematics that allows for fallibility and empirical elements". But this meant extending the scope of mathematics only while demarcating the "rigorous", computer-free one: P.S. In On the New Foundational Crisis of Mathematics (1921) Weyl wrote: “an ensemble of individual points is, so to speak, picked out from the fluid paste of the continuum. The continuum is broken up into isolated elements, and the flowing-into-each-other of its parts is replaced by certain conceptual relations between these elements, based on the “larger-smaller” relationship. This is why I speak of an atomistic conception of the continuum”. Weyl speaks here of points “picked out” arithmetically, as in the classical conception or his earlier constructivist one. Aside from the half-fabricated tradition, a boost to this usage was given by Fechner's psychological experiments in 1860-s, where the participants were shown ten rectangles, and 76% chose as "the most pleasing" the ones with the side ratios of 1.75, 1.62, and 1.50, the 1.62 being closest to the Golden one. Trouble is, later experiments did not reproduce Fechner's findings, and Godkewitsch's meta-study in 1974 concluded that the preference was an artifact. According to Godkewitsch:"The basic question whether there is or is not, in the Western world, a reliable verbally expressed aesthetic preference for a particular ratio between length and width of rectangular shapes can probably be answered negatively". More recent experiments concerning proportions of faces also failed to confirm the golden ratio's hype. As Livio points out: Kant may still have been impressed by the mystiques of mathematical certainty and the moral law ("Two things fill the mind with ever new and increasing admiration and awe, the more often and steadily we reflect upon them: the starry heavens above me and the moral law within me"), and reserved the same mode of justification for both, his synthetic a priori. But already Hume had doubts about both, which Kant chose to overlook, and in mathematics at least the mystique went by the way of logicist "laws of thought" that Frege and Russell never quite found. After the wrangling over Cantorian infinities and the axiom of choice, intuitionism, Gödel's incompleteness, Cohen's set-theoretic pluralism, Quine (mathematics as holistic completion of empirical theories), late Wittegenstein (mathematics as "grammar"), etc., it is hard to see how the analogy can do the justificatory work that rationalists expected of it. Moreover, Dostoevsky, Nietzsche, existentialists, and the two world wars, provided quite independent reasons to cast aside the ethical rationalism itself. Nick Seewald runs a website on golden ratio, which has a lot of up to date information on its occurences, real and mythical. One solution is due to Meinong: objects in logic may not exist but only "subsist", this is Meinong's version of becoming, but it also covers all sorts of fictions and absurdities. If you take this route you have to give up existential generalization, P(a) does not imply existence of x with property P, and allow contradictory sentences, P(a) and ¬P(a) may both hold if a is non-existent. If you give an argument with subsistent objects in the premises you may conclude all sorts of things about them, but it will not get you very much since none of them have to exist. To move from subsistence to existence would be exactly to commit the "defining into existence" fallacy. Quine famously adapted Russell's paraphrase to deny ontological existence to predicates. His criterion of ontological commitment is "to be is to be a value of the bound variable", i.e. to be is to be in the range of existential quantifier of scientific theories after paraphrasing out dispensable fictions, like Pegasus (sets and numbers can not be plausibly paraphrased, according to Quine, they are indispensable). He then replaced Frege's second order logic by first order logic with "semantic ascent" that does not quantify over predicates. Instead, we move to the meta-language and "paraphrase" second order quantification over predicates with schemata that contain placeholders fillable by definable predicates only. This is a formal mirror reflection of medieval nominalism, "real" common natures (objective predicates) are replaced with nomina ("words" for symbolic predicates). Of course, Quine was no logicist, but one could say that he picked up the pieces from Carnap's version of it that could be salvaged by embedding logic and mathematics into his holist web. In Epistemology Naturalized he admits that the original promises of logicism ring hollow: Both definitions are outdated. As Husserl put it already back in 1901:"Only if one is ignorant of the modern science of mathematics, particularly of formal mathematics, and measures it by standards of Euclid and Adam Riese, can one remain stuck in the common prejudice that the essence of mathematics lies in number and quantity". In antiquity mathematics was almost exclusively about numbers and simple shapes, over the course of middle ages and 17th century functions and equations joined them. In 19th and especially 20th century mathematics underwent another transformation, so its content came to be represented exclusively by sets, even relations and operations on sets are sets. In 1950s abstract structures par excellence, mathematical categories, were introduced and deployed in various subfields and applications. A more mainstream version of dealing with Plato's beard, one favored by Quine himself, is due to Russell. It involves eliminating defined objects from premises by using descriptions, before any logical analysis of arguments. Russell's way of talking about say round squares is to use a variable x with predicates R(x) and S(x), rather than a proper name with dubious existential status. The rest depends on how exactly you want to use round squares in premises. If you want to make any existential claim about them, e.g. "some round squares are green" ∃x(R(x)∧S(x)∧G(x)), then any premise involving it will come out as false, and any argument based on it will be unsound, even if valid. But something like "all round squares are round" ∀x(R(x)∧S(x) → R(x)) is not just true but even a logical tautology. For that matter, even "all round squares are green" ∀x(R(x)∧S(x) → G(x)) is a tautology, if we are assuming that R and S contradict each other.