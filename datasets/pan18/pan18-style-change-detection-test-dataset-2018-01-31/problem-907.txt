We can argue in broad generalities that "if the nature is not uniform we are in trouble anyway so we might as well optimistically assume it uniform", and even throw in the anthropic principle for good measure ("in universes with non-uniform natures no intelligent creatures can exist"), but it does not provide what a justification must: when it is supposed to work. So at best it is a vague methodological maxim. As for splitting off the "impossible" part of God, I don't think it can work for theodicy either. The whole point of it is presumably to justify God to humans, if it can not do so in a comprehensible way it becomes pointless, same as it does if it can not justify all of God. If the consistent part could not exclude evil due to consistency, why didn't the inconsistent part do it? If this is beyond our comprehension then why bother with theodicy in the first place. Since then it was pointed out that "absolute certainty" was never absolute, and issues with social acceptance of proofs did not originate with computers. For instance, Kempe's 1879 proof of the four color theorem was accepted as such for 11 years, until Heawood found a flaw in it. A fierce debate about the role of rigor and experiments/simulations commenced when Jaffe and Quinn in 1993 suggested to implement Tymoczko's idea by demarcating "theoretical" (modeled on theoretical physics) and "rigorous" mathematics institutionally, in publications, etc. See discussion and references under What makes something mathematics? The institutionalization idea did not take (some saw the Jaffe-Quinn proposal as an attempt to devalue "theoretical mathematics"), but clear delineation of conjectural/heuristic material remains an informal norm. It is also expected that any substantive use of computer tools, even in calculations let alone proofs, is explicitly mentioned and described enough to be reproducible (which is similar to reporting experiments in science). Conventional wisdom says "no", lack of new predictions being the main criticism. Johansson and Matsubara review string theory from various empiricist perspectives, and the best they can say (for string theory) is that it is not the right time to drop it. Yet. They add that it still dominates physics only because "those involved have strong realist convictions". If concepts are functions, rules, and "unities of the acts of ordering" then can we have private "language of thought" made of them? Or is Wittgenstein right, and we can not? Can we reconcile Kant and Wittgenstein? Berkeley's point was broader, that pragmatic certainties of life leave things wide open on metaphysics. Wittgenstein picked it up, and took it further:"Philosophy may in no way interfere with the actual use of language; it can in the end only describe it... It leaves everything as it is". It is literally vital to reach consensus on using language to coordinate how we act, beyond that it is not so vital. And metaphysical aspects, aspects that "leave everything as it is" open up plenty of room for skepticism and relativism. Wittgenstein's solution is his well known philosophical minimalism: there is no point to these extras because they take words beyond the contexts that give them meaning, and so just juggle empty words:"Philosophy is a battle against the bewitchment of our intelligence by means of language". NE declares that everything is subject to revision, including NE itself, in the face of recalcitrant experience. If that is the case then it should whole-heartedly embrace say Husserlian phenomenology or mystical insight if those prove to be practically successful. And in some areas they might, e.g. heuristic metaphysics, or ethics, or aesthetics. What "practically successful" means is open to interpretation, but according to Quine himself even in science choice between theories is based on pragmatic considerations due to underdetermination by (empirical) evidence. Such considerations may be interpreted as benefiting from some non-empirical experience, e.g. Husserlian "ideal perception". But Quine certainly favored science and scientific method, as I suspect do most philosophers who self-identify as naturalists, and mostly did not venture into areas where alternatives might come into play. They are related in the same way as for idealism, epistemology provides a necessary foundation for ontology, if not strictly logically then morally. It is logically possible to be ontological realist while maintaining epistemological idealism, in fact it is attractive for its subtlety, Kant and Quine are famous examples. But they confirm the rule: if one does not believe that available experience (be it purely empirical or augmented by some sort of intuition) can more or less reveal reality "as it is" ontological realism becomes a formality. What is somewhat implicit in neuro-nets becomes explicit in clustering algorithms, also popular in pattern recognition. They are as if designed to Wittgenstein's family resemblance specifications: Plantinga's free will defense assumes quite a bit more than logical consistency, namely Molinist understanding of free will and God's foreknowledge, which is again a minority position among Catholic theologians. In particular, the argument explicitly requires that creatures act identically in identical circumstances, and that foreknowledge of their choices be part of God's "middle knowledge". But as Felt writes in Impossible Worlds this posits a "determinate outcome of a free agent’s acting while excluding the acting", and thus creates "metaphysically inconsistent fictions which cannot form an object of anyone’s knowledge, not even God’s". But without them Plantinga's "transworld depravity" simply becomes meaningless, and without limitations imposed on God by classical logic it can not do its work of theodicy.