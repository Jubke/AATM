Their ontologies are very different, and so is the idea of "approximation". Popper mostly followed in the footsteps of logical positivists by looking for measures of corroboration and approximation such as degree of verisimilitude, "truthlikeness". The more true sentences a theory entails the more truthlike it is, see SEP's Truthlikeness. Even aside from unpleasant formal consequences (no falsehood can be as close to the truth as a logical truth, a triviality, and no false theory is closer to the truth than any other) Popper's account uncritically imports the notion of truth itself. It seems at times that he wanted to keep the traditional correspondence truth, common to "folk philosophy" of scientists, along with the falsificationist epistemology. Peirce was much more sensitive to Kantian problems that such a mix poses for a realist. Particularly, to unifying conceptual accounts of diverse cognizing subjects into a single reality, and describing subject's interaction with it, see What are the similarities/differences between how Kant thinks 'noumenon' limits understanding compared to C.S. Peirce? 

Thus, while Locke takes methodological, and even some factual, assumptions of the new science for granted, and explores their implications, Hume subjects the very sources of scientific knowledge, human reason and senses, to the same interrogative scrutiny that science gives the world. In that, and in the skeptical conclusions that followed, he anticipated Quine's naturalized epistemology. It does raise the concerns about the circularity of justification of science by science, but Hume is not yet troubled by them as Kant will be after him. Much later Quine will defend it with:"such scruples against circularity have little point once we have stopped dreaming of deducing science from observations". In this, Hume's "psychological" approach to the grounding of science anticipates Peirce's and Quine's, and Quine explicitly credits him as a major inspiration in Epistemology Naturalized: 

It is in this mode that Einstein realized that ether, absolute space, and even individual spacetime points were idealizations in the classical physics, and could be discarded in the new physics. But of course they were that even before there was a need to discard them. Physicists of 19th century believed that light spread in ether, but that was an idealization abstracted from properties of EM waves. In the modern interpretation EM field is self-propagating and there is no need for ether, but EM field is also an idealization. And so are elementary particles, like electrons and quarks. In quantum field theory they appear as placeholders in Feynman diagrams used to compute perturbative expansions, and those lead to notorious divergences when applied to gravity. So quantum gravity description may not involve Feynman diagrams and the particle picture. But that does not have to happen, more often than not there is enough continuity in idealizations that it is enough to modify concepts of what exists rather than discard them. For instance, atoms were imagined in 19th century as indivisible small rigid balls, and we no longer believe those exist, but their quantum successors are close enough to keep the name.