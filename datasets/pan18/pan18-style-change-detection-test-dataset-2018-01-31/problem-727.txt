It should be said that Husserl was philosophically averse to Kant's "creative" transcendental subject, perhaps due to the dominance of absolute idealist interpretations of him at the time, and preferred to derive his lineage from Hume, whom he credits as the principal forerunner of phenomenology. See Mall's Experience and Reason on their connection, which quotes Husserls' 1919 letter to Metzger:"I have learnt incomparably more from Hume than from Kant. I possessed the deepest antipathy against Kant, and he has not (if I judge rightly) influenced me at all". 

See Rise and Fall of Computational Functionalism (pp.16-20) paper for more details on functionalism and Twin Earth, and Realism, Reference, & Possible Worlds: the Approach via Modal Logic on its relation to rigid designation. 

And here we come to a major break with the Kantian theory of sensibility, Husserl rejects the undifferentiated manifold, and the idea that percepts are synthesized from "sense data". The latter is seen as ex post facto extraction from what is originally given to consciousness as already partially structured and unified, if obscurely (this was later confirmed by empirical cognitive science). In other words, rather than having two determinates, "sensibility" and "manifold" interacting, Husserl insists that all determinacy is only forged in the act of perception itsel. No determinates can be presented as "interacting" prior to it, dissolving the question of whether the content of perception "pre-exists" in reality, or is generated by mind. This aspect of perception, which apprehends idealities as immediate unities, Husserl terms categorical intuition, and together with eidetic variation, which shapes concepts into definitional maturity, it forms the process of ideation. Thus, Husserl sails between the Scylla of passive reception of impressions à la Hume, and the Charybdis of German idealist "construction of reality" by the mind. 

The position of functionalists on AI is similar to the position of compatibilists on free will in two important respects. First, they distance themselves from the Cartesian idea that there is some extra special "substance" or "essence" there, and dissociate what is so bundled into effects that can be modeled piecemeal causally and/or computationally. Second, as a consequence of the first, to them there is no bright line between intelligence and non-intelligence just as there is no bright line between freedom and non-freedom, there is only a continuum with extremes and pragmatically placed thresholds. Roughly, the more useful agency talk and other intentional vocabulary is for describing and anticipating behavior of a system the more likely it is to be classified as AI. The Turing test, in its classical form relying only on conversations, is insufficient, it has to be extended to the totality of behavior, including non-verbal actions and responses displaying certain levels of sensitivity to changes in the environment and complexity in adjusting to them, learning from experience as it were. 

"Criterial" approach stems from Wittgenstein's view of meaning as use, and irreducibility of language to propositional knowledge and logic. Roughly speaking, according to Wittgenstein there are types of knowledge, such as skills, that fall under "know how" rather than "know what", and can not be expressed in propositions, truth conditions and inferences without severe distortions. One could call such knowledge "instinctive". For instance, when we swim we do not assess our position and then make inferences to what move to make, we just do, and when we itch we just scratch. Similarly, we do not make inferences as to existence of other minds, we just deal with others, it is part of our make-up. But to upload such non-propositional knowledge into logic one has to split it into propositions connected by inferences, because that is the way logic works, and such upload is unavoidably inadequate to the original content. 

The reporter herself is aware of philosophical issues with "sharing consciousness", she quotes Damasio's Self Comes to Mind:“The fact that no one sees the minds of others, conscious or not, is especially mysterious,” he writes. We may be capable of guessing what others think, “but we cannot observe their minds, and only we ourselves can observe ours, from the inside, and through a rather narrow window”. 

Bundle theory, with "temporary captains" a la Dennet, does not appear clearly in James, but one can see the seeds of it in his descriptions of how alternatives are generated for the first stage: 

So what this comes down to is the historically attested human ability to find new axioms and new forms of reasoning that were not formalized in advance. As humans "find" them however they disagree about their truth and validity. According to humans called intuitionists, Gödel sentences are not true (or false), and neither is the axiom of choice or even the law of excluded middle. The parallel postulate is only "approximately true" these days. "True" axioms are not so much "recognized" as picked up and adopted because they come handy. But why can't a fancy AI attached to a random number generator and with robotic parts to interact with "reality" do the same? There are already computer programs that generate conjectures, like Graffitti, and something like Prolog can already be used to find proofs for them too. Neural networks, simulated on computers, can "induce" new patterns by "learning", what's to stop them from "inducing" the parallel postulate?