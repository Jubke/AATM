More often than not artists do not give arguments about using the golden ratio, they are sufficiently motivated by the long tradition of singling it out as "golden", which accumulated since Pythagoreans and the ancient Greek sculptor/architect Phidias. The perceived presence of golden ratios in his Parthenon now appears to be spurious, but the letter φ often used to denote it comes from the first letter of his name. Plato helped too by promoting dodecahedron, "which the god used for embroidering the constellations on the whole heaven", and which is full of golden ratios just like the Pythagorean pentagram. Both Pythagoreanism and Platonism flourished during the Renaissance, when much of our modern artistic tradition was forged, and even Copernicus and Kepler were self-proclaimed Pythagoreans. This said, the extent of the ratio's use in arts is greatly exaggerated today. Here is Falbo: 

But this argument seems odd to me. It can just as easily be turned around to argue that the apparent scarcity of personal and cultural biases in mathematical controversies makes it more credible that their subject matter has at least some claim to objectivity. If cultural dependence of "linguistic intuitions" is taken to go against the objectivity of their substrate why should it be different for ethical ones? The same is confirmed by what even Clarke-Doane himself points out in another context: 

Here is some quick background. The basis of Brouwer's philosophizing is the non-linguistic "primordial intuition of mathematics", of a continuum without qualitative characteristics or changes, where continuity and discreteness are fused, and where any in-between is inexhaustible. This is his fluid continuum of intuition. But even this primordial intuition is not free from idealization and abstraction, it is the abstraction from qualities that makes this continuum suitable for doing mathematics. The first act of intuitionism calls for tracing the emergence of the discrete from the fluid continuum non-discursively. Its foundation is the "two-ity", the "falling apart of moments of life into two qualitatively different parts". Here is some further commentary on this process from van Atten, van Dalen and Tieszen's Brouwer and Weyl: The phenomenology and mathematics of the intuitive continuum: 

It is fair to say that the concepts are "equivalent" in modern mathematical practice. However, they have different histories and different overtones of meaning. The notion of natural or counting numbers goes back (officially) to Pythagoreans and unofficially to prehistoric times, Ishango bone, a counting artifact, is 20,000 years old. We are dealing with a loose concept imported to mathematics from natural language after centuries of practical and then more abstract use. While in one sense cardinals generalize natural numbers, in another sense they are a much more narrow, technical, and context bound notion. Friedman, for his part, argued that Quine's "web of belief" is not sufficiently stratified, and that mathematics and logic enjoy certain autonomy (his own student Parsons and later Maddy expressed similar sentiments). This opens up a possibility that the reduction to logic and set theory may be more meaningful epistemologically than mere pragmatic convenience. And more recently there is a movement that is even closer aligned with the original logicism, the neologicism or neo-Fregeanism: 

EDIT: I'd like to clarify that mathematical construction is not meant in the narrow sense of constructivism, although those certainly qualify. But modern mathematics is also full of highly non-constructivist constructions, like Cantor's generation of ordinals and cardinals, or maximal ideals, algebraic closures, and everything else involving the axiom of choice. The whole architecture of pure mathematics is based on this, relations and functions are constructed from sets, groups, posets, etc. are sets with relations and functions, then there are spaces of functions on them, operators and functionals on those, and on and on. Since 1950s Quine routinely identified first order logic with the "language of science", Eklund himself mentions him as an influence. As for "true logic" as understood in philosophy Gödel argued quite the contrary. That human reasoning is not only uncaptured by the first order logic, but by any formalized logic, and that his incompleteness results are an indication of that. 

On the prevailing extensional interpretation of modality the difference between possibility and probability is the diffference between quality and quantity, possibility is the quality quantified by probability, see Probability Distributions Over Possible Worlds by Bacchus. This interpretation can be traced back to Leibniz's determinate possible worlds, but it became prevailing after Kripke's extensional formalization of modal semantics in late 1950s. To get probabilities one needs a positive unit measure on the set of all possible worlds, which is more general than counting because an infinity of them can be admitted. For instance, if a dart is thrown at a dartboard the possible worlds will have it sticking out of a particular point on it, of which there are infinitely many. But probabilities can still be assigned to particular regions based on their area (measure). Of course, different measures can be put on the same set of worlds, even when there are finitely many of them, so quantification is not unique. 

Such a surreptitious shift from constructive to platonist proofs is what motivated the intuitionist pushback, but as Wittgenstein points out the self-deception involved is far more pervasive. The point affects even formalists, as such shifts may involve major changes in the rules of the "formal games" of mathematics. On the other hand, it would be silly even for intuitionists not to make use of computer assisted proofs pragmatically. After all, when done by competent and trustworthy professionals the degree of certainty they provide is far greater than from numerical checks of conjectures, which have a venerable history involving Euler and Gauss.