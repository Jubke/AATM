The semantic (which is to say, related to meaning) and justificatory connections among our beliefs produce a hierarchy, where some sentences are more fundamental than others, in so far as others are based on them. Quine would have us think about these more fundamental beliefs as being at the core of our web of belief. Beliefs about logic and mathematics, for instance, are beliefs one which nearly all other justified beliefs are ultimately based. Laws of nature depend on those more fundamental beliefs, but are only slightly less fundamental. Then, as we build out from this core to the periphery of the web of belief, we find at its edges sentences which depend on many others, but on which few or no other beliefs depend. On the very edge of the web might be beliefs about ordinary observations, like that a donut costs $1.79 at the store nearest me. You've already noticed that there are some utilitarian arguments against some kinds of cases, based on risk of the unknown. But it's also clear that those don't apply to all cases. As for arguments that apply to only some cases, and yet still don't get humans involved, some are ecological, but another kind of argument focuses on the experience of the created being, where an animal is mixed with another species. It might be fun to try to add bat wing genetic material to monkeys to see if you can generate flying monkeys. But there is a real risk of creating a hybrid creature (sometimes called a chimera) whose experience of living is terrible, because it turns out wrong. Indeed, because this is such a complicated process and genetic and developmental processes are so intricate, we would likely have to create a lot of hybrids before one succeeded, and these animals might suffer considerably. Yet, this sort of argument only applies to animals, not plants or other species. Still, such arguments are worth taking seriously, I think. The broad form of the question places it in the subfield of Metaphysics, which includes the study of the nature of reality, including what kinds of things exist, and what defines those kinds. Among the problems it addresses, Philosophy of Biology takes up general questions from metaphysics, epistemology, and logic as they apply to living things and the science of them. One of the characteristics of the institution of science is normal science, and normal science involves puzzle-solving. Puzzle-solving is what you may be remembering as the typical activity of the institution of science. It is the characteristic activity of normal science, on Kuhn's view. Not all science is a matter of puzzle-solving, though, since some scientific activity is revolutionary—reevaluating the basic assumptions about what the major questions and puzzles for the discipline are. The reason this is not a special problem for naturalism is that it equally affects its opposite or counterpart position, supernaturalism. Naturalists attempt to justify knowledge by using (and learning how to be smart about using) personal experience, reasoning, learning from others ("testimony"), scientific reasoning, and other ways. But if those ways of justifying knowledge are open to skeptical challenges — and they are! — then so are supernatural or non-natural strategies for justifying knowledge. Indeed, supernatural strategies may even have MORE difficulty answering skeptics than naturalist ones do. I mean this with respect, and just as illustration: your question currently has the form of "What do Plato and Aristotle have to say about the difference between the Constitutional Democracy and the 21st century international banking system?" Answer: nothing. So far, that is just to argue, quickly, that Premise 4 is dubious. But we might also want an argument for the view that natural selection generally has, when it has produced belief-forming apparatuses, favored their reliability. That is just to say that organisms with more reliable belief-forming apparatuses have survived at a higher rate than similar organisms with less reliable belief-forming apparatuses. Philosopher Christopher Stephens offers such an argument in the paper “When is it selectively advantageous to have true beliefs?”. He concludes: Du Bois concludes: But as regards the enigma what matter and force are and how they are able to conceive, he [the investigator of Nature] must resign himself once for all to the far more difficult confession - "IGNORABIMUS!" If you use induction to create conjectures, e.g. the Riemann Hypothesis, that's quite OK. Every method is allowed to create conjectures. Using induction to create conjectures is the method of generalisation. Summing up: The model of spinning electrons is inconsistent and cannot serve to explain the phenomena. Obviously in mathematics the only method is to prove the conjecture - or to disprove it by generating a counter example. But in science one cannot prove general results. A finite number of confirmed cases does not increase the probability that the general result is true. That's the problem of induction. Kant, Immanuel: Metaphysical Foundations of Natural Science (German: Metaphysische Anfangsgründe der Naturwissenschaft) (1786) This is what I was taught the scientific method was in high school. If that is the version you are after, skip ahead to the second part, which explicitly targets that reading. Finally, science tests its theories. This sounds absurd, because it seems so obvious that you should test them. However, a theory is not accepted at all until it is tested. The result is that anyone with a theory must expend the resources to do the testing before science will do anything with it. Other approaches get away with a different style: you use a theory once you have it, and you test it when you get an opportunity to do so. The tests can also be dangerous. (Edit: I had a reference to the LHC and potential to create black holes here, but it was too contentious. Instead, it has been replaced with a hypothetical example) Consider a hypothetical particle physics experiment. The scientist is rather confident that their theory is correct. They begin experimenting, after calculating that they would like 100 samples to do statistics on. Generally speaking, they are finding their theory holds out for test after test. However, on tests which disagree with their hypothesis (which happens in the scientific method due to noise), the observer notices a burst of energy from the test apparatus. That burst becomes stronger and more dangerous with every data point that disagrees with their hypothesis. At some point, the scientist decides to cut the experiment short, because they are uncomfortable putting their life at risk to finish the test. By the strictest reading of the scientific method, that data cannot be analyzed because it is tainted with the scientist's choice to cut the tests off early. This might induce biases because the scientist is more likely to cut them off faster if the results look good for their theory. Other methodologies are capable of using this data (including the intuition of that scientist, who will not try the exact same experiment again).