Following holism, the whole system should be considered, not only its parts and their interactions. A typical example is entanglement in quantum mechanics. That does not mean that new irreducible higher level concepts have to be used to address the whole system. 

This is basically the question of scientific realism. The type of sceptical arguments your are alluding to are well known in philosophy of science. You have Hume's scepticism about induction to start with, and more generally, problems of underdetermination: the fact that several distinct explanations are always compatible with the same evidence. Then you have the pessimistic meta-induction, which is based on the fact that most past theories were eventually replaced by better ones, so we shouldn't believe that our theories are true. 

I fully agree with Cort Ammon's answer but I'd like to add a few points on the topic of definitions. 

This is really an interesting, open philosophical question. On the one hand, convincing is an important part of proving, yet we have logico-mathematical theories of deduction that set standards of proof, and one could thing that respecting these standards should be sufficient. The question is particularly salient when it comes to computers assisted proofs: should we trust them when noone really understand them? 

This leaves open the question of the origin of the intuitions (what are those lines we are trying to define?). In the case of geometry our intuitions certainly come from physical space. In order to know that the theorems of geometry are true of physical space we need what Poincare called coordination postulates: for example the assumption that light travels in straight lines in vacuum. They map primitive terms of an axiomatic system to our observations (Poincare saw them as kinds of conventions). Coordination postulates can be thought of as part of the implicit definition of physical lines but it's important to remember that an axiomatic mathematical system is independent of the way it is mapped to our observations and in a sense, being a line in a mathematical sense is nothing more than obeying the corresponding axioms of geometry. 

Or you can argue against these arguments. Arguments against underdetermination have it that one can use non-empirical criteria such as simplicity, in an "inference to the best explanation", and that this method is a good indicator of truth. They argue that this is the only explanation to the empirical success of theories, particularly when they're extended to new applications and still work: that would be a miracle if they were not true, or in some sense close to the truth. So our methods of theory choice must be efficient, even if not strictly empirical, and we can claim that our theories are true. 

The first two are the most often used in contemporary literature. The third is controversial because some would say that abstract objects, such as numbers, are mind-independent too. 

I'm not very sure this is always the case. Take the explanation for why a house burnt: it was made out of inflammable materials and there was a spark. These are not constituents of the fire. Or take the evolutionary explanation of why giraffes have a long neck. This involves environmental selection, which are not part of the giraffes'neck. It's even more easy to find non reductionist explanations outside of science (involving, say, God). 

Indeed as Popper argues, a complicated theory with many parameters is easy to adjust to fit any data set. For example, you could approximate many more curves or data points with a polynomial of high degree than with a simple line. This could mean that if both fit, the polynomial of high degree is less confirmed by evidence (because, intuitively speaking, its fit was too easy to obtain) than the line. But this is merely an intuitive argument.