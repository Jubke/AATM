In conclusion: you might quibble on the formulation, but given this premiss of continuity of change, Galileo's argument is sound. 

The problem is that we have no uncontroversial criteria today. We want science to be more empirical than metaphysics, which is why criteria of testability or refutability were often invoked, e.g. by Popper. But all scientific theories have untestable claims at their core (such as the principle of inertia in Newtonian physics, which requires an inertial frame of reference to be meaningful, but then it is circular). They have empirical consequences only when all axioms of the theory are taken together to create models, and this requires practical knowledge or auxilliary hypothesis that are external to the theory to map empirical data with theoretical models. Even then it is always possible to invoke an ad-hoc hypothesis to save the theory from experimental failure (for example, planet Vulcan was invoked to save newtonian physics). 

In face of that, you can be a constructive empiricist, like van Fraassen, and only assume that our theories are empirically adequate (but not strictly true). This only requires a form of induction (because empirical adequacy extends to all observable phenomena in the universe) but Hume's scepticism is very radical (would you refuse to take the plane because our theories might fail tomorrow?) so this is not so much a problem to assume empirical adequacy. This is up for debate but you're right that this is a debate about the demarcation between science and metaphysics. 

If now we turn to what philosophers understand by "physical", there are two main approaches: There are three possible understanding of "empirical philosophy" in your question: A strict definition associate a term with an equivalent statement, in such a way that the term can be substituted with the other statement in any sentence without changing the meaning of the sentence. In any case physical objects will be a subset of concrete objects (or exactly the same set for physicalists), and again, I think we can say that the electromagnetic field is physical according to both approaches: it is an object of physics, and it is supposed to be objective and compositional (you can consider a part of the field in a specific region). Alternatively one can restrict the relevant aspects of theories to structure : our theories might not be true, but still they approximately correspond to the structure of the world, and science progress towards more structural correspondence. The idea is that approximation makes more sense about structure. The position is called structural realism. The problem according to some is that this position is not really distinct from empiricism because all we're left with is a structure of observations... 

Following emergence, new properties appear at some level which cannot be deduced from the lower level and the arrangement of parts. This supposes some form of holism, since only the whole system has the property, yet the system could be separable with regards to its lower level properties. Typical (but controversial) examples could be biological and psychological concepts. One cannot necessarily say something is a gene from its molecular structure alone, because it could depend on the cellular context (respectively, a psychological character and a social context). Being a gene is emergent. Yet the gene could be separable into parts at the chemical level. I suspect that when one simply convince by showing, it is implicit that a more rigourous proof could in principle be provided, and that's why it is convincing. But there can be surprises. At the other extreme, someone might have found a valid proof but fail to convince anyone that it is valid. But there is an epistemic uncertainty: maybe she made a mistake somewhere. So the ability to convince brings epistemic confidence. But ultimately, I would say that someone could still be right even though noone understands (yet). Although I admit things are potentially more complex, because one could ask: what exactly is this person talking about? (And is the subject matter of importance, or only the structure of the proof is relevant?) Galileo's argument shows that the magnitude which determines the speed of free fall must be intensive, not extensive. It is not very clear where to situate copenhagen's interpretation, because of its vagueness. One option is to view it as a collapse theory, but it does not specify when, where and how the collapse occurs (only that it is related to measurement). Now perhaps our theories are more and more true because more and more terms actually refer to natural kinds and properties. Or perhaps theoretical properties can be more or less good carvings of the structure of the world, and current properties are better than previous ones in this respect. 

An axiomatic system is a good description of an intended domain if all the consequences we can derive from it seem to be true of the objects of that domain that we are intuiting and of course intuition plays a role here. In the case of lines, the axioms are good because all consequences (theorems...) are true of our representations of lines in euclidean space. 

Similarly the fact that string theory is based on non testable claims at its core should not be a problem so long as the whole theory can be used to create models that make empirical predictions. The problem for string theory is that it does not even provide any testable models so far (it looks more like a framework of possible theories with too much parameters to tell which is the right one for our universe), but as far as I know that's more a technical limitation than a principled one. Perhaps we should wait to see if this research program is fruitful or not.