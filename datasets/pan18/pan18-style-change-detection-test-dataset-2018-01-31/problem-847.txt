Imagine a thought experiment of a simulated reality similar to the matrix except at any point you could hit the reset button and restart the simulation and every time you died it automatically restarted. It could be argued that living in such a simulated reality you would never develop an emotional attachment to anything (not even your own life as there would be no reason to fear death) or fear losing it because you could just hit the reset button every time an experience that you wanted to be extended or repeated ended. Inversely you would never fear any consequence to any act. In such a reality what would be considered ethical behavior. The idea of reincarnation would imply reality itself resembles this kind of simulation. Would human behavior be more or less ethical if decision making was devoid of emotional attachment? i think having a work ethic is not necessarily ethical but can be consistent with ethics in regards to self-development and virtue. i disagree with "Having a work ethic means being disciplined and hard-working" and rather think that everybody has a work ethic regardless of how disciplined and hard-working they are. I think its more like people with less discipline and who are lazy have a poor work ethic in contrast to people who have more discipline and are hard-working have a strong work ethic. Prudence consistently being the number one virtue for many philosophers implies that in terms of enlightened self development that the more regular, consistently and harder one works in this regard the further one will get in self-development. in this context one could conclude that a strong work ethic is a virtue and that laziness is a vice. Also in Aristotles definition of virtue he says that all virtue is a medium on a continuum of excess and deficiency, so one could say that being a control freak or over achiever could be the excess, and laziness the deficiency, with the virtue of a strong work ethic residing in the correct balance. The question of "Could a machine/AI ever feel genuine human-like emotions"? arises in movies such as Bladerunner, IRobot, Star Trek and A.I among others. What is the philosophical arguement behind why the humans generally assume that a machine/AI cant feel genuine human-like emotions and act like it's a big deal when they learn that the machine/AI do feel genuine human-like emotions? Realistically, morality and guilt have nothing to do with this question. Every lawyer knows, guilt on the part of the client has absolutely no bearing on the outcome of a trial, only guilt that can be proven, or absence of proof, will effect the outcome. A lawyer with a conscience might wrestle with the dilemma, but to assume all lawyers have a conscience is erroneous. For most lawyers, it is actually to their benefit financially to have a reputation for successfully defending guilty clients, as this attracts more guilty clients to hire a lawyer with this reputation, as they know, said lawyer will use the law to protect the guilty. Lawyers (not all, but in general) definitely do lie or pretend they don't know somethings, because above all, they are not necessarily concerned about their clients interest, or the public's interest, but about their individual financial interest. Could a lawyer with "flexible" morals justify immorally trying to win a trial if he is aware of the culpability of the defendant to further his financial self-interest? Definitely. "It's not what you know, but what you can prove". Even a judge can be aware of the individuals guilt and this wont change the outcome without evidence. Also, why do humans assume that Machines/AI are untrustworthy because of their percieved lack of emotions? Doesnt Ethical philosophy generally teach that ethical decisions come from reason and principles, not passions? It just seems as though a machine/AI that follows the golden rule could possibly be more ethical than some humans. There are many different philosophical systems that argue either for or against emotional attachment like Buddhism, stoicism, Christianity, and the emotional attachment to the right to pursue happiness is at the center of much of the attitude of modern life as the most desirable goal. What would the implications be on ethical behavior without the desire to attain a feeling? It is posited that the most ethical reason for a person to become a doctor is out of a genuine altruistic desire to help people, in contrast to because it is a profession which pays well. Therefore is a doctor who doesnt help people who cant afford to pay unethical? Is it unethical for a doctor to accept monetary payment for services from the poor?