Nature is returning a permission denied status code for links to paywalled articles. While this is essentially a useless link for most users on many sites, such links are valid for many users on the scientific sites and many users there will have a subscription for this journal. 

I did try out the new, still under development "Broken links" tab of the review page. And while it certainly points out a lot of truly broken links that need to be fixes, I noticed certain categories of false positives on Skeptics that might be connected to paywalls. 

I'm not sure how these cases should be handled for the link checker, the links are valid even though they are behind a full or limited paywall. It might make sense to exclude domains that have many false-positives reported from the tool completely, though that might also hide actual broken links in the future. The script could then roll back all review decisions of that user and block him for some time from reviewing. The same applies to Looks Good. Just because the question is on-topic and doesn't need editing, doesn't mean I think the question is a good fit and well-researched. It'll just be meh. 

Please, no automatic votes on my behalf. 

And crap reviewers make it to 1k reviews faster than those that take reviewing seriously. It's easier to hit 'Looks Good' than to actually look at the post. You can go back in your browser to revisit a review, and then the vote buttons are active. Vote then. And just because a question is off-topic doesn't mean it also deserves a down-vote; it is not unheard of that a well-asked question turns out to be a duplicate, or is basically a request to recommend a library, or turns out to be too broad, but that doesn't mean didn't put the right amount of work in the question content. Those reviewers were entirely correct to reject the edit; the changes made were inappropriate. The limit is not just there to prevent crap reviewers from flooding the system. It's there to give everyone a chance to review. Emphasis mine; this edit was rejected because it defaced the post. 

If you come across a legitimate case of review abuse (e.g. a user that is rejecting everything it reviews with the same rejection reason), pick one post that the user reviewed, and flag it for moderator attention. Use the 'other' option and explain in the post what you found. 

The script could take the time between reviews into account as well as a suspicious deviation from the average percentage of close/not close decisions. But that could be not enough information, requiring at least two reviewers for each item would be a way to collect more information about the accuracy of a review. Then the script could also take into account how often a review decision is overturned by two other users. I'm not sure how these cases should be handled for the link checker, the links are valid even though they are behind a full or limited paywall. It might make sense to exclude domains that have many false-positives reported from the tool completely, though that might also hide actual broken links in the future. For the NY Times I suspect that the reason for that is their paywall implementation, which allows to read a certain number of articles per month for free, and blocks you after that. I'd guess that the bot is hitting this paywall. I'm not sure what the problem with jstor.org and nejm.org is, the preview/abstract is shown fine to me, and the site doesn't seem to return any permission denied or other error code. I did try out the new, still under development "Broken links" tab of the review page. And while it certainly points out a lot of truly broken links that need to be fixes, I noticed certain categories of false positives on Skeptics that might be connected to paywalls.