I'm more familiar with this kind of distinction in the context of the philosopy of science but should apply in the philosophy of law. 

If the creator, or the bot itself, were able to convince the relevant person/people at SE to make an exception or to amend the terms of service, then you wouldn't have a problem. 

If standing up a military is ethical in the first place, then as long as the recruiter provides correct and complete information to/from the applicant and does not apply any coercive threats/rewards to the potential recruit, everything is on the up and up (irrespective of whether the recruiter would be able to serve). The other items point out conditions where the likelihood of ethical lapses increase, but don't in themselves make the activity unethical. Who has explicitly addressed the problem as to when to decide to allow for a specific ethical theory to deviate from common-sense moral intuitions? For question 2, for anyone whose made an oath similar to the quoted one above (note that this includes all naturalized U.S. citizens), then performing their civic duty by assisting with recruitment rather than "front line" military service seems like a good way to discharge the obligations that flow from that oath. What I have in mind in the following situation: an individual has pledged their allegiance to a nation (not necessarily in a official/public way), is unable to directly serve in the military, and instead fulfills their pledge by assisting in recruiting. 

Of course, all this is predicated on (some degree) of objective moral truths. If you wanted to adopt the position of moral relativism, then you'd be deriving the moral truths from exactly these kinds of social/legal forces. 

An incorrect rational taboo holds back research into a field that would/could cause significant benefit; i.e. we think that we have a good reason to forego research into X, but in fact we're wrong. A correct rational taboo holds back research into a topic that would cause harm. Again, as a general matter, it is hard to tell where any specific case lies. Beyond what Ransack indicated, there's another flaw in your presentation: you assert that people who have enough empathy for the victims will be deterred by that, but then dismiss this case by saying that there are many criminals who are not empathetic. But at least a plurality of those non-empathetic criminals are not deterred by the prospect of supernatural punishment, e.g. declared Christians who have stolen from someone else. 

Consider Unite States Army's oath of enlistment: 

Requiring that theories of ethics merely be able to reproduce typical moral intuitions seems unduly limiting -- it rules out the existence of prescriptive moral theories. [Note that arguments that all moral theories cannot/should-not be prescriptive would address this question.] In addition, this seems problematic in the moral test cases where there is no clear common intuition (I don't know of any such cases, but I can't rule out that they exist). 

In this context, the AI aspect is a red herring; what is important is that SE is providing its services with restrictions. If anybody/anything is not able to conform to those restrictions they are lying (falsely entering into a contract); a fact that has negative moral impacts. The casualty rates are irrelevant per se; but again differing levels of risk would affect recruitment rates and thus put pressure on the officers to misrepresent the true casualty rate to potential recruits. To summarize, questions 3 and 4 don't in themselves automatically make the activity unethical, but they do create the conditions where ethical violations are more likely. 

Suppose I set up a suitably elaborated theory of ethics, but it fails to correspond with typical people's responses to the Trolley Problem(s). Should that be interpreted as a failure of the theory of ethics, or as a case where peoples' typical intuitions are morally suspect? In this way of thinking of things, moral considerations come first; then one assess whether the legal obligations align with the moral ones. If they do, then your course of action is obvious, if they don't you have a moral obligation to take the moral position. In some cases the presence of the non-moral (legal) obligations may reduce our culpability for failing to take the morally preferred option, since, especially in the legal case, there are pretty direct coercive forces at play. 

Edit in response to edited question (and some comments). It's pretty clear that there is no direct/logical/necessary relationship between what is legally or socially proscribed and what is moral; most people would claim that some things are moral whether or not they are legal or socially accepted etc. Thus these external influence are separate from actual moral responsibilities. 

Basically, I wouldn't do anything that I wouldn't do if the person were alive, but unable to provide consent for the proposed action, under the theory that any obligations incurred while he/she was alive persist. 

I see similar delineation problems in terms of aesthetics and epistemology, and it might arise in other fields as well. Thus someone may have tackled this as a kind of general problem, if so that is of interest too. 

I read your question as indicting taboos as, in some sense, arbitrary. So consider the following possiblities: 

Science has values. Not just that the people of the scientific community have values, but rather the enterprise of doing science imposes/requires a minimal set of values, i.e. things that are deemed "of value". These include things like: complete, transparent, and accurate reporting of findings; reproducibility of the processes used to derive the results; dispassionate reporting et al. When you interpret scientific results you can (or at least should be able to) assume that these results are being reported in accordance with these values. These are the values of science, as an ideal, and in my estimation/view the values that shape the actual practice of science as we know it. You can go down a rabbit hole of the hows/whys/wheres of deviations from these ideals do occur within science and/or when enough of them are violated we are no longer dealing with "True" (Scotsman) science etc. But for the purposes of this discussion, it makes sense to accept that science (or with more detail: sepcific communities with science) has some intrinsic/essential values.