Turkish has many verbs with so-called noncanonical (indirect) objects where one would expect a direct object. In general, just get a valency lexicon of any language and look for frames of intransitive verbs, you'll find many slots for indirect objects. Informally it's often said that the focus of a sentence is what's being said about its topic. In formal logic, focus is then taken to be an "Aristotelian" predicate. The unmarked sentence John sings (John topical, sings focal) is formalized as sing(John), whereas JOHN sings (John focal, sings topical) would be λP.P(John)(sing). Discourse-configurational languages assign topic/focus structurally. Hungarian is said to be one (Kiss, who coined the term "discourse-configuratonal"), other examples include Russian (King) or Georgian (Meurer). This approach only accounts for nonemotive sentences since intonation can mix things up. As for formal representation in frameworks, FGD uses an ordering on nodes in deep syntax trees to express information structure. In LFG, there's a separate i(nformation)-structure for discourse functions. In the abductive framework of Hobbs, there's no implicit formalization but whatever can't be inferred/proven is taken to be focal. Applicable to what? Meaning assembly? For this, λ-calculus is used only in categorical grammars, which are binary-branching by definition. In syntactic frameworks with a context-free backbone, glue semantics (based on linear logic) is used at times, but it has problems, too. BTW no human language is primitive, English has simpler morphology than Latin but it's more complex elsewhere. In most languages word order is more or less iconic with respect to information structure. Aside from word order and intonation, some languages have morphological discourse markers. There are topic markers in Japanese and Korean, focus markers in Eastern Armenian and both in (many dialects of) Quechua, to name just a few. Yes, of course. In German, for example, the verb "helfen" (to help) is intransitive: "ich habe ihm geholfen" (I helped him). It takes an indirect object (dative) that can't be passivized. 

There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). 

Meaning assembly via composition rules is best done using unification. If one uses Davidsonian (or neo-Davidsonian, i.e., Parsonsian) logical forms, every phrase (including preterminals) is associated with an LF fragment and an individual. If you have a ternary (or, in general, a nonbinary) rule, the individual of a subordinated phrase is unified with a variable in the LF fragment of the head. For example, the predicate of "give" is quaternary and the corresponding rule is VP -> V NP NP. Then the individual associated with V (the eventuality) is unified with that of VP. The individual associated with the indirect object is unified with the fourth argument of the predicate of the verb, etc. The subject variable remains open until it's unified later by another rule. Since (neo-)Davidsonian formulae are existentially closed conjunctions of literals, when the parsing is completed we take all literals used during parsing to be conjuncts and add a quantifier for each variable occurring in the LF. 

The above procedure can be informally described as a "relaxed" lambda calculus, but rather than replace variables we unify them. It can also be used in dependency grammars if we add unifications to ID rules. Many modern European languages are as complex as Latin, Ancient Greek, or Sanskrit. I'd point out Lithuanian but most Slavic languages are typologically similar to the mentioned ancient ones. And yes, native speakers use all constructions their language provides (all languages change, of course, so there are archaic constructions but it has nothing to the with complexity).