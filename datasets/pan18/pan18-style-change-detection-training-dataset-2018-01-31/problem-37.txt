However, during the Sprint Review (my exposition in bold): Your Product Owner should never, ever be assigning work to the team. Your "developers" and "QA people" are not on the same team, or not working closely enough together within the same team to meet Scrum's definition of a Development Team. A good Scrum team should contain all the skills needed to meet each Sprint Goal within the Definition of Done. Quality assurance doesn't sound like it's part of your Definition of Done. You have deadlines instead of iterations with potentially-shippable increments. The team has accepted stories it may not have the bandwidth to complete according to the Definition of Done. Your team (or teams) are not on schedule for successful completion of accepted work, but no one has identified a root cause or actionable impediment, or asked the Product Owner to consider an Early Termination and a return to Sprint Planning. A team with chronic resource constraints is either not following the Scrum methodology properly, is consistently mis-estimating tasks, or treating velocity as a management target. Fix the process! You may have other process problems, too. However, these are the ones that really stand out for me, and I generally recommend teams start with the low-hanging fruit. The point is to show your product doing something. If you can't show it in situ, you can at least create a mock-up or simulator showing how an API or middleware client would interact with your code. Demo that interaction! This is no different than a factory specifying machining tolerances for a widget. However, you need to ensure that your quality criteria are reasonable, concrete, and measurable, rather than simply subjective. This is why the use of a documented style guide is essential; you should not move the developers' cheese after the fact because of undocumented specifications. Someone on the team marks the story "done" on the Sprint Backlog. TL;DR After the first couple of Sprints, a Scrum product should always be a in a potentially-releasable state. That inherently means there's always something to demonstrate at a Sprint Demo, but you may need to be creative and "think outside the box" to find it. Treat demos as first-class work products during Sprint Planning in order to make demonstrations easier and more intuitive to develop. Think "Quality" and "Definition of Done" You can put anything you like into a statement of work, but this is not the path to actually getting what you want. I interpret your real motivation as: Almost any brand of TDD/BDD or continuous integration will require some level of test abstraction. Mocks, stubs, fixtures, and factories often stand in for other pieces of the "real" architecture, even in integration tests. There's nothing stopping you from using such techniques to stub out the functionality of API or middleware clients in your Sprint Demos. Vision Statements Defined According to a Wikipedia entry: Enforce the Scrum framework rigorously. Ensure that Product Backlog stories are properly refined, and granular enough to fit within a single Sprint. Ensure the team uses Sprint Retrospectives to inspect-and-adapt its estimating practices and its Sprint Planning Process. Ensure the team only accepts work into each Sprint that fits within the established velocity range, while still providing enough slack for unforeseen events. Don't fall prey to the %100 utilization fallacy! Ensure unit tests and acceptance tests are part of your Definition of Done, and make sure the whole Development Team shares responsibility for swarming over stories that need additional resources. Whenever the Sprint Goal may not be met, meet with the Product Owner to refine scope or request an Early Termination. You may certainly find other ways to improve the process, too. Don't stop there; Scrum (and agile practices in general) are all about continuous process improvement!