A philosophical concept is no different in principle, no harder or easier to understand than an artistic composition or a mathematical function. 

Now, if we are granting that we are in someone's ancestor simulation, it is certainly possible that we may run into certain limits "hardcoded" by various parameters of the simulation -- an example might be that there could be clauses in the simulation programming designed to identify civilizations that begin to develop the ability to simulate, and take some in-simulation action based on those events (which could include just about anything up to and including triggering the termination of the simulation, etc.) 

I might suggest that Ockham's razor cuts both ways on these sorts of questions. Remember that Bostrom's own presentation of the question is a trilemma: either 

Instrumentalism is an important position characteristic of pragmatism, alongside radical empiricsm and conceptual relativity. Consider the maxim of logic formulated by Pierce (which he called the "maxim of pragmatism" before the movement was even named): 

Finally, it may help to recall Bostrom's own conclusion: 

The hypocritical relationships of the Sophists to money was a major point of criticism against it -- Plato's criticism is basically that they would teach anyone who paid, where Socrates would "promiscuously" teach anyone at all -- or at least those who seemed like they were bright enough to keep up... Maybe kind of interesting today to consider alongside the watering-down of curricula, in lockstep with an ongoing corporatization of an education system which increasingly considers students customers. 

An encounter with the outside "splits" the subject and compels thought. Problems result from external relations, being open to the aleatory encounter which demands thinking: being equal to the Event. The event for philosophy is certainly not the same as a great historical event, and indeed sometimes they are nearly imperceptible (or indeed incorporeal.) But it is exactly here where a ruthlessly lucid empiricism is needed... 

But hand-wringing over "absolute" rigor is not the only concern. Hamming, whose mathematics was computer science oriented, is well known for quipping "typing is no substitute for thinking" and "the purpose of computing is insight, not numbers". One can take it pragmatically, or one can take it more fundamentally. Even pragmatically, simulations of the hurricane movements, say, may give us predictions, but not why they are thus and so. When models disagree we are left in the dark, because the crucial element of insight is missing. And on the Kantian/intuitionistic conception of mathematics and mathematical validity the chasm between human and computer assisted proofs is more than a matter of degree. That a deduction constitutes a proof as long as its every step is according to the "rules" is a formalist idea. But not all scientists or mathematicians are formalists. To some there is that elusive aspect of intuitive insight without which a proof is at best a blind calculation, with a stigma of inferiority attached. "Understanding" just does not take place outside of an understander, and a proof is not a proof until it is understood. Similar sensibilities affect reception of quantum mechanics, complaints about "shut up and calculate" are not uncommon. 

So while I am not sufficiently familiar with the tenets of naturalized epistemology to be sure about what they say, the answer from those fields where naturalized epistemology is supposed to draw inspiration is "no". 

This brings us to the Dawkins/Harris-style application-of-science arguments, which go something like so (again, not a real quote): 

Absence of evidence is almost always evidence of absence, actually. The problem is that it is often really poor evidence. 

This also ignores the point. Yes, there are easy cases, and they're already easy without this supposed framework for morality. Almost nobody seriously advocates for letting malaria run rampant or for spreading it. But there are other common problems, like increasing wealth disparities or the conflict between economic growth and environmental degradation or whether it is noble or evil to publicize the plight of starving children in Africa where you simply must answer many of these why questions. 

So the working hypothesis should be and is that we are "just" chemical computers. It does not follow that all the sorts of qualities that we value in humans--compassion, inspiration, consciousness, etc.--do not exist. We can plainly see that they exist. It just means that these things are implementable, somehow. A beach doesn't compute much; it doesn't follow that silicon cannot be used for computation, only that it's not organized the right way on a beach. Likewise, if our computers don't seem much like us in many ways, the simple explanation is that we haven't arranged them to (not without lack of trying, but building sand-castles isn't going to help much with calculating a square root either--if you don't know what to do, you can't do it, even if you have very good reason to believe that it could be done). 

But hand-wringing over "absolute" rigor is not the only concern. Hamming, whose mathematics was computer science oriented, is well known for quipping "typing is no substitute for thinking" and "the purpose of computing is insight, not numbers". One can take it pragmatically, or one can take it more fundamentally. Even pragmatically, simulations of the hurricane movements, say, may give us predictions, but not why they are thus and so. When models disagree we are left in the dark, because the crucial element of insight is missing. And on the Kantian/intuitionistic conception of mathematics and mathematical validity the chasm between human and computer assisted proofs is more than a matter of degree. That a deduction constitutes a proof as long as its every step is according to the "rules" is a formalist idea. But not all scientists or mathematicians are formalists. To some there is that elusive aspect of intuitive insight without which a proof is at best a blind calculation, with a stigma of inferiority attached. "Understanding" just does not take place outside of an understander, and a proof is not a proof until it is understood. Similar sensibilities affect reception of quantum mechanics, complaints about "shut up and calculate" are not uncommon.