There's only one place where we can truly achieve Lean Software Production in the same way as a production line, and that's in continuous integration and deployment, where we really are building the same thing over and over again. Software development generally doesn't work that way. 

Most focus for Lean Software Development has historically been on the production principles. However, there's now a new school of thought which suggests that those principles are somewhat limited where it comes to software development. 

For instance, if you can get something that's extremely valuable to the business into production, do it before anything else, even if it means other pieces of work hang around for longer. After that, do what's needed to keep work flowing through the system. This may mean having queues or buffers like "ready for development" or "ready for test", even though those would be considered inventory, and therefore waste. After that, eliminate waste. For actual Lean Software Development, I recommend David Anderson's Kanban and Jurgen Appelo's Management 3.0. The Toyota Production System was developed by applying Lean Thinking principles to the context of Toyota, and in that respect, Kanban is very similar: it applies Lean Thinking principles to the context of a particular team. It does not apply TPS brute-force to that team, nor do most of the successful Lean Software Development outfits. (As a note, I hear that most production lines which have tried to replicate TPS have had limited success). 

This article by Esther Derby suggests alternatives to using KPIs in pay reviews. There's a growing body of evidence that suggests that using performance as a way of measuring pay is destructive. My experience is certainly that it encourages heroism and a blame culture rather than team work, particularly when the KPIs are measuring unwanted items. 

The new school of thought treats software not as a Lean production line, but as Lean product development - in that it's more like designing new cars than building the same car over and over again. 

You can do this, but you may be measuring the wrong thing. This smells like an X/Y problem: you have a problem to solve with the deliverables hand-off, have decided that coding style is the thing that will fix the problem, and are now trying to solve for "coding style" rather than the underlying process issue. Treat demos as first-class work products, not afterthoughts. When you reframe the work this way, the best way to demonstrate the work increments usually becomes self-evident. is a whole lot better for everyone involved in the project than: 

In addition, it will help your team a great deal to focus on the iterative nature of agile methodologies; agility stresses refinement of the product over time, rather than on perfect execution of "specifications" within each increment. The Product Backlog is a living document, and stories on the backlog are added, changed, or removed at least every iteration. It is not a one-pass set of requirements, and your team needs to leverage that flexibility. 

What About Stories That Aren't Done Right? At the end of the Sprint, the team collects points for all stories that the whole team (including the Product Owner) agree were completed according to the Definition of Done. If the stories were thus completed, but are unsatisfactory in some way, that's grist for the mill during the Sprint Retrospective and for other inspect-and-adapt meetings. It doesn't change the fact that the stories were done in an agreed-upon way—an agreement to which the Product Owner was an active party—and so there's no "acceptance" of the stories to be done. 

Then, during Backlog Refinement or Sprint Planning, the Product Owner can take any stories that were completed but didn't deliver the desired value and: 

Good Vision Statements According to one source, good vision statements should focus on "what your business does and what...you would like it to do[.]" Your boss's generic slogan doesn't measure up in that regard, even though it's probably a worthwhile team objective. 

It doesn't matter how pretty the code is. Code can follow style guides all day long and still be poorly documented, needlessly complex, or improperly designed. For example, imagine you want to convert some financial application from dollars to euros; if the exquisitely beautiful code you receive as a deliverable makes it impossible to figure out how to make a basic change like that, then the code is spaghetti code even if it's indented properly. 

The objections all of the contributors are on target and these are true with all KPIs. You establish a metric to increased desired behavior, that desired behavior is "paid for" by the removal of other behaviors, some of which are also desired. This is why establishing your KPIs is very challenging and you need to do so with care. 

Yes. That's the idea of a CR. It can be accepted, rejected, deferred, delayed, etc. In fact, a change denial before it begins the process is damaging to the overall project, though it likely happens a lot on many projects and maybe even more so in certain industries. Similarly, a change accepted before it starts the process is scope creep by definition. 

The piece of work farmed out needed to have other deliverables as part of that contract, things like a schedule, reports on a set frequency, intermediate verification and validation of scope and milestones, things like this. Flying blind to the end date is using hope for strategy. 

It's interesting that you called this a CR *rejection/*approval process and then asked if it was appropriate for it to be rejected, which is part of the process's name. So I sense there is another, more accurate question to your concern. I am wondering if it has to do with the costs incurred during the technical feasibility analysis, i.e., if rejected so too are all costs associated with it.