That these core values of science affect how results are conveyed and interpreted is a more subtle point, and can be described in a way that makes this interaction less interesting than the theory-ladenness of science. In considering theory ladenness it is relatively easy to come up with counterfactuals: e.g. without special relativity, measuring masses in GeV/c^2 doesn't make sense. Trying to construct the same kind of counterfactuals where the essential values of science are violated leads to situations where you are not dealing with science. Public policy research institutes, which adhere to a different set of values, produce research papers that include specific objective facts, but are not doing science. As I currently understand it, some segments of the biomedical research community have stopped doing (or failed to do) science, since the cherry picking of positive outcomes amounts to incomplete reporting of their results. The point is these core values are, for some contexts, less interesting because they are just part of the background that comes into play when discussing science at all. Lying is generally considered unethical. Falsely entering into a contract is a form of lying. The Terms of Service for the Stack Exchange sites indicate that accepting the terms indicates that the subscriber is "an individual". The rest of the contract makes clear that the intention is that the subscriber is a (human) person. A bot would be unable to live up to this aspect of the terms of service. An arbitrary taboo holds back research into a field that would/could provide significant human benefit. An arbitrary taboo holds back research into a field that will cause significant human harm. An arbitrary taboo holds back research that is just a waste of time (i.e. the only cost is an opportunity cost one, we could have used the resources better) In terms of act utilitarianism, these three cases represent different moral outcomes. Given that all three of these possibilities seem possible at the outset, it is hard to try to formulate the decision in a more rule-based (rule utilitarianism) way. Suppose I set up a suitably elaborated theory of ethics, and I find that gratuitous murder is perfectly acceptable. This seems like a good reason to reject that theory of ethics, i.e. it seems that matching at least some basic moral intutions is a requirement of any ethical theory. Stoicfury's comments point out the following fact: we do (generally) try to respect the wishes of people even when the effects extend past the end of their lives. Wills, funeral arrangements, keeping, say a personal oath of secrecy (the usual don't tell anybody about this type of thing), are examples of exactly this kind of thing. I do not view this as providing rights to the deceased, it is merely that one's ethical obligations don't magically disappear someone else's death. Language is the primary medium through which social exchange takes place; it is not your words or the lack thereof that you ought to blame. The problem is that lying is always manipulative, a way in which one person disregards the humanity of another. The categorical imperative is a way to express this "always": the very nature of the action is such that it effectively bars itself from being performed morally, i.e., strictly treating others as ends in themselves rather than a means towards your own goals. Any reading of any text whatsoever demands the most basic 'faith' in the veracity of the author -- you have to believe in good will at some point or the entire discursive chain is undone. Finally, the question of why you would cause a machine intelligence to feel pain also seems worth investigating to me. After all, it would seem possible to design a machine intelligence without providing an equivalent of 'physical' pain -- so why would we go out of our way to provide this "benefit"? Note that among sentient machines are likely to be uploaded human minds, which presumably would prefer not to experience physical suffering again; the pain response would seem more like an unfortunate meatspace necessity we would be gratefully ridding ourselves of (as with aging and death.) Example #1 - It's fine utilitarianism. If you're trying to maximize pleasure and don't value consent at all, then you are in the clear as long as the research is likely to increase pleasure for the most in the long run. In other words, there's two big question that are not made clear just by reading Nietzsche: It seems much harder to prove either preference or rule utilitarians could allow this regardless of what they want to maximize. Approaches to ethics that assert absolute values reject the idea that all ethical goods can be quantified. Instead, they take it as prima facie that some ethical goods cannot be sacrificed regardless of other consequences. The rule utilitarian hedges the question a bit better by coming up with rules that are designed to optimize said goal but that don't depend on the individual agent arriving at the epistemic best path under the stress of the moment. It doesn't completely eliminate the horns, but it dilutes their painfulness, because it can suggest that our duty is to follow rules that seem optimized to produce the ends we want even if in particular cases they don't.