David Lewis' account of the logic of imperatives is in terms of the possible worlds in which the imperative is obeyed. Here is a handout for a class which extensively deals with the formal semantics of questions. In Sanskrit, there is a quotative particle iti, which you would use in something like "he is known as Bhagavān", "he is called Bhagavān", which seems to be along the lines of what you're looking for. In the realm of questions about what constitutes a "word", I believe from his comments here that John Lawler has a position to the effect that Lushootseed utterances are not composed of multiple words, they are composed of multiple morphemes, and "word" is not a useful concept in analyzing the structure of the language (leaving aside a possible and under-investigated stress-related facts). It is certainly not obvious that a distinction between "sentence" and "word" is mandatory for the language. The second regards roots vs. words. If "glit" is a verb, you can form multiple inflected forms from it such as "glits", "glitted", "glitting" and so on. Some people don't want to include inflected forms because they aren't "really" different words, they are just forms of one word. Which then requires you to ask "What do you mean by 'word'?". There are languages with very robust morphology which allow huge numbers of words, and languages with recursive or at least iterative morphology, which have infinite word-formation potential. So if you are only looking at a count of roots, and if you are taking the I-language perspective, then the repository of roots in an individual's mental representation of their language at a given time is finite, thus countable. That is, no lexicon (repository) has an actual infinity of representations. OTOH, there is no specific number of items which defines the largest possible number of entries in a language repository. This also happens in Shona and some other African languages, and there's a book on the topic, Quotative Indexes in African Languages: A Synchronic and Diachronic Survey by Tom Güldemann, which mentions other languages with this construction. Anyhow, there is more to what linguists do than assign semantic interpretations to sentences. We use formalisms for all aspects of language, and at least for the non-quantitative, the interpretation of those formalisms requires the use of logic. Formal logic is important in interpreting these formalisms, since it provides a clear method of interpretation. Modal logic has proven useful in accounting for the meaning of sentences, and is of no use in interpreting phonological rules. Not every useful tool have to be useful for all problems. Yes, Shannon's model for entropy has no concept of meaning or even sequence. Each symbol (letter or word, in our context) is assumed to occur completely independent of another symbol. I don't know enough to recommend textbooks on psycholinguistics, but how about this course page: http://www.coli.uni-saarland.de/courses/experimental_psycholinguistics_2011/schedule.php? Evaluation based on experimental work, active participation in lectures and the final research report. But no one who made such models claimed that their model is precise. As far as I know. Having said that, to answer your broader question, Semeval 2012 and 2013 focused on Semantic Textual Similarity. The above equation has no conception of sequence. Later models that don't ignore the sequence, such as n-gram models and Markov models have shown significantly better entropy figures. I agree (roughly) with the first part of the sentence; we are getting closer and closer to the upper bound of the expected per-word entropy. I disagree with the second part because there can't be an "exact way": entropy figures can be exact if were're dealing with a closed-world system, but when it comes to text, no matter how large the corpus, new text is usually bound to contain words never seen before. A sentence like "pass me the SATA cable" would never have been uttered a couple of decades ago, but are fairly common now.