Its flaws are well-known and serious. To recall, an inference from A to B is valid iff all interpretations of "non-logical constants" that make A true also make B true. What are interpretations, a.k.a. models or possible worlds? These are metaphysically loaded, a nominalist would reject their use, and inherently vague, the leading theories, like Kripke's or Lewis's, disagree on basics of how they function. It is hard to agree on truth of A and B if we do not agree whether "water" refers to anything in a given interpretation. This is of course related to having to understand "meanings" of sentences to ascertain their truth, and brings up a bag of problems with Carnap's analytic/synthetic distinction, Quine's criticisms of meaning and synonymy, etc. Apparently, "grounding" is a new word of choice in metaphysical debates, there was a philosophical Conference on Grounding and Emergence held in Glasgow last May. This is a reaction to fading hopes of reducing sciences to more fundamental ones and ultimately to physics, which was the original programme of scientific realism, known as reductionism or physicalism. The original replacement was supervenience, changes in composite objects can not happen without changes in their physical substrates, but supervenience proved to be too weak and too vague a notion to satisfy many philosophers with realist leanings. This rings all sorts of Kantian alarm bells for me. The reason for "described by" in textbooks is that "mathematical structure" is a representation, while "physical world" is not, so one can not literally "be" the other for conceptual reasons. Representation by itself is not a representation of anything, it can only represent something else through a correspondence scheme, just like a book without a 'reader' (possibly inanimate) is only an object combining ink and paper. In case of correspondence to something physical the scheme itself would normally consist of some physical procedures that relate "forces" to forces, "masses" to masses, "motion" to motion, etc. This is how "such and such is described by mathematics" is usually interpreted. Tegmark's expansive formulation though seems to leave no room for such an interpretation. It would not help to say that the physical procedures involved are themselves mathematical structures, or realizable on a Turing machine, because what we are trying to understand is exactly what it means for the physical to be so structured, or so realizable. It would not help to say that in place of "mathematical structure" it means some physical realization of it for the same reason, both set off infinite regress. It is the abandonment of moral judgments about reality ("existence is blameworthy") that seems key to me here; an experimental ethics yields a very different spirit of analysis than a transcendental morality that knows everything in advance. Very simply it is less gloomy, less bored -- it lacks the gravity and sad passions that motivate moral responsibilities (bitterness, grief, melancholy, resentment, vengeance and so on.) And so just to complicate this schema a bit, perhaps an absolutely immanent ethics makes use of a certain "transcendent" moment in its own way. But there is a different relation; it is no longer a starting point (Spinoza makes this very clear -- it is important not to start out with the idea of God, but rather to reach it as swiftly as possible...) My sense of the hypothesis is that basically it isn't falsifiable or empirical; there is an analytic dimension to Bostrom's analysis here that simply can't be validated in any experimental way. Hence I don't think it poses a significant problem for scientific realism. I happen to think this is pretty interesting and definitely agree there is something to this. Zizek might perhaps suggest that there is a kind of terrifying excess when one really 'follows' the rules, without respecting the various meta-rules which make them workable. Finally, it may help to recall Bostrom's own conclusion: Is this the general consensus amongst philosophers of science or one of a number of different viewpoints still debated (and if the latter, what is this viewpoint called)? Is the simulation hypothesis outside of science? If so, then does it pose a problem for scientific realism? If not, is it falsifiable? Is it an empirical question? On the question of the simulation hypothesis (i.e. that reality is a simulation), a friend of mine once remarked he didn't accept it on the grounds of Ockham's razor. To me (with my admittedly instrumentalist view of science) this seems a misuse of Ockham's razor in that all my friend is really claiming is that it isn't empirically necessary (and hence outside of science). Is it the general view amongst philosophers of science that science isn't about truth but rather adequately predictive models and therefore it doesn't make sense to speak of a scientific theory as true so much as an adequate model of the evidence?