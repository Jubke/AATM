I'm thinking it could and likely should work in much the same way that suggested edit review audits currently work, with a caveat for duplicates. Those that would do a proper review are crowded out by those that game the system to get another colored dot next to their name, keeping the queue empty and ineffective. To better address the robo-review problem I propose an automated flag set up to trip whenever someone uses the same review response X number of times in a row or when they use an identifiable pattern. Completed at least 250/1,000 constructive* review tasks. (*No Action Needed Not Included) This badge is awarded once per review type. Perhaps I'm oversimplifying a lot, but wouldn't it be kind of practical to just move the pass fail check to when the actual close vote is cast? If we can catch things like vote fraud with an automated process, why not try something similar with robo-reviewing? While I can understand the desire not to delete content that could possibly be improved or expanded on, I think that adding a timer would likely add unnecessary complication to what should be a straight forward process. As mentioned in other posts on this problem the robo-reviewers tend to just hit the no action needed button to try and fly through as quickly as possible. I know there are cases when there doesn't seem to be any problem with a post, but you don't feel it warrants an up-vote, hence the No Action Needed button, but should we be handing out badges to people who just hit the button 250 or 1,000 times? Also, if we see the same sort of robo-review problems we're already seeing in the other queues, this queue would just make for more low hanging fruit. Sorry, I'm with Caleb on this one, this seems like a bad idea. The signal to noise ratio would be shot. Of course we wouldn't want to auto-ban people who tripped a flag in this way, there is an outside chance that someone really did run into 5 or 10 posts of the same quality in a row, or that their responses fell into a pattern for a short run, but as its unlikely, it would be an easy way to spot troublesome reviewers and bring them to a moderator's attention. Present the user with an audit If the user clicks the close button, show the dialog as usual If the user closes as a duplicate, behave as if it were a normal close vote If the user chooses another close reason and clicks "vote to close" then pass/fail the audit appropriately Like I said this is likely oversimplifying things by leaps and bounds, but I think this is the expected behavior. I think that current system would benefit from all the solutions suggested in the question, but I think its obvious that the primary cause of the problem is robo-reviewers. If an answer looks promising but needs more explanation you can always: I'm seeing another possible problem with this proposal, has anyone considered the potential for a huge backlog in this proposed comment review? Currently we have approximately 1,642,000 users with less than 50 rep. If each one of them posted one comment that went into a review queue we would have a massive problem. But I would go further and change the Reviewer/Steward Badges to something like: If we can catch things like vote fraud with an automated process, why not try something similar with robo-reviewing? Of course we wouldn't want to auto-ban people who tripped a flag in this way, there is an outside chance that someone really did run into 5 or 10 posts of the same quality in a row, but as its unlikely, it would be an easy way to spot troublesome review patterns and bring them to a moderator's attention. I agree with Bart's comment here The case that you sited in your question looks like a rare exception to me. I'm sure you've posted many more comments on low quality posts to no avail, I know I have. If the backlog grew well into the thousands, and I wouldn't be surprised if it did, it would take quite a while for a comment to reach its intended destination and with comments being transient in nature, I doubt that many of the comments would still be relevant days or weeks later. If the concern is generating "fake" close votes so that the audit seems to show that other users voted to close the post for some reason or other... I'm not sure its entirely necessary. I routinely see close vote reviews that don't have any indication of how many close votes the post has. I assume these are posts that have flags rather close votes, I would guess that most users are accustomed to seeing these reviews and wouldn't really think twice about not seeing the blue numbers.