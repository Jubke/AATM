There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). Meaning assembly via composition rules is best done using unification. If one uses Davidsonian (or neo-Davidsonian, i.e., Parsonsian) logical forms, every phrase (including preterminals) is associated with an LF fragment and an individual. If you have a ternary (or, in general, a nonbinary) rule, the individual of a subordinated phrase is unified with a variable in the LF fragment of the head. For example, the predicate of "give" is quaternary and the corresponding rule is VP -> V NP NP. Then the individual associated with V (the eventuality) is unified with that of VP. The individual associated with the indirect object is unified with the fourth argument of the predicate of the verb, etc. The subject variable remains open until it's unified later by another rule. Since (neo-)Davidsonian formulae are existentially closed conjunctions of literals, when the parsing is completed we take all literals used during parsing to be conjuncts and add a quantifier for each variable occurring in the LF. 

where e is an eventuality (also called situation, possible event, or state of affairs). Such logical forms can express everything one can encounter in language (such as quantification and logical connectives) so there's no reason not to use them if it helps elsewhere. And help it does a lot in pragmatically interpreting discourse. Meaning assembly that produces this kind of logical forms can be easily implemented (with or without lambda calculus) in both phrase-based and dependency-based grammar formalisms. He had his car stolen. He had his house repossessed. He's had three books published. These are different from the causative have construction: e.g. (1) doesn't mean "He caused his car to be stolen". What this construction seems to do is line up the subject with an argument that could be called an "affectee", i.e. someone affected by the action; another term might be "beneficiary", but in the broad sense of a referent who is either advantaged or disadvantaged by the action. What are the semantics of this construction? What constraints are there on the semantic role of the subject and its relationship to the action of the verb? For example, one constraint seems to be that the subject should be aware of the action: He had his car stolen, but he didn't realize it sounds strange to me. Also, it seems like the action has to be agentive (though the agent is someone other than the subject): He had his key lost is obviously bad. When and how did this construction arise? Is it historically an extension of the causative have construction? What other languages have similar constructions, i.e. ones that line up the subject with an "affectee" or "beneficiary" argument, and what do these look like morphosyntactically? (The "benefactive pivot" construction of some Philippine languages seems to be one example.) where e is an eventuality (also called situation, possible event, or state of affairs). Such logical forms can express everything one can encounter in language (such as quantification and logical connectives) so there's no reason not to use them if it helps elsewhere. And help it does a lot in pragmatically interpreting discourse. Meaning assembly that produces this kind of logical forms can be easily implemented (with or without lambda calculus) in both phrase-based and dependency-based grammar formalisms. 

There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). 

In the lexicon, ||loves||=位x.位y.love(x,y) and ||obviously||=位P.obviously(P). On this view, syntactic composition is function application (hence the name "functionism" for this approach). 

Thus in glue semantic the meaning of both "John loves Mary" and "Mary John loves" can be assembled using the same rule though in the latter sentence it's the subject what is "attached" first. In syntactic frameworks with a context-free backbone, glue semantics (based on linear logic) is used at times, but it has problems, too. Some say that lambda calculus is unwieldy for implementing meaning assembly because it isn't monotonic. But it's a wrong approach to use lambda calculus at the level of surface syntax. (Linguistic) meaning is part of deep syntax, hence it should be assembled there. Deep syntax structures are unordered (or can be viewed as unordered for the purpose of semantic representation) and thus a 位-expression can refer to grammatical functions raher than the order in which syntactic structures are built up. In glue semantics (which uses linear logic) the meaning of "love" is taken to be