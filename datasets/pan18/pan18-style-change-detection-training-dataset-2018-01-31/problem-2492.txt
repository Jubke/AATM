It appears that the relative stability correlates with textual frequency (i.e. the more frequent the lexeme, the higher its stability), but this is the semantic side of the equation. I wonder if this can be true of the form, that is: (a) Is the form also generally preserved better in the stable part of the lexicon? And (b) is it thanks to the higher frequency? And also, if we "measured" the proportion of irregularities (appearing randomly, though not causelessly) within a Swadesh-like list, (c) would they be lower or higher than the ones found in a comparable list of culture-specific lexemes? What if we, however, unlike in the mainstream Mycenaean Greek reading, but like the fringe (pseudo-)Proto-Slavic reading, (1.) interpreted the vertical stroke not as a word delimiter, but a consonantal wildcard - one that is capable of representing up to a dozen different consonants. (2.) Consequently, word boundaries could be placed anywere in the string, between any two syllabograms. (3.) In addition to that, consonant clusters could no longer be resolved by means of consecutive rhyming syllabograms, and (4.) neither sonorants nor /s/ could be thought of in syllable codas. 

Fortunately, the body of information concerning sound laws and developments is constantly growing on both Wikipedia and Wiktionary - you might want to try these articles for starters: 

The closest linguistics (in its broadest sense) comes to this is a search for universals. There are many debates about what is and can be universal in linguistics - most of them happening in the realm of syntax (with some phonetics). But even the most formal of these universals (as represented by Universal Grammar) do not claim the status of a 'law' with the same predictive modeling power as laws of thermodynamics. Leonard Talmy outlines some universals in his chapter on Universals of Semantics but they are very much linked to broader universals of cognition or conversely more constrained combinatory universals from morphology. There are also many broad but not universal tendencies such as the use of space to model time, etc. Some of these were proposed by Emmon Bach in his reaction to Everett. While these do not relate to change, you could expect change to happen within these constraints. 

However, most of these are universal by virtue of being axiomatic because without them communication or reasoning would be impossible. For instance, if we found a language that did not use categories - we would find ourselves in an alien world beyond anything imaginable. What if we, however, unlike in the mainstream Mycenaean Greek reading, but like the fringe (pseudo-)Proto-Slavic reading, (1.) interpreted the vertical stroke not as a word delimiter, but a consonantal wildcard - one that is capable of representing up to a dozen different consonants. (2.) Consequently, word boundaries could be placed anywere in the string, between any two syllabograms. (3.) In addition to that, consonant clusters could no longer be resolved by means of consecutive rhyming syllabograms, and (4.) neither sonorants nor /s/ could be thought of in syllable codas. number of phonological combinations a particular string can represent their subset with respect to the given language's phonology and phonotactics Or, more simply, how many (phonotactically permissible) texts can be transliterated by a particular string of text using the script, i.e. how many (phonotactically persmissible) texts can a particular string of text written in the script represent (as compared to another script). It follows that we would always have to apply the tool on existing text and thus, in fact, measure ambiguity of reading a particular text written in a particular script as compared to the same text being writen in another script. 

Leonard Talmy outlines some universals in his chapter on Universals of Semantics but they are very much linked to broader universals of cognition or conversely more constrained combinatory universals from morphology. There are also many broad but not universal tendencies such as the use of space to model time, etc. Some of these were proposed by Emmon Bach in his reaction to Everett. While these do not relate to change, you could expect change to happen within these constraints. 

The 'laws' you propose have been formulated but they could never achieve the status of invariant principles let alone something which you could build predictive models on (despite computational attempts at such a thing). The principal problem with them is that they are formulated with the assumption that 'meaning' is something that 'words' have in the same way you see in the dictionary. But meaning cannot be described statically in this way. It is a process that includes a complex networks of usage patterns that are constantly shifting. They are sufficiently stable to allow for communication but not stable or discrete enough to enable prediction or even a foundation for some sort of a retrospective model. 

However, most of these are universal by virtue of being axiomatic because without them communication or reasoning would be impossible. For instance, if we found a language that did not use categories - we would find ourselves in an alien world beyond anything imaginable.