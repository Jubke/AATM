(There may also be rational reasons to not bother informing yourself on what is known, e.g. there's more pressing stuff to do.) When it comes to mathematical proofs, you start off knowing that you can't know empirically whether the Riemann hypothesis is true. So you can gather evidence about where it holds, and reason or experiment or analyze data based on that. But it's just science at that point--it's going the wrong way, in a sense, from a more reliable way to know things (in a very limited domain) to a less reliable one. Now, note what we did: we took multiple different predictions and they agreed with each other. We didn't take one and trust it, we checked. If some parameter changed in the past, it would somehow have to have affected these different factors by the same amount, so that they'd agree. The two situations are essentially the same, as far as you've described them. Because, as I began, the fundamental problem is pretty simple: the premises are false, and sometimes badly so, and it's just not that tricky of a philosophical issue when you're in that situation. Dating via radioisotopes is done much the same way. There are all sorts of radiogenic decay processes (for over a dozen, see Radiogenic Isotope Geology by Alan P. Dickin, Cambridge University Press (1995)) that agree with each other. So any "parameter change" would have to affect all radioactive decay the same way. Since decay of U235 and U238 is responsible for much of the internal heating of earth, if it had happened much much faster, the earth would have been a sphere of boiling lava unless thermal conductivity were also different. (And the sun wouldn't work properly either.) Furthermore, these methods agree with completely non-radioactive methods of relative time such as sediment accumulation, coral reef growth, sea-floor spreading, and so on. So not only can we perform scientific studies in the past, when it comes specifically to radioisotope dating, we can check and cross-check and cross-cross-check, and everything checks out. (To be perfectly clear: these are tests of the hypothesis!) Now, all measurements have some associated error, so if you want to tell to 0.1% how old something is, it can be quite a challenge. But if you want to tell if something is 5 million or 500 million years old it's really easy, and really robust (if you follow proper procedures and measure or avoid sources of error, as you will if you e.g. read Radiogenic Isotope Geology). This can be made precise and quantitative, but the bottom line is that it is correct mathematical reasoning. However, just because it is some evidence, it doesn't mean that it is conclusive or that it is sufficient to warrant changing your beliefs very much. The SEP article covers various modern responses. In particular, approaches inspired by Popper's approach to the scientific method (including the slightly more forgiving formulation by Lakatos) seem to indicate that economics is no science and should be treated with skepticism. It's not at all clear to me that this is wrong even if it is not acceptable to economists. As to what the difference is--well, goodness, where to begin! The architecture is utterly different--not so different that it's not Turing-computable, but everything from clocks to error-handling to distributed processing etc. etc. etc. is not done the same way in our brains. For some tasks which we do well, emulating what we do gives far better results (e.g. image recognition these days pretty much has given up on trying to do it the classic computer-science way and is just mimicking what our retina and visual cortex do). So the bottom line is really that we don't know what the key differences are because there are so many, but we have every reason to believe it's just a matter of different engineering, not something utterly fundamental. But the more telling blow to determinism is the success of entangled/superimposed states that are stochastically collapsed under certain conditions. The double-slit experiment is the most famous of these, but it's really Bell's inequality and experiments (that failed) to confirm it that made determinism look like a bad model of reality. The experiments are too technical and detailed to describe here, but so far Bell's inequality has been routinely violated, and hence, there is no room for a deterministic model where the relevant state stored locally. (Of course, a computer simulation with all state stored globally can reproduce anything, in principle, but that doesn't make it a parsimonious way to explain results in physics.) Counterfactuals are different yet again, having to do more with the relationship between models of reality and reality than any particular empirical study. It's not clear to me that the interesting cognitive science thing about counterfactuals is that they don't correspond to reality because nothing needs to correspond to reality in the brain (what is remarkable is that many things do!). And so it's not clear to me that there is even a phenomenon there that you're studying. The difference, undescribed, lies elsewhere. The theory of gravitation has immense predictive power--you can calculate times of eclipses, send spacecraft to Saturn, etc. etc.. This doesn't mean that every attractive force is gravity, though (e.g. magnets!). But a lot are, and the model fits observation extremely well. (So much so that people infer the presence of "dark matter"--something that has a gravitational effect but isn't visible--to explain things like galaxy shape. Dark matter is not as certain as gravitation, but gravitation works so well that dark matter needs to be very seriously entertained as a hypothesis.)