The usual way someone gets your email address if it isn't published in your profile is by correlating your account with another one that has the mail associated. If your user profile contains enough information to identify you, the mail address could then be taken from your Twitter or Facebook account or any other source that can be somehow connected to your account. Chat moderation is the complete opposite of that. A spam/offensive flag notifies every single 10k+ and chat moderator that is online at that moment. There are around 25-50 moderators logged into chat at all times, and the number of 10k chat users is likely even higher. Additionally, I think we should just scrap the badges for reviewing. They seem to cause more trouble than they're worth. This might even be enough alone, saving the effort of implementing other solutions. The result is that if flags are starting to be cast, you get a whole lot more users involved than you actually need to deal with the issue. A chat flag is a big huge blinking sign saying "Hey, here is some drama happening in chat. Get the popcorn and lets watch the trainwreck". While watching such drama can be occasionally entertaining, as long as you're not actually trying to do something about it, this does not really help deescalate the situation. Still, if a new platform arises that has the goal of producing "chickens" in the sense you mean it, being less focused on shallow reputation points, I'll be very interested to see it. The trusted user status gives them access to certain moderation tools that are closed to everyone else, even 20k+ users. (To me, the review queue should be one of them.) New moderation tools could become "trusted only" instead of 5k+, 10k+, 20k+ tools. (Migrating some existing privileges to "trusted only" would probably be too unfriendly an act to those who'd stand to lose it that way.) Suppose the discussion on Area 51 identifies Telling people to fetch and host the content privately and individually is a half-baked approach to the issue: for example, that was how it was done for the famous boat programming question, with the result that there seems to be no copy of it out there any more, because the privately hosted copies have gone down. That sucks. One way that might suck slightly less than all the others is a simple yes/no system. I'm entirely happy with the way rudeness is moderated on the SE network, and wouldn't want to see it change. I can not remember an instance where I felt a serious, important viewpoint was suppressed during the enforcing of politeness. Now, machines and algorithms can always be gamed. The best detectors of dedicated users who are "for real" would arguably be - you and me! The dedicated, veteran users. In our everyday interactions on SO and maybe Meta, we come across fellow users and, over time, reach some judgement of them and their actions on the site. I could probably name fifty users right away who I would absolutely trust using the review queue. I could also find fifty users who I would absolutely not trust using the review queue. There have been plenty of discussions about the new graduation mechanics, and the most controversial one is that the higher privilege levels that come with graduation are still coupled to the design. SE is also working on some new measures against spammers and trolls, I think it is reasonable to wait until those are implemented to see how effective they are. I think technical measures are better than legal ones for all but the most serious cases. They scale much better and act a lot faster. One important part would be to make the limits less sensitive to short bursts of activity, e.g. by defining the limit over longer times and not as a hard limit between to actions. For example, instead of one edit every 10 seconds, allow 6 edits every minute. This would stop bot-like behaviour of users still pretty quickly, but it would not be triggered by common behaviour. Additionally, there could be a large per-day cap to limit the damage any user can do. The script could take the time between reviews into account as well as a suspicious deviation from the average percentage of close/not close decisions. But that could be not enough information, requiring at least two reviewers for each item would be a way to collect more information about the accuracy of a review. Then the script could also take into account how often a review decision is overturned by two other users. I absolutely agree there is a dark side to the reputation system, influencing the way we post and word answers more deeply than is comfortable. No doubt. What the rep system fails to support is deep research on a subject, the exchanging of ideas, trying out different possibilities, developing new things, playing without the pressure of producing an answer that can be upvoted and accepted. I assume - forgive me if I'm wrong - that this goes into the direction you mean by "chickens." Yeah. And there's a lot more wrong with that ad than just that. I've thought about the same thing often. Answering lazy questions enables help vampire behaviour and is usually counter-productive. If their lazy request finds a good answer, they will come back with the next lazy request because they learned they can get away with it. Still, if a new platform arises that has the goal of producing "chickens" in the sense you mean it, being less focused on shallow reputation points, I'll be very interested to see it.