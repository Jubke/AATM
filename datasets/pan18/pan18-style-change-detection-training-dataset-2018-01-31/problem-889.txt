I do agree it's weird, and better give different color to the logo, but this might as well be by design. The fixed color that was defined for the logo in the beginning has been removed, so that's why we see the default link color. For example here it's blue, on gaming.SE it's dark blue, and on EL&U the color of the links is the red you see. Well, you can also remove it from within the box itself by clicking the highlighted star: See in the bottom of the Our Team page: I keep seeing the following scenario: Worth to mention that in the Welcome Message neither of them is linked to a profile. So the additional X is indeed not really required - I agree for its removal. 

Whenever I try to flag or close a question here, when clicking the submit button the button gets disabled, three dots animating and nothing really happens. Onebox can of course remove it as well, but not sure if it's worth the efforts. 

Black on black, I can see just a floating number - really confusing and out of any context. Update: appears to work fine from within the review system, but otherwise it's the same on Stack Overflow as well JS console show a single error: 

What can we do to encourage people to use the "@" more? It's for the good of the whole community, as people respond to comment, expecting reply but get none because nobody was notified. 

I don't want myself listed in that popup. Turns out it's happening only in Meta which means it's a bug, some change not yet deployed to all sites. In the name of Consistency, can we please have the same color on all sites? 

The top bar is showing (again) the suggested edits count and 10K flags count, like it was before. On Stack Overflow it looks like this: (that's the original color, same as in old top bar) Natalie does have a Stack Exchange account so the link is correct, however it looks like Valentina Perez doesn't have any account and for some reason it's linking to the previous employee. Please fix, programming questions and spam are beginning to pile up. :( The bug is whatever gave you that URL, if it was internal SE link and not added manually by someone. I just found by chance this behavior: Regex, it's always the regex. Seriously now, the reason is a rogue trailing slash you have in the URL, which is not supposed to be there. John Doe posts a question. The question is either not clear enough or missing some code. People post comments asking to clarify and/or post his code. The OP post comment back or edit his code and comment about it, but put no @namehere in the comment so those who commented never see it and the poor OP get no help. Personally if I post comment and not answer I usually add the question to my favorites thus "follow" it to see such comments, but looks like I'm the only one who is doing this. I also thought maybe I miscounted, but I just went through again and got the same number. EDIT 2: It seems that if ignored users leave a room for long enough to drop out of the "users in room" list and later come back, their avatars in the "people in room" list return to full size. According to the badges page, the requirements for Yearling are: The number of "votes" to have, whether to include the SO logo, and other details are subject to change. 

Using stackoverflow to determine which questions should go to which site after the split is a good idea, but I'm afraid the tag is applied too inconsistently for that to work out in practice. This is especially true for MSO's oldest questions: the ones that were asked on SO before MSO existed, using the tags sofaq or stackoverflow. Back then, questions about the network and questions about SO were basically the same thing (which, of course, is why we're in this situation to begin with). More recent questions are also affected. Those that are specific to SO, perhaps because they're about particular questions or users, often don't get tagged with stackoverflow. This is most likely because infrequent MSO users don't even know the tag exists. The reverse is also true. Many meta questions (both SO-specific and network-specific) get asked on SO and migrated here. Sometimes they bring along the stackoverflow tag; sometimes, the stackoverflow.com tag; sometimes both; sometimes neither. It may also be helpful to consider the tags specific-question and specific-user when deciding where to send questions. I'm not against "spring cleaning," but I can't think of a good way to automate it, and I doubt it would have enough value to justify the effort. As for which questions should be "spring cleaned," maybe we could start with the ones that ask why SO doesn't have a per-site meta and the ones that ask why MSO also does the job of MSE. Furthermore, the inaccuracy should be explained in the tooltip. Instead of showing ”~42 review items“, show “42 total pending reviews” — still reasonably short, but hints that it's not counting just reviews that you can do. 

None of the usual workarounds such as using the API or the data dump or Google are usable because many of my searches involve deleted questions. Hiding the information doesn't serve a useful purpose. If you feel that the number is not tied closely enough to the word “review”, reduce the space between them, and make “reviews” plural when there are 2 or more, so that the text smoothly reads smoothly “N reviews”. 

If there's good content in an old, off-topic question, I'd rather migrate it to a site that wants it than delete it. For a discussion on a meta site where I'm a moderator, I'm in the process of compiling a few statistics. Here's some data on suggested edit review speed. Methodology: I computed the average time between the submission and the approval or rejection of suggested edits over a 1-week period, or over 100 consecutive suggested edits on the few sites that received more than 100 per week. I measured three 1-week period: the week since the top bar went live (it's been a week today, except on MSO and AU; I excluded AU and took the date of the top bar introduction as a reference on MSO), the week before, and the week before. I chose weeks to avoid effects related to the time of day or day of week, and show two weeks before to get a small idea of the variance. For each period, the table below gives the average delay in minutes (rounded down) and the number of suggested edits that this averages over. The last column expresses the new-top-bar value as a percentage of the old-top-bar value (e.g. 100% means no change, 200% means that reviews take twice as long). I only looked at sites with at least 7.75 questions per day, because slower sites tend not to have a statistically significant sample of suggested edits. (7.75 is where the API throttled me out…) I then filtered out sites where there weren't at least 10 suggested edits during the weeks concerned.