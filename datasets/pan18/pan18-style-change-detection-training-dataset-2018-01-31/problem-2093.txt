At some point, good science becomes part of common sense and limits what will be reasonable to use or present in philosophy. But ideas that are of real value either get understood in context or get filtered for what sense they make, and what can be carried forward and re-expressed in less objectionable forms. 

The refinement of being scientific, which is gained by paradigm establishment, is meant to improve the efficiency of science, and I think that we can all agree that that really works. Modern sciences can move forward cleanly and efficiently. But efficient and effective are not the same thing. Besides, we know that the small, smooth things that allow us to perceive are metallic ions in nerve channels. To indicate that information is a real part of physics does not mean neurology is false. We know what yellow is, and it is not a wavelength of light, or any other single physical thing, because we can use mixtures of other colors to make it up. The idea that qualia encode wide ranges of physical causes in the same way prevents them from being physically represented by identifiable mediating particles. The information still needs to be mediated by all the effects we observe it working through, unless we decide God is prone to joshing us to a degree that is just plain mean. A lot of 'critical theory' people launch from there and simultaneously undermine or devalue all of our shared standards of aesthetics as well, and that leads to the notion of global anti-realism. But they cannot cast that whole debt back on Nietzsche, they generally need to pull in Marxism, psychoanalysis or other leverage to accomplish the more important half of the task. Nietzsche clearly evaluates past moralities as constructive or destructive, and expresses strong feelings about the goal of a life from 'One must make of one's Self a work of art' to 'Man is what must be overcome' and boundaries on action 'Fighting monsters we must not be made monsters'. The article puts the root of the question of AI with Descartes. The root of the Turing Test is Turing. It is a radical departure from all previous accounts of the problem, ignoring the philosophical roots applied to it by everyone before him. The Turing Test is a renunciation of the root this idea has in Descartes. So people are already doing meta-analysis, meta-ethics, meta-semiotics, meta-psychology... and they don't seem to feel that takes them out of their own branch of philosophy, much less into some separate place named meta-philosophy. It is awkward to snip out just the recursive parts of a given vertical, so we would have a hard time populating meta-philosophy that way. The result would not be a discipline, just a filter. 

When we re-injected the notion of atoms into physics to explain Brownian motion and thermodynamics, those 'new' ideas came from very old philosophy. And philosophy had developed, over time, many of the arguments that let physics make peace with this new perspective and integrate it. Atomists had existed all along, just banished from physics (sometimes literally, q.v. Boltzmann) 

What is left would we whole philosophies, in the systematic sense of Aristotle, Kant, Spinoza etc. that are themselves recursive, and include other philosophies in their model of the world in an explicit way. That would be an insane demand, as it would require an ethics that applies to semiotics as well as ontology, a psychology that applies epistemology and ethics... We have a tacit understanding that the internals of any given science are best left alone by philosophy, to the degree they are working and not causing damage. And we refer to only 'what is left over' after partitioning off science as 'philosophy per se'. But that is a polite convention, and not a fact. 

Two passes of induction will evidently kill your ability to define truth in your system. If you want a strong, consistent system, one is not enough, and two is too many.