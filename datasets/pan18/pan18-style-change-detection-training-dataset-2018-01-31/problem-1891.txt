The nature of "non empirical claims" in science is a thorny matter. It was only in the 19th century that philosophically inclined scientists and mathematicians started to appreciate the role that analytic statements play in science.1 

Consider the hypothesis-schema that all persons born under X are less likely to display Y than persons born under Z (H2, where X, Z are astrological signs and Y is a behaviour). Again, H2 is certainly scientific in the sense of (1), but it is not scientific in the sense of (2). But this time not because we have established that H2 is false (arguably nobody took the effort to test it), but because it lacks contact, as it were, with the body of actual scientific theories (which, incidentally, is the reason why nobody tested it). 

The aphorism is obviously an example of surreal humor - the ascription to Einstein is somewhat shaky. I'd say that it has really nothing to do with Ockham's Razor and therefore cannot even be called a bad formulation of it. 

Many claims about non-observable entities (unobservables) can, and indeed are, empirical. Other answers have already given more examples, so I won't go into more detail. 

He then suggests Cassirer as a starting point for reconciliation, the only major philosopher who wrote treatises on both mythical thought and general relativity. 

The thorniest issue, that both Peirce and Popper struggled with, was to reconcile the historical and cultural dependence of fallible knowledge with postulated mind independence of the "objective" reality that it is supposed to "approximate". And here their paths diverge. Some philosophers are very fond of this argument. Gendler uses it as a prototypical example of how "reasoning about particular entities within the context of an imaginary scenario can lead to rationally justified conclusions". Snooks goes further saying "it is striking that one leaves the falling balls example with something approaching certainty for its outcome". And Brown goes all the way and claims that Aristotle's theory is "self-contradictory", and we gain a priori knowledge here. The argument does give off that flavor of "synthetic a priori" reasoning, as in geometry but without images. But is it a proof or a fallacy? Even Gendler admits that some "obvious" premises are missing, and Atkinson even calls it a "non-sequitur" for similar reasons. But Galileo's logic is not questioned it seems. Shouldn't it be? 

But hand-wringing over "absolute" rigor is not the only concern. Hamming, whose mathematics was computer science oriented, is well known for quipping "typing is no substitute for thinking" and "the purpose of computing is insight, not numbers". One can take it pragmatically, or one can take it more fundamentally. Even pragmatically, simulations of the hurricane movements, say, may give us predictions, but not why they are thus and so. When models disagree we are left in the dark, because the crucial element of insight is missing. And on the Kantian/intuitionistic conception of mathematics and mathematical validity the chasm between human and computer assisted proofs is more than a matter of degree. That a deduction constitutes a proof as long as its every step is according to the "rules" is a formalist idea. But not all scientists or mathematicians are formalists. To some there is that elusive aspect of intuitive insight without which a proof is at best a blind calculation, with a stigma of inferiority attached. "Understanding" just does not take place outside of an understander, and a proof is not a proof until it is understood. Similar sensibilities affect reception of quantum mechanics, complaints about "shut up and calculate" are not uncommon.