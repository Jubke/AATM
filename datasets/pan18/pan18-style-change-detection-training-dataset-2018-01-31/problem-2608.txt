Edit It occurred to me that it probably means instead that the semantics of the control verb are responsible for deciding whether its subject or object is the subject of the subordinate clause. 

A verb is said to have selectional restrictions if there are certain well-defined properties that licit arguments must have. The verb massacre, for example, selects for a plural object. In semantic selection, the selected property must be semantic, and in studies on argument structure "semantic" usually means a role like agent, patient, instrument, etc., which are grammaticalized into grammatical relations such as subject, object, etc. In the Wikipedia article, what they are getting at is that the subject of the subordinate clause is either the subject or the object of the control verb. 

In the Role and Reference grammar framework, clause chaining is usually analyzed as a type of cosubordination (a type of clause linkage where two non-finite clauses are both embedded in a matrix clause). Cosubordination exists alongside the more familiar clause-linking prototypes coordination and subordination. Complementization is a type of subordination construction where the verb of a finite matrix clause takes as one of its arguments a subordinate clause. Situating clause-chaining and complementization within the more general typology of clause-linkage, we see that these are instantiations of general clause-linkage strategies, all of which are well-attested cross-linguistically. For a language to have both clause-chaining and complementization then should not be unusual or surprising, then. 

Assuming that syntactic analysis is more interested in functional rather than lexical aspects, it would be not implausible that in general, certain POS categories can be subsumed under one syntactic category label in order to caputure syntactic commonalities between different word classes, while at the same time it seems possible to create syntactic lables that do not have a direct equivalent in POS classes at all if such a new category label is syntactically well-motivated. 

None of these are phrases headed by a preposition, but nevertheless they serve a similar job as the examples mentioned by you: They modify a structure that is itself already a full sentence. 

Containedness is a relative term, i.e. you must always specify in which node something is (immediately) contained; w.r.t. to a whole syntax tree this notion can't apply, so the question "Which nodes are immediatly contained in a syntactic tree" doesn't really make sense if you don't say what nodes they are supposed to be immediately contained in. 

S as the root of a sentence is a rather old assumption that is rejected my most modern linguists and is only used for simplification when a detailled analysis of what a sentence constitutes is not relevant (e.g. when explaining how phrase structure grammars work in general, or in very simple "toy grammars" to demonstrate parsing algorithms in computer science/NLP). When not immediately related to natural language syntax, but for the description of formal languages (such as {anbn : n ∈ ℕ}, i.e. the language consisting of all possible sequences of aaaaa...bbbbb... where there are just as many a's as there are b's) with phrase structure grammars, which has its use mostly in computer science and mathematics, S is still the convention for the root label, but usually thought of as an abbreviation for "start symbol" rather than "sentence".