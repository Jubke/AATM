There are many resource, but a good informal one which accessible to mathematicians and non-mathematicians alike is Douglas Hofstadter's "I am a Strange Loop" Chapter 10 - Gödel's Quintessential Strange Loop. Indeed, this is the starting assumption that AI and machine learning specialists work off of: The mind is already performing such calculations, the challenge is to reproduce those processes in digital computers. 

The real question, is how much of these pattern recognition processes are innate and how much are acquired? That’s where Kant comes in, and where I am no longer qualified to discuss the topic. 

The mechanism he describes for consciousness and self perception is based purely on formal logic. There is no reason why this ability should be limited to biological brains. In chapter 17, he states that any symbol system isomorphic to the brain's higher symbolic levels should be able to implement (strong) AI. Quine, in his paper "Two Dogmas of Empiricism" questioned the analytic-synthetic distinction, and suggested that even analytic propositions were dependent on empirical evidence. Since the rules of logic were analytic propositions par excellence, they too, were ultimately dependent on empirical data, and were not absolute laws. Hilary Putnam discussed this in depth in his paper "Is Logic Empirical?", later republished as "The Logic of Quantum Mechanics.". In it he argued that, just as empirical physical results - relativity - forced us to abandon Euclidean geometry, so it is possible that the results of quantum mechanics will force us to abandon classical logic. As an after thought, one of my favorite Sci-fi short stories discusses the idea that while logic is indeed subjective, we learn classical logic at a very young age and once we grow into adults, we are incapable of unlearning it. If we were to somehow come across non-classical logics at a very young age, we would be capable of all sorts of superhuman feats. The story is of course, just sci-fi, but I do find the idea compelling. Actual infinities collected into sets were not officially contemplated by (philosophizing) mathematicians until Cantor (with some anticipation by Bolzano) countered Aristotelian and scholastic arguments that such objects are paradoxical, see How does actual infinity (of numbers or space) work? Ironically, Cantor rejected the infinitesimals themselves, see What was Cantor's philosophical reason for accepting the infinite but rejecting the infinitesimal? 

[...] It is not logic - I should like to say--that compels me to accept a proposition... when there are a million variables in the first two pairs of brackets and two million in the third. I want to say: logic would not compel me to accept any proposition at all in this case. Something else compels me to accept such a proposition as in accord with logic... I want to say: with the logic of Principia Mathematica it would be possible to justify an arithmetic in which 1000 + 1 = 1000; and all that would be necessary for this purpose would be to doubt the sensible correctness of calculations. But if we do not doubt it, then it is not our conviction of the truth of logic that is responsible." 

To give a concrete example, the strings of length 3: 0 heads = 1 string ({T, T, T}), 1 heads = 3 strings ({H, T, T}, {T, H, T}, {T, T, H}), 2 heads = 3 strings ({H, H, T}, {H, T, H}, {T, H, H}), 3 heads = 1 string ({H, H, H}). 8 total strings, each with a probability of occurring of 1/8. Thus, by addition, probability of 0 heads = 1/8, 1 heads = 3/8, 2 heads = 3/8, 3 heads = 1/8 

What makes Euclid's straight line interesting is that it is highly effective at making predictions about how our world works. Euclid claims the sum of the angles of a triangle add up to 180 degrees. In 2300 years, nobody has been able to make a triangle with a different sum of angles (without "cheating"), so we're pretty sure that Euclid's definition of straight lines is useful enough to warrant teaching children. 

This visualization can be done in many ways. One way is to look at all the different sequences of heads and tails that can occur. Clearly each sequence occurs with equal probability (with a fair coin). However, when you put these into "bins" based on how many heads you see, you find that there are many more sequences with an "average" number of heads than those which have extraordinary numbers of heads. This causes us to see average numbers more often than extraordinary numbers.