Some say that lambda calculus is unwieldy for implementing meaning assembly because it isn't monotonic. But it's a wrong approach to use lambda calculus at the level of surface syntax. (Linguistic) meaning is part of deep syntax, hence it should be assembled there. Deep syntax structures are unordered (or can be viewed as unordered for the purpose of semantic representation) and thus a 位-expression can refer to grammatical functions raher than the order in which syntactic structures are built up. In glue semantics (which uses linear logic) the meaning of "love" is taken to be Thus in glue semantic the meaning of both "John loves Mary" and "Mary John loves" can be assembled using the same rule though in the latter sentence it's the subject what is "attached" first. In the lexicon, ||loves||=位x.位y.love(x,y) and ||obviously||=位P.obviously(P). On this view, syntactic composition is function application (hence the name "functionism" for this approach). In syntactic frameworks with a context-free backbone, glue semantics (based on linear logic) is used at times, but it has problems, too. There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). where e is an eventuality (also called situation, possible event, or state of affairs). Such logical forms can express everything one can encounter in language (such as quantification and logical connectives) so there's no reason not to use them if it helps elsewhere. And help it does a lot in pragmatically interpreting discourse. Meaning assembly that produces this kind of logical forms can be easily implemented (with or without lambda calculus) in both phrase-based and dependency-based grammar formalisms. Under either approach to plurality, a singular and plural form have distinct denotations, so I don't think they would be considered the same word semantically (assuming that the notion of a semantic word would correspond to a denotation, although I don't know if that is what's usually done.) Ritter, E., & Rosen, S. T. (1997). The function of< i> have. Lingua, 101(3), 295-321. They argue that the introduction of additional arguments to a construction correlates with the introduction of additional event structure to a predicate. So since "have" systematically introduces a new argument, it also systematically introduces more event structure. It is unspecified, however, for what kind of extra event structure it contributes. It can thus be interpreted as extending the event beyond the original starting point of the event - in which case the introduced argument will be interpreted as the causer of the event. Or it can be interpreted as extending the event beyond the original ending point of the event - in which case the introduced argument will be interpreted as the experiencer/affectee of the event. (I tried to schematize this below in a sort of diagram.) As for other languages with similar predicates, they propose that the Japanese adversative -rare is similar to "have," but specified as extending the endpoint (hence the argument is always an experiencer/affectee), and that the French faire is similar to "have" but specified as extending the initial point (hence the argument is always a causer). There are also both benefactive and malefactive interpretations for some Salish applicatives (see, for example, Kiyosawa & Gerdts 2010. Some googling has also resulted in a book "Benefactives and malefactives: Typological perspectives and case studies" by Zuniga and Zeppo (2010). That might give you a good place to look for the crosslinguistic morphosyntactic properties of malefactives. STS is related to both Textual Entailment (TE) and Paraphrase, but differs in a number of ways and it is more directly applicable to a number of NLP tasks. STS is different from TE inasmuch as it assumes bidirectional graded equivalence between the pair of textual snippets. In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car. STS also differs from both TE and Paraphrase in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS is a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). This graded bidirectional nature of STS is useful for NLP tasks such as MT evaluation, information extraction, question answering, and summarization. But no one who made such models claimed that their model is precise. As far as I know. Yes, Shannon's model for entropy has no concept of meaning or even sequence. Each symbol (letter or word, in our context) is assumed to occur completely independent of another symbol. "Semantics" does not have just one, static meaning. For the context of entropy, we could use distributional semantics to predict the next word or phrase in the sequence "pass me the ______". Ngram models would only predict the list of objects that they have previously seen in the context of these words in their training corpus. Models based on word meaning in context could do better and predict a candidate from all the words or phrases that denote tangible objects that can be held with one's hand, even if they had not been seen in the context of "pass me" before. Given this, if you were to apply the object over the verb, and then apply the subject over the resultant, you'd have the man and the woman each visiting a garden, that may or may not be the same garden. However, if you were to apply the subject over the verb, and then apply the object over the resultant, you'd have them both visiting the same garden.