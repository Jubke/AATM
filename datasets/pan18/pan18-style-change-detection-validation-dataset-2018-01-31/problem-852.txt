Philosophical Zombies can have mental states and would still be Zombies. The whole point of Chalmer's Zombie thought experiment is to show that having mental states isn't enough to account for subjective/phenomenological first person experience. After all, computers have "mental states" - their internal memory states and software configurations - but don't have conscious experience. So Dennett claims that LOTH is descended from, but different than, the picture theory of ideas. If Dennett is correct, Ram's answer that the two theories addresses compleltely different questions doesn't hold. The two theories do address the same or similar issues. Thomas Nagel describes the problem in his paper "What's it like to be a bat" (Nagel, Thomas (1974). The Philosophical Review 83 (4): 435â€“450.): And yet those who subscribe to the computational theory of mind and those who support strong AI don't seem to draw a difference between knowing and understanding (Fodor's LOTH, the Turing test and more recent variations - such as having a computer being able to verbally relate the content of a video that it has just recorded). From such theories of the mind, it seems sufficient for there to be a proper mapping between propositions and facts from the outside world. What am I missing? How can Putnam's statements ("Functionalism + Turing reducibility of the mind" and "General AI is impossible") be reconciled? However, recently I have been examining the question of freewill, and I have come to the conclusion that there can be no freewill without the mind having a non physical component. Saying that the mind has a non physical component is essentially substance dualism. A purely materialist/physicalist account of how the mind works eliminates the possibility of freewill no matter how you try to interpret it. My question is the following: Is this indeed the case, that denying the possibility of strong AI implies substance dualism? But I don't see why it is an argument against physicalism in general. If anything it seems to me like the existence of qualia is a solid argument for the type identity theory of mind, (which is a more radically physicalist position than functionalism since it denies mental states any independent existence at all): knowledge about red is different from actually seeing red not because of any dualist mental substance, but because they correspond to different neurons firing in different parts of the brain. This would confirm type identity theory exactly: knowledge of red corresponds to one brain state and the sensation of red corresponds to another brain state. The sensation of seeing red, the actual qualia, is not multiply relizable, hence qualia are an argument for type-identity, and against functionalism, not in support of dualism. Here you are touching on the problem of freewill: A libertarian (one who believes in absolute freewill) would say that such an NPC didn't have freewill and therefore didn't qualify as truly sentient. A compatibilist, believes that agent has freewill as long as it is free to act according to its own internal motivations. In the case of your preprogrammed NPC, they have freewill according to compatibilists, since they are acting according to their own internal motivation, instead of being driven by outside forces. Whether, a compatibilist would make the next step of saying that your NPC is therefore sentient or not depends on who you ask. Daniel Dennett a well know compatibilist would say "yes they do", since he believes that strong AI and sentient robots are possible.