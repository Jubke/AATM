"Grounding" is an ostensibly stronger relation, which still falls short of a reduction. Since attempts to express it in terms of supervenience were unsuccessful Fine, Rosen and Schaffer recently proposed to treat it as a primitive instead. What "grounding" is supposed to express is that some states/processes "happen in virtue of" or "metaphysically depend on" others, and therefore are not "over and above" those others, which metaphysically "constitute" them. In application, grounding is supposed to complement statistical correlation and causal dependence as a mode of scientific explanation. Emergence is traditionally the opposite of reduction, higher level processes irreducible to lower level ones are called "emergent", grounding is consistent with epistemic emergence, emergence to us, true metaphysical emergence is excluded. 

According to Kant our empirical experience is synthesized from sensations through categories. Apparently, unconscious "productive ability of imagination" mediates the process using the schemes of space and time. Curiously, Kant's examples of this (mental) synthesis come from scientific reconstructions, such as Euclidean geometry or Newtonian mechanics. So it seems that Kant identifies mental synthesis of knowledge with its reconstruction in sciences, where indeed some mathematical structures are a priori necessary to make empirical claims meaningful (like notions of geometry and calculus in mechanics). 

Kripke's and Putnam's attempts to extend the causal theory to natural kinds (like wood, red, bee, etc.) remain controversial, see e.g. Ben-Yami's Semantics of Kind Terms, and even they did not attempt it for artificial kinds. Scientific units seem to involve aspects common to names, and both kinds. 

Underdetermination of theory by evidence, explored in great detail by Quine, means that from finitely many observations and measurements, that we are able to make by any point in time, even combined with perfect methodology, we are unable to determine a unique theory consistent with them. In other words, even if there were such a thing as the correct theory of nature we lack physical capabilities to find out what it is for sure, distinct theories may well be empirically equivalent. This is over and above the interpretational indeterminacy, where theories are "mathematically" equivalent despite postulating existence of different kinds of entities. For instance, Copenhagen and Bohmian interpretations of quantum mechanics are interpretationally distinct, but mathematically equivalent. They, or rather their natural extensions, may become inequivalent if in the future certain new types of measurements beyond quantum mechanics will become possible. These presumed extensions are only empirically equivalent for the time being. 

There are, I believe, a number of cases of distinct scientific theories that are equally good at explaining phenomena...until experimental evidence favors one over the other. 

Creationism per se cannot be construed as a science, normal or otherwise. The causal entity defined as an "intelligence" exceeding "human intelligence" simply cannot be falsified, experimentally demonstrated, or even adequately defined by its subsidiary "human intelligence" or the necessary self-limitations of science. The ID hypothesis can be endlessly pursued as "puzzle-solving," of course, as in Thomism. It may even be true. But I do not see that it can ever conform to what is meant by a "scientific" paradigm, even in the most relativistic interpretation of Kuhn. 

But this problem arises long before Messrs. Baudrillard and Derrida. Indeed, we could place it on the doorstep of Newton himself, who described "gravity" mathematically while refusing to say what it "really is." He simply sidestepped the metaphysical questions of "fundamental truth" and all working scientists followed him. The problems of "fundamental truth" continue famously through Descartes and Kant, who clears the field once and for all. Human knowledge is and must be "conditional" at some level, which we might call... well, the "human condition." Again, Kant intended his work partly as a defense of scientific knowledge, Newtonian mechanics in particular. Science can only progress in practice by accepting a probabilistic, conditional construction of knowledge that is freed from any requirement to demonstrate "fundamental truth," much as some scientists may say otherwise. 

The weakest link is the "additivity of cause" premise. It is true of Newtonian forces, but that part is empirical, and not in an intuitive way, unlike geometry. Quen_tin points out that two narrowly separated bodies falling differently than two touching ones leads to a counterintuitive discontinuity. That is true, but "touching" is not the same as "strapped together". The strapping introduces rigidity that turns two bodies into a single item, it is not a priori clear that the cause should move this item just as it would two bodies that are only touching. Aristotle would probably reject this premise, especially since to him falling is a "natural" motion rather than "forced". So his theory is not exactly self-contradictory or counterintuitive, but Galileo's argument is still valid under broader assumptions than just Newtonian physics. 

Quine explains it nicely in On What There Is:"Here we have two competing conceptual schemes, a phenomenalistic one and a physicalistic one... The physical conceptual scheme simplifies our account of experience because of the way myriad scattered sense events come to be associated with single so-called objects; still there is no likelihood that each sentence about physical objects can actually be translated, however deviously and complexly, into the phenomenalistic language... Viewed from within the phenomenalistic conceptual scheme, the ontologies of physical objects and mathematical objects are myths. The quality of myth, however, is relative; relative, in this case, to the epistemological point of view." The word "myth" here does not carry a negative connotation, work on mythology shows that since the dawn of civilization myths were instrumental in rationalizing and managing the world by human societies. 

You are right that Bell's inequalities do not rule out "superdeterminism" (Bell's term), as he himself acknowledged:"...if our measurements are not independently variable as we supposed...even if chosen by apparently free-willed physicists... then Einstein local causality can survive. But apparently separate parts of the world become deeply entangled, and our apparent free will is entangled with them". Bell's inequalities are instead conditional: if experimenters are free to choose which experiments they perform then the outcomes of those experiments as predicted by quantum mechanics (and confirmed in labs) are inconsistent with local realism. Testing determinism requires assuming indeterministic freedom. So at first glance the Bell's inference appears circular, as Brown puts it in Von Neumann's Postulate and Bellâ€™s Freedom:" He proved, assuming the predictions of quantum mechanics are valid (which the experimental evidence strongly supports), that not all events can be strictly consequences of their causal pasts, and in order to carry out this proof he found it necessary to introduce the assumption that not all events are strictly consequences of their causal pasts!"