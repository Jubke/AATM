I hope from all that I have explained above, it's clear that Shannon's definition of entropy was the beginning of the field of information theory, and not the end of it. In short, Shannon entropy sets a lower bound for a random sequence of symbols, but we know natural language is decidedly not a jumble of words. With better models, we get lower entropy figures. 

Update: The paper by Montemurro & Zanette (2011) answers your question a somewhat more directly. For English, they report an average entropy of 9.1 for shuffled text, and 5.7 for the original text. So, that shows you how much you gain by taking "grammar" (read "word order") into account. When I spoke to Prof. Pollard, who, I suppose, can be considered to be a paper-and-pencil linguist, I believe he said that the reason he did HPSG, and not the more logic-driven approach he is pursuing now (CVG) was simply that he did not know logic at the time. The slides for his course on such a formalism are at http://www.coli.uni-saarland.de/courses/logical-grammar/page.php?id=materials. In any case, doing derivations for sentences in such a grammar is exactly like doing derivations in mathematical logic. 

You can sign-up for the Stanford NLP class here. It is conducted by Jurafsky and Manning, authors of the extremely popular textbooks that the others mentioned. You can watch videos by leading researchers in the field at videolectures.net. I recommend starting with Clark's lecture and moving on to other lectures suggested in the section called "See Also". "Semantics" does not have just one, static meaning. For the context of entropy, we could use distributional semantics to predict the next word or phrase in the sequence "pass me the ______". Ngram models would only predict the list of objects that they have previously seen in the context of these words in their training corpus. Models based on word meaning in context could do better and predict a candidate from all the words or phrases that denote tangible objects that can be held with one's hand, even if they had not been seen in the context of "pass me" before. 

Perhaps Computational Psycholinguistics is part of Computational Linguistics, but not a part of Natural Language Processing. 

But no one who made such models claimed that their model is precise. As far as I know. 

I don't know what "perfect" would entail in this context. 

A note about Pollard's logical grammar: it addresses 3 aspects of sentences: phonology, syntax and semantics, all with exactly the same, logic-based methods. The terms are often used interchangeably. More often than not*, NLP is used when there is actual processing involved, rather than abstract theoretical topics. Comprehensive websites such as ACL wiki page write both separately, or combine them into CL/NLP or NLP/CL. Yes, Shannon's model for entropy has no concept of meaning or even sequence. Each symbol (letter or word, in our context) is assumed to occur completely independent of another symbol. You can find the published papers at S12 and S13, the relevant sections of the ACL anthology. 

The above equation has no conception of sequence. Later models that don't ignore the sequence, such as n-gram models and Markov models have shown significantly better entropy figures. 

There are several points here that I should address. I'll mention just one of the aspects I have seen myself. 

Bigrams, trigrams, N-grams are all models of language, just as grammar models used in parser are. As before, there is no point looking for a "precise" model because the real-world language is best handled with an open-world model. But my point here is that there are plenty of grammar models. And some of them could be quite usable for deriving entropy figures for text. Table 1 of Learning Accurate, Compact, and Interpretable Tree Annotation shows the different (specialized) classes of words that the algorithm has induced. NNP-14 are months, NNP-4 and NNP-5 are common terminal words of names of corporations, etc. So, an IN-5 is rather more likely to be followed by something that ends with NNP-4, and an IN-2 is more likely to be followed by NNP-14, and so on. After all, such grammar models contain the probabilities of the co-occurrences of words with these POS tags.