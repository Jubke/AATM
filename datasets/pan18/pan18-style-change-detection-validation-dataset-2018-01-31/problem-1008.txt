Anything that is computable algorithmically can be modelled as counting. That is a lot of mathematics. As I mentioned in my original answer (above), Cantor formalized the concept of counting by defining the ordinal numbers. The Continuum Hypothesis asks, what is the cardinality of the continuum. All cardinalities are defined as certain types of ordinal numbers. The cardinality of the continuum is given by the cardinality of the well-ordered set [0,1] ( = the set of real numbers between 0 and 1). So the continuum hypothesis is absolutely about counting. Is asks how many ordinals do I need to count in order to count the cardinality of the continuum. 

In practice, I do not believe that any unevenness in the coins composition - i.e., the coin having anything other than the ideal centre of gravity - would have any real effect on the predictability of the outcome of an individual toss or a sequence of tosses. This is because other factors, such as the force of the toss and the binary nature of the outcome, would overwhelm any bias attributable to unevenness. The outcome would be completely determined by the original orientation of the coin, the net initial forces exerted, and the characteristics of the landing surface. For any unevenness to play a role in determining the outcome, we would require a near-astronomical number of rotations of the coin, which, of course, is not the case in practice. "Euclid wrote The Elements" is a contingent truth. In some other possible world, Euclid herded goats and somebody else wrote the first modern math book. Whether you trust, and what you trust, is up to you. You drive over bridges. Sometimes the bridges fall down. Over the years we learn to make better bridges, never perfect bridges. You'd be foolish to trust all the experts all the time. But you'd be even more foolish to never leave the house for fear of a falling bridge. Someone the other day asked the difference between rationality and logic. Rationality is what lets you drive over a bridge that you know might fall down, even though you can never personally investigate every nut, bolt, and corrupt government contract. 

There is no need for countable additivity to define the sum of a convergent infinite series. Perhaps the OP can supply more context for this obviously incorrect quotation. 

Summary: Each theorem T of Euclidean geometry is neither true nor false by itself. If you give me an interpretation, I'll tell you if it's true or false. So if you say that the median of the ordered set ..., -3, -2, -1, 0, 1, 2, 3, ... is zero, you're right. And if you say the median is 47, or -119, you're still right! Every number is the median, by the definition of median. You mentioned Riemann integration. That's another good example, because in higher math the Riemann integral is no longer used. There's a more general theory called Lebesgue integration which behaves better. That doesn't mean that we were wrong to "trust" Riemann. Trust really has nothing to do with it. We live in the world as it is. 

Continuing like this, we see that the odds of getting the sequence HTTHTTHTHT are 1/1024. 

So when we assert T, we are really making the assertion "T is a theorem of E" where E represents the axioms of Euclidean geometry. "E proves T" is a true statement; and moreover, it's a necessary truth, because it follows directly from logic. In fact you could write a computer program that can verify the validity of the derivation from E to T. Theorem checking is algorithmic procedure. [Theorem finding, especially the finding of interesting theorems, requires a human.] 

Quine, in his paper "Two Dogmas of Empiricism" questioned the analytic-synthetic distinction, and suggested that even analytic propositions were dependent on empirical evidence. Since the rules of logic were analytic propositions par excellence, they too, were ultimately dependent on empirical data, and were not absolute laws. 

An order relation based definition of God is the basis for the ontological argument: 

As an after thought, one of my favorite Sci-fi short stories discusses the idea that while logic is indeed subjective, we learn classical logic at a very young age and once we grow into adults, we are incapable of unlearning it. If we were to somehow come across non-classical logics at a very young age, we would be capable of all sorts of superhuman feats. The story is of course, just sci-fi, but I do find the idea compelling. 

To compliment Conifold's answer, here's another way to look at it: Statements about number theory always end up being statements in number theory as well. Take any number theoretical theorem and replace the symbols with numbers using a suitable encoding, and you end up with an equation. 

Let's restate the two expressions as I have been tempted to ask this, but my lack of maturity has made me reluctant since it may be a rather sophomoric point. The question is: where does one draw the line? If both p and p' are known for n, but p' is unknown for n+1, what's so special about n. 

Therefore, it is not correct to say that "all" of mathematics is of questionable consistency. 

In practice, I do not believe that any unevenness in the coins composition - i.e., the coin having anything other than the ideal centre of gravity - would have any real effect on the predictability of the outcome of an individual toss or a sequence of tosses. This is because other factors, such as the force of the toss and the binary nature of the outcome, would overwhelm any bias attributable to unevenness. The outcome would be completely determined by the original orientation of the coin, the net initial forces exerted, and the characteristics of the landing surface. For any unevenness to play a role in determining the outcome, we would require a near-astronomical number of rotations of the coin, which, of course, is not the case in practice.