But I suspect that these algorithms will have trouble with pictures of certain shrubs and bushes, just like a human would. The official botanical definition, "a woody plant at least 5 metres high, with a main stem the lower part of which is usually unbranched", does not strike me as particularly sharp either. Sufficiently trained neuro-net will probably classify Pluto differently than Earth and Mars, but it is hard to see the distinction between planets and dwarf planets as sharp or written into the stars. To the extent that we can take it as evidence for philosophical positions the success of fuzzy methods in pattern recognition would rather indicate that Wittgenstein was right: our classifications rely on multiple resemblances and similarity measures, involve arbitrary choices and produce blurred results. "Sharp boundaries" are mostly degenerations or idealizations. EDIT 2: Found this paper that analyzes Galileo's argument in detail, and reaches the same conclusion. As Dupré in Metaphysical Disorder sarcastically remarked about supervenience, "if this dependency is not to be wholly mysterious, there is presumably some set of facts that could be known that would permit the inference of the macroscopic from a sufficient knowledge of the microscopic. Perhaps we could not, even in principle, know these facts. But God, I suppose, would need merely to exist in order to know them". Grounding is replacing this relation with essentially reduction, only we release ourselves from having to demonstrate it explicitly because it takes place metaphysically "in reality" rather than semantically "in theory". This has a ring of "advantages of theft over honest toil", in Russell's words. It remains to be seen if grounding can offer more than that. But many conventional criticisms give me an impression of missing the point. Does one really need to believe in reality made of math, like Tegmark, to embrace unification, with or without new predictions? At the time of Galileo experimental results were at a premium, and theoretical speculations in the absence of empirical constraints were plenty. Today the situation is nearly reversed, experimental methodologies are plenty and routine, we have far more data than we can process, let alone organize and put to use. Experiments in modern chemistry, for example, are little more than analog substitutes for quantum mechanical computations that we are unable to perform efficiently, no one seeks to falsify QM there. The vast empirical input condensed in the Standard Model (SM) and General Relativity (GR) is far more constraining on any unifying theory than any new test can possibly be. This is the same conclusion that Wittgenstein and others arrived at. Wittgenstein started out as holding views very similar to Carnap and the Logical Positivists: Logical statements and empirical statements are the only meaningful statements. But then he abandoned those views and in his later philosophy subscribed to the view that meaning is use: Statements derive their meaning not from the way correspond to empirical facts, but from the way we use them. The term "unlucky" is used in a certain way when we communicate and that is enough to give it meaning. See Ordinary Language Philosophy and Pragmatism. There are some caveats: When I spoke to the researchers working on this in 2009, it was still a work in progress - no actual cures or solutions had been discovered using this method yet. For possibility (2) one can start from James and Russell's neutral monism. From what I understand, Russell arrived at this position from the idea that relations are more fundamental than substance. For those that hold that qualia exist, the problem it poses is that of one hand being something whose existence is certain, and the other hand being something that can't submit to any empirical (and therefore scientific) analysis. Qualia are non-amenable to empirical investigation because a scientist can never adopt the first person perspective of different person. Thomas Nagel describes the problem in his paper "What's it like to be a bat" (Nagel, Thomas (1974). The Philosophical Review 83 (4): 435–450.): David Chalmers takes the reasoning you describe and flips on its head: Levels of complexity can never account for the purely ontological nature of consciousness (his famous "hard-problem of consciousness"), and therefore if physicalism is true, then consciousness must be fundamental, i.e. it has to be a basic property of matter like electrical charge or mass. Otherwise some for of dualism is true. I suppose this what you are alluding to in point (1). Finally I might suggest that identity is not a trivial characteristic; is the candle the "same" object after it has melted into a lump of wax? Perhaps, but even so it has undergone some kind of transformation. Since at an energetic level everything is effectively continuously transforming, identity is as you suggest -- a common notion, an axiom, but not something reflective on an underlying truth. But perhaps the opposite is true as well, at least once we detach the ought from axiomatic moral judgment; maybe this could be explored in terms of Hume's empiricism and the inseparable gulf between an is and an ought. This can also perhaps be read alongside Nietzsche's doubts about ways of life that denounce existence in favor of something else. In other words: there is also an ethical relation of obligation that is independent of moral axioms and the logic of judgment and punishment; a positive order of joy and levity, characterized by an empirical investigation into ways of living, thinking and feeling -- this is ethics in the classical sense as an art of living. You are mixing two issues here: There are some caveats: When I spoke to the researchers working on this in 2009, it was still a work in progress - no actual cures or solutions had been discovered using this method yet. Would a reductionist idealist claim that everything reduces to psychology? Kuhn also adresses this in his book. Per Kuhn, science has multiple phases (see section 3 of this answer for details), and one of the main phases is the puzzle solving phase, when researchers just spending their time confirming already existing theories by solving minor puzzles, instead of truly questioning them and coming up with new ones. This was a question that was brought most famously by T. Kuhn in his book "The Structure of Scientific Revolutions". Based on the previously mentioned problem of underdetermination, Kuhn argued that theories are never fully proven or disproven by experiments. Instead various sociological considerations go into which science theories are accepted and which are not. For example, the switch from classical mechanics to quantum mechanics and relativistic mechanics in the first half of the 20th century was due to new experimental results, but also to the fact that a younger generation of physicists and college professors was willing to accept the new theories, whereas the older generation was committed to the previous theories for various reasons. Some schools have gone further and claimed that all of science is socially constructed (i.e. that is, science is similar to religion or to political ideologies) - see the Strong Programme.