Phenomenology has a narrow meaning in contemporary philosophy as a style of philosophical inquiry originated by Husserl, and I do think that it is particularly congenial to a mathematician. Husserl worked as Weierstrass's assistant in his youth, and later personally knew and corresponded with Cantor, Hilbert, Courant, Minkowski, and other major mathematicians of his time (he worked in in GÃ¶ttingen until 1916), see On Husserl's Mathematical Apprenticeship and Philosophy of Mathematics. 

EDIT 2: Here is Friedman's diagnosis of the philosophical split. 

See SEP article on underdetermination of theory by evidence, and Worall's defense of structural realism. 

The thorniest issue, that both Peirce and Popper struggled with, was to reconcile the historical and cultural dependence of fallible knowledge with postulated mind independence of the "objective" reality that it is supposed to "approximate". And here their paths diverge. 

These are historical examples of the Gettier problem of knowledge as justified true belief (JTB), when a person is justified in her belief, and the belief is true, but the justification relies on elements that are false. To put it plainly, a person believes the right stuff for the wrong reasons, but this "wrongness" is external to justification. Officially, Gettier knowledge is not knowledge, but "X knew that p" is colloquially interpreted as "X believed that q, which is translated into modern terms as p, and comes out as true", otherwise if X still believed that q then "X mistakenly believed that p". This sounds lovely, and was very popular for a while, but as it came under criticism, a new idea from Milton Friedman (yes, the famous economist) gained hold, which is that you should see how accurate specific predictions are not worry too much about false assumptions. Also sounds lovely, but it's basically a dodge (we're wrong; who cares?) and inadequate because you want not just to get little easy-to-measure predictions right, but also make sure your whole economy doesn't tank (i.e. you want some assurance of whether big rare events are being made more or less likely). So while I am not sufficiently familiar with the tenets of naturalized epistemology to be sure about what they say, the answer from those fields where naturalized epistemology is supposed to draw inspiration is "no". 

Secondly, we can identify things like a cold spot as existing, which actually have less energy than their surroundings; or like a printed triangle, which has the same energy as many other configurations of ink on paper. So although you need mass and energy as a substrate, it is convenient to make distinctions about what exists on the basis of no difference in energy or a reduction of energy. These sorts of distinctions tend to be richer and more complicated than whether or not there is any matter at all, and so the insight that no matter and no energy is nothing does not get one very far. Whether or not this leaves room for some manner of divine being, all the details of what people claim about it/them are probably wrong, so the rational thing to do is to act and reason the same way you would if there wasn't any divine being, hence pragmatic atheism. Let's ask a different question: how do you know you're not twenty minutes old? That is, "you" didn't actually write the question, you just think you did because things were different in the past such that, well, somehow or other you exist now and have memories of writing a post, but there are parameters which changed and somehow spoiled stuff. Because of this, the closest thing to a scientifically objective morality is something like this: things are good to the extent that they maximize the chances for indefinite survival of human life (if not possible, fall back to other life in proportion to how closely related it is); things are bad to the extent to that they jeopardize it. 

This does not mean one should take this as a serious criticism any more than one should take the Evil Demon as a serious argument that all empirical knowledge is bankrupt. It would be very difficult, for instance, to survive as a human without any concept of something akin to "fall" (or "object", or "ground"). 

As far as I can tell, the anti-Evil Demon manoeuvre (which basically involves the pragmatic decision to ignore the argument) hasn't been sufficiently vigorously explored with regards to logical positivism. Note that empirical scientists, to great success, have basically taken this pragmatic approach; Kuhn* notwithstanding, issues of theory-laden perspective even in case of revolution basically don't come up in the harder sciences because of how strongly constrained the entire theoretical framework is by all the data we've collected. 

There's no doubt that science is a wonderful tool for providing us with information about many things. That it would have a lot to say about well being of humans is unsurprising. 

The Uncertainty Principle is not directly problematic for determinism; it just says you can't measure your states that accurately. You could always assume that the states were there, but you just couldn't measure them. Einstein preferred this view, and together with Podolsky and Rosen devised a paradox that would show that uncertainty is not fundamental. Unfortunately for Einstein, the experiments delivered the seemingly paradoxical result, showing that uncertainty is fundamental and determinism, if true, is not local. (Actually, it even shows that causality is not local.) 

As Dorfman points out below, its not a given that underlying reality can be expressed in mathematical form. Verlinde, inventor of Entropic Gravity has stated same in an interview. (I'd provide a link but I forget where I saw it). This wasn't true when philosophy first began. It arose out of mythology, and the two worlds intertwined for some time. As an example, Empedocles & Parmenides wrote their philosophy in verse (as did mathematicians). The question then is what kind of logic does this quantum phase space support. Von Neumann & Birkhodd showed that this is non-distributive lattice, and by interpreting the join as or, and the meet as and they had a logic which they christened Quantum Logic. Its main drawbacks is that its difficult to interpret as a logic because of the failure of distributivity: Formally he interprets Quantum Theory (solely in its mathematics) as a probability calculus based on a non-classical lattice. (Technically speaking it is measure theory generalised from sigma-algebras to ortho-algebras). This lattice is not distributive & is ortho-complemented, whereas the classical boolean lattice is distributive and complemented.