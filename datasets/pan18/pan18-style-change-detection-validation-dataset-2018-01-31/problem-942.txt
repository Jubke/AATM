Added. To avoid any misunderstanding due to language: By "science" I mean "natural science" in my answer. Obviously in mathematics the only method is to prove the conjecture - or to disprove it by generating a counter example. But in science one cannot prove general results. A finite number of confirmed cases does not increase the probability that the general result is true. That's the problem of induction. You do not give your definition of ontological relativism. But one can denote Kant’s position as ontological relativism. On the opposite, ontological relativism in the sense of Quine would be irrelevant for your question. Ad 3 and 4. I do not know a representative of this type of model. Probably today's main border is between the humanities and natural science. But interdisciplinary work becomes more and more important. E.g. in the domain of cognitive science, some researchers vote for cooperation even across this border. Determinism and randomness play a double role in Quantum Mechanics (QM): The fundamental equations of QM like Schroedinger or Dirac equation are differential equations similar to the differential equations of classical mechanics. And differential equations are the paradigms of a deterministic development. Underdetermination of theory by evidence, explored in great detail by Quine, means that from finitely many observations and measurements, that we are able to make by any point in time, even combined with perfect methodology, we are unable to determine a unique theory consistent with them. In other words, even if there were such a thing as the correct theory of nature we lack physical capabilities to find out what it is for sure, distinct theories may well be empirically equivalent. This is over and above the interpretational indeterminacy, where theories are "mathematically" equivalent despite postulating existence of different kinds of entities. For instance, Copenhagen and Bohmian interpretations of quantum mechanics are interpretationally distinct, but mathematically equivalent. They, or rather their natural extensions, may become inequivalent if in the future certain new types of measurements beyond quantum mechanics will become possible. These presumed extensions are only empirically equivalent for the time being. Dummett's original position is expressed in Reality of the Past, and is simply that all we can mean by claims about the past reduces to organizing traces of it in the present, or rather of what can be usefully treated as "traces of the past". Under his moderate version of anti-realism, justificationism, we do not commit to "reality" of such claims, but neither do we deny it. For instance, consider "king James II had migraine on the day of his 32 birthday". If there is a record of it somewhere then we know how to understand it, but what if there is not? What does it mean exactly? That it "really" did happen out there? But if we can have no access to out there in principle it is questionable that we "really" understand what that means. How then can it be confirmed or refuted? It would seem that we are implicitly imagining "in principle" something like a time machine. What about "Hilbert had discalculia"? Discalculia is a learning disorder that was only identified in 1974, Hilbert passed away in 1943. Even more than a time machine would be needed to make sense of this one. Note that some of the physicists who elaborate quantum theory participate to the Vienna circle (logical positivist philosophy group) which was more or less influenced by neo-kantian philosophy. Logical empiricist's attempts to reduce theoretical knowledge to an observation vocabulary (phenomenism, operationalism) is clearly anti-realist. EDIT I think it is problematic to define lines as functions (y=ax+b) because a coordinate system is not a geometric space. It has an origin, a length unit and preferred directions, which geometric spaces lack. One should rather see a coordinate system as a way to assign sets of numbers to geometrical points and a function is a way to define a sets of numbers that can correspond to lines but all this already requires other concepts, so the right way to define a line is by using purely geometric axioms. Concerning the third one, the idea of experimental philosophy is that when it comes to questions about the meaning of concepts involved in metaphysics (for example, what free will is, or moral issues), one should question folk usage to have an answer. There are indeed debates on whether experimental philosophy is just psychology rather than philosophy and on whether the whole approach is relevant. You can read the SEP entry on experimental moral philosophy for more information, in particular this section: https://plato.stanford.edu/entries/experimental-moral/#WhaCouExpOppPsy Certainty is the personal feeling to have true knowledge. It is a subjectice term and does not guarantee truth. Ad 2. In a religious the model is always combined with a personal creator. The model has been packed up by Christian mythology. In cosmology it is one of the favoured models since the investigation of the accelerated cosmological expansion. The latter is ascribed to the “dark energy”. Kant does not consider synthetical knowledge a priori the principle of induction. It was just the aim of Metaphysical Foundations of Natural Science to show which insights can be derived without using induction. To answer your question: It is not human progress which reduces individuals to Nietzsche's last men. The reason is that the LAST MAN Du Bois concludes: But as regards the enigma what matter and force are and how they are able to conceive, he [the investigator of Nature] must resign himself once for all to the far more difficult confession - "IGNORABIMUS!" If you use induction to create conjectures, e.g. the Riemann Hypothesis, that's quite OK. Every method is allowed to create conjectures. Using induction to create conjectures is the method of generalisation.