If you choose to entertain a mathematical universe with more than one element, this doesn't mean that the notion of the singular evaporates, or is subsumed into the notion of "first". The role of "singular" and "first" play significantly different roles in the formulation of mathematics, even in universes where there is more than one object. Qualities such as "singular" apply to elements of sets, whereas "first" applies to items in sequences, which we may construe as functions from the positive integers to a set. For instance, consider the sequence p = (2, 3, 5, 7, 11, ...) of the primes in order, such as one may construct by a sieve technique. In this, the first element is p1 = 2; however, this is not to say that 2 is the one and only prime number. Similarly, one may consider a singleton set {3}, and the (rather boring) sequences one can define over it, e.g. (3, 3, 3, 3, 3). Here, the unique element 3 is the first item in the sequence — and also the second, third, fourth, and fifth. More advanced technologies are often more sophisticated, in that they are more difficult to use. This is not always the case: an alphabetic writing system is arguably more advanced (because more flexible) than hieroglyphs, and likely to be simpler in some meaningful ways of measuring. But most of the time, when making distinctions between advanced technologies, we have in mind something closer to the American specially and pains-takingly designed space pen, versus the pencil until recently favoured by Russian cosmonauts. This example involves two devices which at least are very similar in their usage, but require different levels of infrastructure first to develop, and subsequently to support as a routine tool. And like the space-pen versus the pencil, there may be genuine reasons (a wish to avoid free-floating graphite particles) to adopt the more sophisticated technology over the less sophisticated technology. However, technologism tends to drive people to adopt sophisticated technologies without any motivation based in personal utility.       ω2 := 0 ∪ ω ∪ 2ω ∪ 3ω ∪ 4ω ∪ ... The reason why this is important is that they aren't capturing any difference of size at all — after ω, all of the ordinals that I've described to you thus far have exactly the same number of elements, when we describe the size of a set by cardinality. The ordinal numbers are clearly capturing a lot of information about structure, in that there are many limit ordinals which only come after infinitely long sequences of successive iterations, e.g. as 8ω2+3ω comes only after the entire sequence 8ω2+2ω+1, 8ω2+2ω+2, 8ω2+2ω+3, ... but if you allow yourself to consider ways of matching up elements in a way which does not preserve the order of the elements, you can match the elements of all of these "polynomials" of ω with each other. Something even more provocative happens for N = n(n-1)/4 (so that each pair of points is connected with probability 1/2), and take the limit as n goes to infinity. This is known as "the" infinite random graph; and the reason why it is called "the" infinite random graph is that although different pairs of points are connected depending on which edges are chosen to be in the graph, with probability 1 the outcome will be a particular graph up to relabeling of the names of the points. That is, the outcome is just as much the same graph as all circles of fixed radius 1 are in some sense representations of "the same circle". And so it turns out that a random process, infinitely extended, gives rise to what one might call a deterministic outcome. Yet there are other infinite graphs which you could consider which are not the same as "the" infinite random graph: but the probability of realising them by this process is zero, because it would require a conspiracy of infinitely many events which have some probability of failing. Is there anything contingent about the structure of this graph, then? Much is known about it (see the linked article above), but it comes about as if by an inevitable accumulation of accidents. And even though it is "the" infinite random graph, whether or not any particular realization of it (in the construction process) joins two particular points by an edge is still a random event with probability 1/2. We are speaking essentially of a single mathematical object whose representation is contingent. Distributed complexity burden and the complexity arms race. If you order the finite whole numbers, what you get is a well-order. So, those who like to explore foundations of mathematics through set theory usually use the same construction to build both the finite ordinals and the finite cardinals. You may see some people describing cardinals by 1, 2, 3, ... and distinguishing them from ordinals by writing the ordinals as 1st, 2nd, 3rd, etc.; however, the usual mathematical construction for both is to define Contingency apart from axioms Consider for example random graph theory. In their seminal paper on the subject Paul Erdős and Alfred Rényi (Erdős+Rényi 1960, "On the evolution of random graphs") consider a process where one connects n abstract 'points' or 'vertices' in pairs by edges, selecting up to some number N of pairs to connect, uniformly at random and in a random order. The description of "what is going on" is presented in highly suggestive time-dependent language. (The very idea of the fact that there is something which is "going on", as opposed to just simply being statically the case, is already a hint of this.) The word 'evolution' in the title is meant literally, for example: they speak of the graph changing with time, of connected components "melting" into one another, and so forth.