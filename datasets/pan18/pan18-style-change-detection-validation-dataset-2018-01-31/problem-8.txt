9:15 Setup meeting room and screen-cast recorders, dial-in external stakeholders 9:30 Start review sessions per team Start evaluation meeting after a short break. Lunch Start planning sessions 17:00 Have a beer and snacks :) 

I think management is worried about hours, because they pay developers per hour. Every hour is extra money. Explain how Scrum gives developers more focus and that its harder to side-track on non important things. Senior: Assist management to create structural and cultural changes in the organisation to become more Agile. Leads the path through Agile Fluency. 

TL;DR: Scrum doesn't provide for a lot of things, you need to figure out a way that works for your team and project. 

To many cooks? It's a team, they are the cooks. Teach the team something about collective code ownership as an Agile principle. Now you can use the velocity metric for the whole team effort, including testing. 

Still velocity is defined as the number of points you achieve in a Sprint. Just sum them. Then it is wise to average velocity of the last three Sprints, something Jeff Sutherland calls Yesterday's Weather. During the Sprint our teams starts each story with an "architecture and design" task, this is a session which results in a design and technical sub-tasks. Including design, coding, but also testing tasks. This is the moment the actual detailed design becomes clear based on the higher-level acceptance criteria noted down in the Sprint planning. After this session we swarm to complete the user story as quick as possible. The sessions length depend on complexity of the story at hand. I would say this is a dev-team group session, but sometimes if the complexity is low a single person could prepare everything with a short introduction to the rest of the team. The definition of done is to see if the feature is ready to be deployed to production and only for programming team members. The definition could include that the feature has tests, documentation is updated, release notes are written, version control is cleaned and anything your team needs todo to make it able to release this feature. Some also call it DoneDone as in really done. 

Example Definition of Done from the Scrum Shock Therapy: I think the different levels can relate to the Agile Onion as described by Simon Powers. 

The Scrum-team should be self-organising. After careful deliberate thought and experiments the team should come to a point in time that best fits their process. Therefor I suggest they experiment with different times and see which one nets the best results for the team. The retrospective could be a good meeting to discuss if the current way of breaking down stories is optimal. https://www.infoq.com/news/2013/02/swarming-agile-teams-deliver The take-a-way of knowing about Hawthorne, bias, and stochastic observations is about knowing how to apply a healthy skeptism of what you are observing. In other words, stick to the null hypothesis until proven otherwise. If you make a change and you observe some favorable results, the finding you walk away with is, 'this seemed to cause that,' not 'this caused that.' 

EDIT to answer question in comments: 

"Benefits" is very subjective. The answers provided here will likely be more of opinion, just like my answer above. Your final decision is really going to be based on what you are seeing in your domain in your geographical area. If you choose to get it, your cost is time to study, maybe a training class, and the cost of the test. It won't make you less valuable in the market place so I think if you can afford those costs the question becomes: why not? Like a teaching hospital. A patient or his insurance company will not get billed for the fifteen physicians doing rounds. 

I assume the surveys will yield a score each macro area, say like from 1 to 9, 9 being very high interest. I'd attach an ordinal scale to your t-shirt size estimates, 1 through 5. To make things relative, I'd take the user score for each macro area and divide it by the total user scores and do the same with the size estimates. Then I would simply divide the relative user score with the relative work size. This will bubble up the priority of those macro areas that score high for the user and will discriminate between two high scoring macro areas with different size work estimates where the lower work estimate will bubble up higher.