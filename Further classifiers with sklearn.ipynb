{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Marius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.externals.joblib import Memory\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import FEATURE_SELECTOR_v4\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_filter_params(var_word_n_grams=(True, 'absolute', [5000, 500, 500]),\n",
    "                     var_char_n_grams=(True, 'absolute', [500, 500, 500, 500]),\n",
    "                     var_pos_n_grams=(True, 'absolute', [500, 500, 500]),\n",
    "                     freq_word_n_grams=(True, 'absolute', [2000, 2000, 2000]),\n",
    "                     freq_char_n_grams=(True, 'absolute', [2000, 2000, 2000, 2000]),\n",
    "                     freq_pos_n_grams=(True, 'absolute', [2000, 2000, 2000])):\n",
    "\n",
    "    return {\n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # word_n_grams])\n",
    "        'var_word_n_grams': var_word_n_grams,      \n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # char_n_grams])\n",
    "        'var_char_n_grams':var_char_n_grams,   \n",
    "        # Filter by variance: (flag, Type_of_filter, [thresholds for each n in\n",
    "        # pos_n_grams])\n",
    "        'var_pos_n_grams':var_pos_n_grams,       \n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in word_n_grams])\n",
    "        'freq_word_n_grams':freq_word_n_grams,\n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in char_n_grams])\n",
    "        'freq_char_n_grams':freq_char_n_grams,           \n",
    "        # Use the most common features: (flag, Type_of_filter, [thresholds for each n\n",
    "        # in pos_n_grams])\n",
    "        'freq_pos_n_grams':freq_pos_n_grams\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_features_to_calc(char_n_grams= [1,2,3,4],        \n",
    "    word_n_grams = [1,2,3],                                          \n",
    "    pos_n_grams=[1,2,3],                      \n",
    "    avg_sent_len=True,                         \n",
    "    avg_word_len=True,                         \n",
    "    token_per_sent=True,                      \n",
    "    vocabulary_richness=True):\n",
    "\n",
    "    return{\n",
    "        # list with n for char_n_grams\n",
    "        'char_n_grams': char_n_grams,        \n",
    "        # list with n for word_n_grams\n",
    "        'word_n_grams': word_n_grams,                                          \n",
    "        # list with n for pos_n_grams\n",
    "        'pos_n_grams': pos_n_grams,                      \n",
    "        # flag if average sentence length should be calculated\n",
    "        'avg_sent_len':avg_sent_len,                         \n",
    "        # flag if average token length should be calculated\n",
    "        'avg_word_len':avg_word_len,                         \n",
    "        # flag if token per sentences should be calculated\n",
    "        'token_per_sent':avg_word_len,                      \n",
    "        # flag if vocabulary_richness should be calculated\n",
    "        'vocabulary_richness':vocabulary_richness,                 \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_token_params_1 (\n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+'),      \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle'), \n",
    "    get_tokens = 'sentence',                                                    \n",
    "    word_strain = 'stem',\n",
    "    filter_length = ('long', 0),                                                \n",
    "    handle_stopwords = '',                                                      \n",
    "    get_sentences = False,                                                      \n",
    "    punctuation = \"[,;.!—]\",                                                   \n",
    "    lower_stop_words = set(stopwords.words('english')),                         \n",
    "    uncapitalized = True):\n",
    "    \n",
    "    \n",
    "    return{\n",
    "    # Word tokenizer\n",
    "    'word_tokenizer':RegexpTokenizer(r'\\w+'),  \n",
    "    # Sentence tokenizer    \n",
    "    'sent_tokenizer':nltk.data.load('tokenizers/punkt/english.pickle'), \n",
    "    # Group tokens: 'text'|'sentence'\n",
    "    'get_tokens':'sentence',                                                    \n",
    "    # 'lemma'|'stem'\n",
    "    'word_strain':'stem',\n",
    "    # Get tokens with length >= <int> or <= <int>: ('long',<int>)|('short',<int>)\n",
    "    'filter_length':('long', 0),                                                \n",
    "    # 'get'|'remove'\n",
    "    'handle_stopwords':'',                                                      \n",
    "    # Sentences tokens\n",
    "    'get_sentences':False,                                                      \n",
    "    # Remove punctuation\n",
    "    'punctuation':\"[,;.!—]\",                                                   \n",
    "    # Stopwords\n",
    "    'lower_stop_words':set(stopwords.words('english')),                         \n",
    "    # Original or uncapitalized stemms/lemmas\n",
    "    'uncapitalized' : True                                                      \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11039"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Path to data\n",
    "path_data='.//datasets//constructed_2.csv'\n",
    "\n",
    "df_texts=pd.read_csv(path_data,sep=',',header=0, index_col=0)\n",
    "serie_texts=df_texts.text\n",
    "\n",
    "df_texts.text.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_tokenizer': RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), 'sent_tokenizer': <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x00000268A9AAC2B0>, 'get_tokens': 'sentence', 'word_strain': 'stem', 'filter_length': ('long', 0), 'handle_stopwords': '', 'get_sentences': False, 'punctuation': '[,;.!—]', 'lower_stop_words': {'any', \"needn't\", 'd', 'to', 'below', \"didn't\", 'couldn', 'mightn', 'yourself', 'were', 'can', 'ma', \"wasn't\", 'an', 'herself', \"you'd\", 'the', 'yours', 'does', 'we', 'what', 'didn', 'after', \"don't\", 'so', 'now', \"aren't\", 'hers', 'her', \"shan't\", 'had', 'weren', 'once', \"you're\", 'wasn', 'as', 'by', 'shan', 'or', 'their', 'under', \"it's\", 'doing', 'be', 's', 'will', \"hasn't\", 'and', 'with', 'me', \"that'll\", 'won', 'all', 'too', 'it', 'have', 'than', 'my', \"weren't\", 'himself', 'theirs', 'until', 'isn', 'she', 'why', \"mustn't\", 'while', 'you', 'don', 'up', 'against', 'has', 'll', \"haven't\", 'down', 'very', 'your', 'this', 'are', 'should', \"isn't\", 'ours', 'been', 'through', 'myself', 'is', 'few', 're', \"doesn't\", 'more', 'there', 'wouldn', 'hadn', 'own', 'i', 'aren', 'itself', 'again', 'out', 'they', 'here', 'him', 'before', 'nor', 'about', 'into', 'was', 'no', 'm', 'each', \"shouldn't\", 'haven', 'during', 'these', 'o', \"mightn't\", 'between', 'above', 'on', 'ain', 'did', \"won't\", 'just', 'whom', 'where', 't', 've', \"wouldn't\", 'his', 'further', 'both', 'ourselves', 'needn', 'a', 'from', 'most', 'for', 'hasn', \"you've\", 'its', 'shouldn', 'how', 'our', 'being', 'he', 'not', 'only', 'them', 'that', 'such', 'but', 'over', 'when', \"couldn't\", 'off', 'in', \"you'll\", 'other', 'having', 'of', 'mustn', 'y', 'do', 'if', \"she's\", 'themselves', \"should've\", 'am', 'those', 'because', 'who', 'same', 'doesn', \"hadn't\", 'which', 'yourselves', 'then', 'at', 'some'}, 'uncapitalized': True}\n",
      "{'char_n_grams': [1, 2, 3, 4], 'word_n_grams': [1, 2, 3], 'pos_n_grams': [1, 2, 3], 'avg_sent_len': True, 'avg_word_len': True, 'token_per_sent': True, 'vocabulary_richness': True}\n",
      "{'var_word_n_grams': (False,), 'var_char_n_grams': (False, 'absolute', []), 'var_pos_n_grams': (False,), 'freq_word_n_grams': (False,), 'freq_char_n_grams': (False,), 'freq_pos_n_grams': (False,)}\n"
     ]
    }
   ],
   "source": [
    "token_params_1 = set_token_params_1 ()\n",
    "features_to_calc=set_features_to_calc()\n",
    "filter_params=set_filter_params(var_word_n_grams=(False,),\n",
    "                     var_char_n_grams=(False, 'absolute', []),\n",
    "                     var_pos_n_grams=(False, ),\n",
    "                     freq_word_n_grams=(False, ),\n",
    "                     freq_char_n_grams=(False,),\n",
    "                     freq_pos_n_grams=(False,))\n",
    "print(token_params_1)\n",
    "print(features_to_calc)\n",
    "print(filter_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//Features//test_features2019_01_03_14_14.txt\n"
     ]
    }
   ],
   "source": [
    "# Export Path\n",
    "path_selected_features='.//Features//test_features{0}.txt'.format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "print(path_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Das System kann die angegebene Datei nicht finden: 'C:\\\\Users\\\\Marius/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d4c3bfc93395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpath_to_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_selected_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# TFIDF / Text_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     normalization_type='')\n\u001b[0m",
      "\u001b[1;32mE:\\Marius\\Documents\\Studium\\3_Semester\\Seminar Text Mining\\Repository\\AATM\\FEATURE_SELECTOR_v4.py\u001b[0m in \u001b[0;36mselect_features\u001b[1;34m(serie_texts, features_to_calc, token_params_1, filter_params, path_to_features, flag_extract_features, cv_min_df, normalization_type)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# Get pos-tags grouped by sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     series_pos_grouped_by_sentences = series_original_word_tokens_grouped_by_sentences.apply(\n\u001b[1;32m--> 218\u001b[1;33m         lambda row: [nltk.pos_tag(sent) for sent in row])\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Get pos-tags grouped by texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mseries_pos_grouped_by_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_pos_grouped_by_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mE:\\Marius\\Documents\\Studium\\3_Semester\\Seminar Text Mining\\Repository\\AATM\\FEATURE_SELECTOR_v4.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# Get pos-tags grouped by sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     series_pos_grouped_by_sentences = series_original_word_tokens_grouped_by_sentences.apply(\n\u001b[1;32m--> 218\u001b[1;33m         lambda row: [nltk.pos_tag(sent) for sent in row])\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Get pos-tags grouped by texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mseries_pos_grouped_by_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_pos_grouped_by_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Marius\\Documents\\Studium\\3_Semester\\Seminar Text Mining\\Repository\\AATM\\FEATURE_SELECTOR_v4.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# Get pos-tags grouped by sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     series_pos_grouped_by_sentences = series_original_word_tokens_grouped_by_sentences.apply(\n\u001b[1;32m--> 218\u001b[1;33m         lambda row: [nltk.pos_tag(sent) for sent in row])\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Get pos-tags grouped by texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mseries_pos_grouped_by_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_pos_grouped_by_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             AP_MODEL_LOC = 'file:' + str(\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             )\n\u001b[0;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_features = FEATURE_SELECTOR_v4.select_features(\n",
    "    # Contents\n",
    "    serie_texts=serie_texts,                          \n",
    "    # Dictionary with the features which should be calculated\n",
    "    features_to_calc=features_to_calc, \n",
    "    # Dictionary with the parameters for tokenizing the contents\n",
    "    token_params_1=token_params_1,              \n",
    "    # Dictionary with the parameters for filtering the n_grams\n",
    "    filter_params=filter_params,\n",
    "    # Threshold for tokens (only token which occure in more documents than in 20%)\n",
    "    cv_min_df = 0.1,                           \n",
    "    # Flag if the features should be selected or extracted\n",
    "    flag_extract_features=False,\n",
    "    # Path to already selected features       \n",
    "    path_to_features=path_selected_features,\n",
    "    # TFIDF / Text_length\n",
    "    normalization_type='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected features\n",
    "with open(path_selected_features, 'w') as f:\n",
    "    json.dump(selected_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_n_grams_1 ['a', 'abandon', 'abl', 'about', 'accept', 'across', 'act', 'action', 'actual', 'after', 'again', 'against', 'age', 'agre', 'all', 'allow', 'almost', 'alon', 'along', 'alreadi', 'also', 'although', 'american', 'among', 'an', 'and', 'ani', 'anoth', 'appar', 'appear', 'are', 'armi', 'around', 'arriv', 'as', 'ask', 'assist', 'at', 'attack', 'attempt', 'author', 'away', 'back', 'base', 'battl', 'be', 'beauti', 'becaus', 'becom', 'been', 'befor', 'begin', 'behind', 'believ', 'best', 'between', 'black', 'bodi', 'book', 'both', 'boy', 'break', 'bring', 'brother', 'brought', 'build', 'but', 'by', 'call', 'can', 'cannot', 'captur', 'care', 'carri', 'case', 'caus', 'chang', 'charact', 'child', 'children', 'citi', 'claim', 'close', 'come', 'command', 'commit', 'commun', 'complet', 'concern', 'confront', 'consid', 'contact', 'continu', 'control', 'convinc', 'could', 'countri', 'creat', 'danger', 'dark', 'daughter', 'day', 'dead', 'deal', 'death', 'decid', 'describ', 'despit', 'destroy', 'determin', 'develop', 'did', 'die', 'differ', 'disappear', 'discov', 'discuss', 'do', 'doe', 'doesn', 'down', 'dream', 'drive', 'due', 'dure', 'each', 'earli', 'earth', 'encount', 'end', 'enough', 'enter', 'entir', 'escap', 'even', 'event', 'eventu', 'ever', 'everi', 'everyon', 'exist', 'experi', 'explain', 'eye', 'face', 'fact', 'fail', 'fall', 'famili', 'far', 'father', 'fear', 'feel', 'few', 'fight', 'final', 'find', 'fire', 'first', 'five', 'flee', 'follow', 'for', 'forc', 'form', 'former', 'found', 'four', 'free', 'friend', 'from', 'full', 'further', 'futur', 'gener', 'get', 'girl', 'give', 'given', 'go', 'goe', 'good', 'govern', 'great', 'group', 'grow', 'ha', 'had', 'half', 'hand', 'happen', 'have', 'he', 'head', 'hear', 'heart', 'help', 'her', 'herself', 'hi', 'hide', 'high', 'him', 'himself', 'histori', 'hold', 'home', 'hope', 'hous', 'how', 'howev', 'human', 'husband', 'i', 'if', 'immedi', 'in', 'includ', 'inform', 'initi', 'insid', 'instead', 'interest', 'into', 'introduc', 'investig', 'involv', 'is', 'it', 'job', 'join', 'journey', 'just', 'keep', 'kill', 'king', 'know', 'known', 'land', 'larg', 'last', 'late', 'later', 'law', 'lead', 'leader', 'learn', 'leav', 'led', 'left', 'let', 'letter', 'lie', 'life', 'like', 'littl', 'live', 'local', 'locat', 'london', 'long', 'look', 'lose', 'lost', 'love', 'made', 'main', 'make', 'man', 'manag', 'mani', 'marri', 'marriag', 'may', 'mean', 'meanwhil', 'meet', 'member', 'men', 'mind', 'miss', 'money', 'month', 'more', 'most', 'mother', 'move', 'mr', 'much', 'murder', 'must', 'mysteri', 'name', 'natur', 'near', 'need', 'never', 'new', 'next', 'night', 'no', 'not', 'noth', 'novel', 'now', 'number', 'of', 'off', 'offer', 'offic', 'old', 'on', 'onc', 'one', 'onli', 'open', 'or', 'order', 'origin', 'other', 'out', 'outsid', 'over', 'own', 'parent', 'part', 'parti', 'pass', 'past', 'peopl', 'person', 'place', 'plan', 'play', 'plot', 'point', 'polic', 'polit', 'possibl', 'power', 'prepar', 'present', 'prison', 'problem', 'promis', 'protect', 'provid', 'put', 'question', 'quickli', 'rather', 'reach', 'read', 'real', 'realiz', 'reason', 'receiv', 'refer', 'refus', 'relationship', 'releas', 'remain', 'rescu', 'respons', 'rest', 'result', 'return', 'reveal', 'right', 'room', 'run', 's', 'same', 'save', 'say', 'school', 'search', 'second', 'secret', 'see', 'seek', 'seem', 'seen', 'self', 'send', 'sent', 'seri', 'set', 'sever', 'share', 'she', 'ship', 'short', 'should', 'show', 'side', 'sinc', 'sister', 'situat', 'small', 'so', 'societi', 'some', 'someth', 'son', 'soon', 'speak', 'spend', 'stand', 'start', 'state', 'stay', 'still', 'stop', 'stori', 'strang', 'struggl', 'success', 'such', 'suffer', 'suggest', 'support', 'surviv', 'suspect', 't', 'take', 'taken', 'talk', 'tell', 'than', 'that', 'the', 'their', 'them', 'themselv', 'then', 'there', 'these', 'they', 'thi', 'thing', 'think', 'third', 'those', 'though', 'thought', 'threaten', 'three', 'through', 'thu', 'time', 'to', 'togeth', 'told', 'too', 'toward', 'town', 'train', 'travel', 'tri', 'true', 'turn', 'two', 'unabl', 'under', 'unit', 'until', 'up', 'upon', 'use', 'variou', 'veri', 'visit', 'wa', 'wait', 'walk', 'want', 'war', 'warn', 'watch', 'way', 'well', 'were', 'what', 'when', 'where', 'whi', 'which', 'while', 'who', 'whom', 'whose', 'wife', 'will', 'wish', 'with', 'within', 'without', 'woman', 'women', 'word', 'work', 'world', 'would', 'write', 'year', 'yet', 'young']\n",
      "word_n_grams_2 ['a few', 'a man', 'a new', 'a small', 'a young', 'abl to', 'about the', 'after a', 'after the', 'against the', 'agre to', 'all of', 'all the', 'along with', 'and a', 'and ha', 'and he', 'and her', 'and hi', 'and in', 'and is', 'and it', 'and she', 'and that', 'and the', 'and their', 'and then', 'and they', 'and to', 'as a', 'as an', 'as he', 'as the', 'as they', 'as well', 'at a', 'at the', 'attempt to', 'back to', 'be a', 'be the', 'becaus of', 'becom a', 'befor the', 'begin to', 'believ that', 'between the', 'but he', 'but is', 'but the', 'by a', 'by hi', 'by the', 'come to', 'death of', 'decid to', 'discov that', 'doe not', 'doesn t', 'due to', 'dure the', 'each other', 'end of', 'find a', 'find out', 'find the', 'follow the', 'for a', 'for her', 'for hi', 'for him', 'for the', 'forc to', 'from a', 'from her', 'from hi', 'from the', 'go to', 'goe to', 'group of', 'ha a', 'ha been', 'had been', 'have a', 'have been', 'he and', 'he can', 'he find', 'he ha', 'he had', 'he is', 'he wa', 'he will', 'her and', 'her father', 'her to', 'hi father', 'hi friend', 'hi life', 'hi mother', 'hi own', 'hi wife', 'him and', 'him in', 'him that', 'him the', 'him to', 'in a', 'in an', 'in her', 'in hi', 'in love', 'in order', 'in the', 'in their', 'in thi', 'in which', 'into a', 'into the', 'is a', 'is also', 'is an', 'is in', 'is not', 'is the', 'is to', 'it is', 'it to', 'it wa', 'known as', 'learn that', 'leav the', 'live in', 'love with', 'make a', 'manag to', 'member of', 'most of', 'not to', 'of a', 'of an', 'of her', 'of hi', 'of the', 'of their', 'of them', 'of thi', 'on a', 'on her', 'on hi', 'on the', 'one of', 'order to', 'out of', 'out to', 'over the', 'part of', 'plan to', 'realiz that', 'refus to', 'return to', 'reveal that', 's death', 's father', 'seem to', 'she ha', 'she is', 'she wa', 'so that', 'some of', 'stori of', 'take the', 'tell him', 'tell the', 'that he', 'that hi', 'that is', 'that it', 'that she', 'that the', 'that they', 'the book', 'the citi', 'the death', 'the end', 'the famili', 'the first', 'the hous', 'the last', 'the new', 'the next', 'the novel', 'the onli', 'the other', 'the rest', 'the same', 'the second', 'the stori', 'the time', 'the two', 'the way', 'the world', 'them to', 'there is', 'they are', 'they have', 'thi is', 'through the', 'to a', 'to be', 'to do', 'to escap', 'to find', 'to get', 'to go', 'to have', 'to help', 'to her', 'to hi', 'to him', 'to kill', 'to leav', 'to make', 'to return', 'to save', 'to see', 'to take', 'to the', 'to their', 'tri to', 'turn out', 'unabl to', 'under the', 'up in', 'up the', 'up to', 'up with', 'use the', 'wa a', 'want to', 'way to', 'well as', 'when he', 'when she', 'when the', 'when they', 'where he', 'where the', 'where they', 'which he', 'which is', 'who ha', 'who had', 'who is', 'who wa', 'will be', 'with a', 'with her', 'with hi', 'with him', 'with the', 'year old']\n",
      "word_n_grams_3 ['as well as', 'end of the', 'in love with', 'in order to', 'of the book', 'of the novel', 'one of the', 'out of the', 'return to the', 'that he is', 'the end of', 'to be a']\n",
      "char_n_grams_1 [' ', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '’']\n",
      "char_n_grams_2 [' \"', \" '\", ' (', ' -', ' 1', ' 2', ' A', ' B', ' C', ' D', ' E', ' F', ' G', ' H', ' I', ' J', ' K', ' L', ' M', ' N', ' O', ' P', ' Q', ' R', ' S', ' T', ' U', ' V', ' W', ' Y', ' Z', ' a', ' b', ' c', ' d', ' e', ' f', ' g', ' h', ' i', ' j', ' k', ' l', ' m', ' n', ' o', ' p', ' q', ' r', ' s', ' t', ' u', ' v', ' w', ' y', '\" ', '\",', '\".', '\"T', '\"t', \"' \", \"'s\", \"'t\", '(a', '(t', '(w', ') ', '),', ').', ', ', '- ', '-a', '-b', '-c', '-d', '-e', '-f', '-h', '-i', '-l', '-m', '-o', '-p', '-s', '-t', '-w', '-y', '. ', '.\"', '0 ', '00', '19', '20', ': ', '; ', 'A ', 'Ad', 'Af', 'Al', 'Am', 'An', 'Ar', 'As', 'At', 'Au', 'Ba', 'Be', 'Bi', 'Bl', 'Bo', 'Br', 'Bu', 'Ca', 'Ch', 'Ci', 'Cl', 'Co', 'Cr', 'Da', 'De', 'Di', 'Do', 'Dr', 'Du', 'Ea', 'El', 'Em', 'En', 'Ev', 'Fa', 'Fe', 'Fi', 'Fl', 'Fo', 'Fr', 'Ga', 'Ge', 'Go', 'Gr', 'Gu', 'Ha', 'He', 'Hi', 'Ho', 'Hu', 'I ', 'In', 'Is', 'It', 'Ja', 'Je', 'Jo', 'Ju', 'Ka', 'Ke', 'Ki', 'La', 'Le', 'Li', 'Lo', 'Lu', 'Ma', 'Me', 'Mi', 'Mo', 'Mr', 'Mu', 'Na', 'Ne', 'Ni', 'No', 'On', 'Or', 'Pa', 'Pe', 'Po', 'Pr', 'Qu', 'Ra', 'Re', 'Ri', 'Ro', 'Ru', 'Sa', 'Sc', 'Se', 'Sh', 'Si', 'So', 'Sp', 'St', 'Su', 'Ta', 'Te', 'Th', 'Ti', 'To', 'Tr', 'Un', 'Va', 'Vi', 'Wa', 'We', 'Wh', 'Wi', 'Wo', 'Yo', 'a ', \"a'\", 'a,', 'a.', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ap', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'b ', 'ba', 'bb', 'be', 'bi', 'bj', 'bl', 'bo', 'br', 'bs', 'bt', 'bu', 'by', 'c ', 'c,', 'c.', 'ca', 'cc', 'ce', 'ch', 'ci', 'ck', 'cl', 'co', 'cr', 'cs', 'ct', 'cu', 'cy', 'd ', \"d'\", 'd,', 'd-', 'd.', 'da', 'dd', 'de', 'dg', 'dh', 'di', 'dl', 'dm', 'dn', 'do', 'dr', 'ds', 'du', 'dv', 'dw', 'dy', 'e ', 'e\"', \"e'\", 'e)', 'e,', 'e-', 'e.', 'e:', 'e;', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'f ', 'f,', 'f-', 'f.', 'fa', 'fe', 'ff', 'fi', 'fl', 'fo', 'fr', 'ft', 'fu', 'fy', 'g ', 'g,', 'g.', 'ga', 'ge', 'gg', 'gh', 'gi', 'gl', 'gn', 'go', 'gr', 'gs', 'gt', 'gu', 'gy', 'h ', \"h'\", 'h,', 'h-', 'h.', 'ha', 'hb', 'he', 'hi', 'hl', 'hm', 'hn', 'ho', 'hr', 'hs', 'ht', 'hu', 'hy', 'i ', 'i,', 'i.', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ik', 'il', 'im', 'in', 'io', 'ip', 'ir', 'is', 'it', 'iu', 'iv', 'ix', 'iz', 'ja', 'je', 'jo', 'ju', 'k ', \"k'\", 'k,', 'k.', 'ka', 'ke', 'ki', 'kl', 'kn', 'ks', 'ky', 'l ', \"l'\", 'l,', 'l-', 'l.', 'la', 'lc', 'ld', 'le', 'lf', 'li', 'lk', 'll', 'lm', 'lo', 'lp', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'ly', 'm ', 'm,', 'm.', 'ma', 'mb', 'me', 'mi', 'mm', 'mo', 'mp', 'ms', 'mu', 'my', 'n ', 'n\"', \"n'\", 'n)', 'n,', 'n-', 'n.', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'ny', 'o ', \"o'\", 'o,', 'o-', 'o.', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'p ', 'p,', 'p.', 'pa', 'pe', 'ph', 'pi', 'pl', 'po', 'pp', 'pr', 'ps', 'pt', 'pu', 'py', 'qu', 'r ', \"r'\", 'r)', 'r,', 'r-', 'r.', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'ry', 's ', 's\"', \"s'\", 's)', 's,', 's.', 's:', 's;', 'sa', 'sb', 'sc', 'se', 'sf', 'sg', 'sh', 'si', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'ss', 'st', 'su', 'sw', 'sy', 't ', 't\"', \"t'\", 't,', 't-', 't.', 'ta', 'tc', 'te', 'th', 'ti', 'tl', 'tm', 'tn', 'to', 'tr', 'ts', 'tt', 'tu', 'tw', 'ty', 'u ', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'ui', 'ul', 'um', 'un', 'uo', 'up', 'ur', 'us', 'ut', 'uy', 'va', 've', 'vi', 'vo', 'w ', 'w,', 'w.', 'wa', 'we', 'wh', 'wi', 'wl', 'wn', 'wo', 'wr', 'ws', 'x ', 'xa', 'xc', 'xe', 'xi', 'xp', 'xt', 'xu', 'y ', 'y\"', \"y'\", 'y)', 'y,', 'y-', 'y.', 'ya', 'yc', 'ye', 'yi', 'yl', 'ym', 'yn', 'yo', 'yp', 'yr', 'ys', 'yt', 'za', 'ze', 'zi', '’s']\n",
      "char_n_grams_3 [' \"T', ' \"t', ' (a', ' (t', ' (w', ' - ', ' 19', ' A ', ' Af', ' Al', ' Am', ' An', ' Ar', ' As', ' At', ' Au', ' Ba', ' Be', ' Bi', ' Bl', ' Bo', ' Br', ' Bu', ' Ca', ' Ch', ' Ci', ' Cl', ' Co', ' Cr', ' Da', ' De', ' Di', ' Do', ' Dr', ' Du', ' Ea', ' El', ' Em', ' En', ' Ev', ' Fa', ' Fe', ' Fi', ' Fl', ' Fo', ' Fr', ' Ga', ' Ge', ' Go', ' Gr', ' Ha', ' He', ' Hi', ' Ho', ' Hu', ' In', ' Is', ' It', ' Ja', ' Je', ' Jo', ' Ju', ' Ka', ' Ke', ' Ki', ' La', ' Le', ' Li', ' Lo', ' Lu', ' Ma', ' Me', ' Mi', ' Mo', ' Mr', ' Mu', ' Na', ' Ne', ' Ni', ' No', ' On', ' Pa', ' Pe', ' Po', ' Pr', ' Qu', ' Ra', ' Re', ' Ri', ' Ro', ' Ru', ' Sa', ' Sc', ' Se', ' Sh', ' Si', ' So', ' Sp', ' St', ' Su', ' Ta', ' Te', ' Th', ' Ti', ' To', ' Tr', ' Un', ' Va', ' Vi', ' Wa', ' We', ' Wh', ' Wi', ' Wo', ' Yo', ' a ', ' ab', ' ac', ' ad', ' af', ' ag', ' ai', ' al', ' am', ' an', ' ap', ' ar', ' as', ' at', ' au', ' av', ' aw', ' ba', ' be', ' bi', ' bl', ' bo', ' br', ' bu', ' by', ' ca', ' ce', ' ch', ' ci', ' cl', ' co', ' cr', ' cu', ' da', ' de', ' di', ' do', ' dr', ' du', ' ea', ' ed', ' ef', ' ei', ' el', ' em', ' en', ' es', ' ev', ' ex', ' ey', ' fa', ' fe', ' fi', ' fl', ' fo', ' fr', ' fu', ' ga', ' ge', ' gi', ' gl', ' go', ' gr', ' gu', ' ha', ' he', ' hi', ' ho', ' hu', ' id', ' if', ' il', ' im', ' in', ' is', ' it', ' jo', ' ju', ' ke', ' ki', ' kn', ' la', ' le', ' li', ' lo', ' lu', ' ma', ' me', ' mi', ' mo', ' mu', ' my', ' na', ' ne', ' ni', ' no', ' nu', ' ob', ' oc', ' of', ' ol', ' on', ' op', ' or', ' ot', ' ou', ' ov', ' ow', ' pa', ' pe', ' ph', ' pi', ' pl', ' po', ' pr', ' pu', ' qu', ' ra', ' re', ' ri', ' ro', ' ru', ' sa', ' sc', ' se', ' sh', ' si', ' sk', ' sl', ' sm', ' sn', ' so', ' sp', ' st', ' su', ' sw', ' sy', ' ta', ' te', ' th', ' ti', ' to', ' tr', ' tu', ' tw', ' un', ' up', ' us', ' va', ' ve', ' vi', ' vo', ' wa', ' we', ' wh', ' wi', ' wo', ' wr', ' ye', ' yo', '\" a', '\", ', '\". ', \"'s \", \"'t \", '(th', '(wh', ') a', '), ', '). ', ', \"', ', A', ', B', ', C', ', D', ', E', ', F', ', G', ', H', ', J', ', K', ', L', ', M', ', N', ', P', ', R', ', S', ', T', ', a', ', b', ', c', ', d', ', e', ', f', ', g', ', h', ', i', ', k', ', l', ', m', ', n', ', o', ', p', ', r', ', s', ', t', ', u', ', w', '-ol', '-ye', '. A', '. B', '. C', '. D', '. E', '. F', '. G', '. H', '. I', '. J', '. K', '. L', '. M', '. N', '. O', '. P', '. R', '. S', '. T', '. U', '. W', '.\" ', ': t', '; a', '; h', '; t', 'Aft', 'All', 'Alt', 'Ame', 'And', 'As ', 'At ', 'Bri', 'But', 'Car', 'Cha', 'Chi', 'Chr', 'Col', 'Com', 'Con', 'Des', 'Dur', 'Ear', 'Eng', 'Eve', 'Fin', 'For', 'Fra', 'Fre', 'Gra', 'Gre', 'Har', 'He ', 'Her', 'His', 'How', 'In ', 'It ', 'Joh', 'Kin', 'Lat', 'Lon', 'Man', 'Mar', 'Mea', 'Mor', 'New', 'Nor', 'On ', 'One', 'Par', 'Pri', 'Pro', 'She', 'Sta', 'The', 'Thi', 'Tho', 'Thr', 'To ', 'Uni', 'War', 'Whe', 'Whi', 'Wil', 'Wit', 'Wor', 'a \"', 'a C', 'a S', 'a a', 'a b', 'a c', 'a d', 'a f', 'a g', 'a h', 'a i', 'a j', 'a k', 'a l', 'a m', 'a n', 'a o', 'a p', 'a r', 'a s', 'a t', 'a v', 'a w', 'a y', \"a's\", 'a, ', 'a. ', 'aba', 'abi', 'abl', 'abo', 'abs', 'acc', 'ace', 'ach', 'aci', 'ack', 'acr', 'act', 'ad ', 'ad,', 'ad.', 'ada', 'add', 'ade', 'adi', 'adl', 'adm', 'ado', 'ads', 'adu', 'adv', 'ady', 'afe', 'aff', 'aft', 'aga', 'age', 'agi', 'ago', 'agr', 'agu', 'aid', 'ail', 'aim', 'ain', 'air', 'ais', 'ait', 'ak ', 'ake', 'aki', 'aks', 'al ', 'al,', 'al.', 'ala', 'ale', 'alf', 'ali', 'alk', 'all', 'alm', 'alo', 'alr', 'als', 'alt', 'am ', 'ama', 'amb', 'ame', 'ami', 'amo', 'amp', 'ams', 'an ', \"an'\", 'an,', 'an.', 'ana', 'anc', 'and', 'ane', 'ang', 'ani', 'ank', 'ann', 'ano', 'ans', 'ant', 'anw', 'any', 'ap ', 'apa', 'ape', 'aph', 'api', 'apo', 'app', 'aps', 'apt', 'ar ', 'ar,', 'ar-', 'ar.', 'ara', 'arb', 'arc', 'ard', 'are', 'arg', 'ari', 'ark', 'arl', 'arm', 'arn', 'aro', 'arr', 'ars', 'art', 'ary', 'as ', 'as,', 'ase', 'ash', 'asi', 'ask', 'aso', 'ass', 'ast', 'at ', 'at,', 'ata', 'atc', 'ate', 'ath', 'ati', 'ato', 'atr', 'ats', 'att', 'atu', 'aug', 'aul', 'aun', 'aus', 'aut', 'ava', 'ave', 'avi', 'avo', 'aw ', 'awa', 'ay ', 'ay,', 'ay.', 'aye', 'ayi', 'ays', 'bab', 'bac', 'bad', 'bal', 'ban', 'bar', 'bas', 'bat', 'be ', 'bea', 'bec', 'bed', 'bee', 'bef', 'beg', 'beh', 'bei', 'bel', 'ber', 'bes', 'bet', 'bil', 'bin', 'bir', 'bit', 'bje', 'bla', 'ble', 'bli', 'blo', 'bly', 'boa', 'bod', 'boo', 'bor', 'bot', 'bou', 'boy', 'bra', 'bre', 'bri', 'bro', 'bs ', 'bse', 'bui', 'bur', 'bus', 'but', 'by ', 'c a', 'c s', 'c t', 'c, ', 'c. ', 'cal', 'cam', 'can', 'cap', 'car', 'cas', 'cat', 'cau', 'cce', 'cci', 'cco', 'ccu', 'ce ', 'ce,', 'ce.', 'cea', 'ced', 'cee', 'cei', 'cel', 'cen', 'cep', 'cer', 'ces', 'ch ', 'ch,', 'ch.', 'cha', 'che', 'chi', 'cho', 'cia', 'cid', 'cie', 'cil', 'cin', 'cio', 'cip', 'cis', 'cit', 'ck ', 'ck,', 'ck.', 'cke', 'cki', 'ckl', 'cks', 'cla', 'cle', 'cli', 'clo', 'clu', 'cog', 'col', 'com', 'con', 'cor', 'cou', 'cov', 'cra', 'cre', 'cri', 'cro', 'cru', 'cs ', 'ct ', 'ct,', 'cta', 'cte', 'cti', 'cto', 'cts', 'ctu', 'cue', 'cul', 'cum', 'cur', 'cus', 'cut', 'cy ', 'd \"', 'd A', 'd B', 'd C', 'd D', 'd E', 'd F', 'd G', 'd H', 'd J', 'd K', 'd L', 'd M', 'd N', 'd P', 'd R', 'd S', 'd T', 'd W', 'd a', 'd b', 'd c', 'd d', 'd e', 'd f', 'd g', 'd h', 'd i', 'd j', 'd k', 'd l', 'd m', 'd n', 'd o', 'd p', 'd r', 'd s', 'd t', 'd u', 'd v', 'd w', \"d's\", 'd, ', 'd. ', 'da ', 'dam', 'dan', 'dar', 'dat', 'dau', 'day', 'dde', 'ddi', 'ddl', 'de ', 'de,', 'de.', 'dea', 'dec', 'ded', 'dee', 'def', 'del', 'dem', 'den', 'dep', 'der', 'des', 'det', 'dev', 'dge', 'dia', 'dic', 'did', 'die', 'dif', 'din', 'dir', 'dis', 'dit', 'div', 'dle', 'dly', 'dmi', 'do ', 'doc', 'doe', 'dom', 'don', 'doo', 'dow', 'dra', 'dre', 'dri', 'dro', 'dru', 'ds ', 'ds,', 'ds.', 'dua', 'duc', 'due', 'dul', 'dur', 'dva', 'dve', 'dy ', 'e \"', 'e (', 'e 1', 'e A', 'e B', 'e C', 'e D', 'e E', 'e F', 'e G', 'e H', 'e I', 'e J', 'e K', 'e L', 'e M', 'e N', 'e O', 'e P', 'e R', 'e S', 'e T', 'e U', 'e V', 'e W', 'e a', 'e b', 'e c', 'e d', 'e e', 'e f', 'e g', 'e h', 'e i', 'e j', 'e k', 'e l', 'e m', 'e n', 'e o', 'e p', 'e q', 'e r', 'e s', 'e t', 'e u', 'e v', 'e w', 'e y', 'e\" ', \"e's\", 'e, ', 'e. ', 'e: ', 'e; ', 'ea ', 'eac', 'ead', 'eak', 'eal', 'eam', 'ean', 'eap', 'ear', 'eas', 'eat', 'eau', 'eav', 'eca', 'ece', 'ech', 'eci', 'eck', 'ecl', 'eco', 'ecr', 'ect', 'ecu', 'ed ', 'ed,', 'ed.', 'ede', 'edg', 'edi', 'edl', 'eds', 'edu', 'ee ', 'eed', 'eei', 'eek', 'eel', 'eem', 'een', 'eep', 'eer', 'ees', 'eet', 'ef ', 'efe', 'eff', 'efo', 'eft', 'efu', 'ega', 'ege', 'egi', 'ehi', 'eig', 'ein', 'eir', 'eit', 'eiv', 'ek ', 'eks', 'el ', 'el,', 'el.', 'ela', 'eld', 'ele', 'elf', 'eli', 'ell', 'elo', 'elp', 'els', 'elt', 'elv', 'ely', 'em ', 'em,', 'em.', 'ema', 'emb', 'eme', 'emi', 'emo', 'emp', 'ems', 'en ', \"en'\", 'en,', 'en.', 'ena', 'enc', 'end', 'ene', 'eng', 'eni', 'enl', 'enn', 'eno', 'ens', 'ent', 'eon', 'eop', 'eor', 'ep ', 'epa', 'epe', 'eph', 'epi', 'epl', 'epo', 'epr', 'eps', 'ept', 'equ', 'er ', \"er'\", 'er,', 'er-', 'er.', 'era', 'erc', 'ere', 'erf', 'erg', 'eri', 'erl', 'erm', 'ern', 'ero', 'erp', 'err', 'ers', 'ert', 'erv', 'erw', 'ery', 'es ', 'es,', 'es.', 'esc', 'ese', 'esi', 'esn', 'eso', 'esp', 'ess', 'est', 'esu', 'et ', 'et,', 'et.', 'eta', 'ete', 'eth', 'eti', 'etr', 'ets', 'ett', 'etu', 'etw', 'ety', 'eva', 'eve', 'evi', 'evo', 'ew ', 'ews', 'ex ', 'exa', 'exc', 'exe', 'exi', 'exp', 'ext', 'ey ', 'ey,', 'ey.', 'eye', 'f A', 'f B', 'f C', 'f D', 'f E', 'f G', 'f H', 'f J', 'f L', 'f M', 'f P', 'f R', 'f S', 'f T', 'f a', 'f b', 'f c', 'f d', 'f e', 'f f', 'f g', 'f h', 'f i', 'f l', 'f m', 'f n', 'f o', 'f p', 'f r', 'f s', 'f t', 'f w', 'f, ', 'f. ', 'fac', 'fai', 'fal', 'fam', 'fan', 'far', 'fas', 'fat', 'fe ', 'fe,', 'fe.', 'fea', 'fec', 'fee', 'fel', 'fen', 'fer', 'fes', 'few', 'ff ', 'ffa', 'ffe', 'ffi', 'ffo', 'fic', 'fie', 'fig', 'fil', 'fin', 'fir', 'fit', 'fla', 'fle', 'fli', 'flo', 'flu', 'fly', 'fol', 'foo', 'for', 'fou', 'fra', 'fre', 'fri', 'fro', 'ft ', 'fte', 'ful', 'fun', 'fur', 'fus', 'fut', 'g M', 'g S', 'g a', 'g b', 'g c', 'g d', 'g e', 'g f', 'g g', 'g h', 'g i', 'g l', 'g m', 'g n', 'g o', 'g p', 'g r', 'g s', 'g t', 'g u', 'g w', 'g, ', 'g. ', 'gag', 'gai', 'gal', 'gan', 'gar', 'gat', 'ge ', 'ge,', 'ge.', 'ged', 'gel', 'gen', 'ger', 'ges', 'get', 'gge', 'ggl', 'gh ', 'gho', 'ght', 'gic', 'gin', 'gio', 'gir', 'giv', 'gla', 'gle', 'gli', 'gly', 'gn ', 'gna', 'gne', 'gni', 'go ', 'goe', 'goi', 'gon', 'goo', 'got', 'gov', 'gra', 'gre', 'gri', 'gro', 'gs ', 'gs,', 'gs.', 'gua', 'gue', 'gui', 'gun', 'gur', 'gy ', 'h A', 'h C', 'h M', 'h S', 'h a', 'h b', 'h c', 'h d', 'h e', 'h f', 'h g', 'h h', 'h i', 'h l', 'h m', 'h n', 'h o', 'h p', 'h r', 'h s', 'h t', 'h w', \"h's\", 'h, ', 'h. ', 'hab', 'had', 'hai', 'hal', 'ham', 'han', 'hap', 'har', 'has', 'hat', 'hav', 'he ', 'hea', 'hed', 'hei', 'hel', 'hem', 'hen', 'her', 'hes', 'het', 'hey', 'hic', 'hid', 'hie', 'hig', 'hil', 'him', 'hin', 'hip', 'hir', 'his', 'hit', 'ho ', 'hol', 'hom', 'hon', 'hoo', 'hop', 'hor', 'hos', 'hot', 'hou', 'how', 'hre', 'hri', 'hro', 'hs ', 'ht ', 'ht,', 'ht.', 'hte', 'hts', 'hum', 'hun', 'hur', 'hus', 'hy ', 'hys', 'i, ', 'ia ', 'ia,', 'ia.', 'iag', 'ial', 'ian', 'iar', 'iat', 'ibe', 'ibi', 'ibl', 'ic ', 'ic,', 'ica', 'ice', 'ich', 'ici', 'ick', 'ics', 'ict', 'icu', 'id ', 'ida', 'idd', 'ide', 'idi', 'idn', 'ids', 'ie ', 'ie,', 'ied', 'ief', 'iel', 'ien', 'ier', 'ies', 'iet', 'iev', 'iew', 'if ', 'ife', 'iff', 'ifi', 'ift', 'ifu', 'iga', 'ige', 'igh', 'igi', 'ign', 'igu', 'ike', 'il ', 'ila', 'ild', 'ile', 'ili', 'ill', 'ilo', 'ils', 'ilt', 'ily', 'im ', 'im,', 'im.', 'ima', 'ime', 'imi', 'imm', 'imp', 'ims', 'in ', 'in,', 'in.', 'ina', 'inc', 'ind', 'ine', 'inf', 'ing', 'inh', 'ini', 'inj', 'ink', 'inn', 'ino', 'ins', 'int', 'inu', 'inv', 'iol', 'ion', 'ior', 'iou', 'ip ', 'ip,', 'ip.', 'ipp', 'ips', 'ir ', 'ira', 'irc', 'ird', 'ire', 'iri', 'irl', 'irs', 'irt', 'is ', 'is,', 'is.', 'isa', 'isc', 'ise', 'isg', 'ish', 'isi', 'isl', 'ism', 'iso', 'isp', 'iss', 'ist', 'it ', 'it,', 'it.', 'ita', 'itc', 'ite', 'ith', 'iti', 'itl', 'ito', 'its', 'itt', 'itu', 'ity', 'iva', 'ive', 'ivi', 'ix ', 'iza', 'ize', 'jec', 'job', 'joi', 'jou', 'jur', 'jus', 'k a', 'k b', 'k c', 'k d', 'k f', 'k h', 'k i', 'k o', 'k s', 'k t', 'k w', \"k's\", 'k, ', 'k. ', 'ke ', 'ked', 'kee', 'ken', 'ker', 'kes', 'ket', 'kid', 'kil', 'kin', 'kly', 'kne', 'kno', 'ks ', 'ks,', 'l S', 'l a', 'l b', 'l c', 'l d', 'l e', 'l f', 'l g', 'l h', 'l i', 'l l', 'l m', 'l n', 'l o', 'l p', 'l r', 'l s', 'l t', 'l w', \"l's\", 'l, ', 'l. ', 'la ', 'lab', 'lac', 'lad', 'lag', 'lai', 'lam', 'lan', 'lar', 'las', 'lat', 'lau', 'law', 'lay', 'ld ', 'ld,', 'ld.', 'lde', 'ldi', 'ldr', 'lds', 'le ', 'le,', 'le.', 'lea', 'lec', 'led', 'lee', 'lef', 'leg', 'lem', 'len', 'ler', 'les', 'let', 'lev', 'ley', 'lf ', 'lf,', 'lf-', 'lf.', 'lia', 'lib', 'lic', 'lie', 'lif', 'lig', 'lik', 'lim', 'lin', 'lio', 'lis', 'lit', 'liv', 'liz', 'lk ', 'll ', 'll,', 'll-', 'll.', 'lla', 'lle', 'lli', 'llo', 'lls', 'lly', 'lmo', 'loc', 'log', 'lon', 'loo', 'lop', 'lor', 'los', 'lot', 'lou', 'lov', 'low', 'loy', 'lp ', 'lre', 'ls ', 'ls,', 'ls.', 'lse', 'lso', 'lt ', 'lte', 'lth', 'lti', 'luc', 'lud', 'lue', 'lus', 'lut', 'lve', 'lwa', 'ly ', 'ly,', 'ly.', 'm a', 'm b', 'm c', 'm d', 'm e', 'm f', 'm h', 'm i', 'm o', 'm p', 'm s', 'm t', 'm w', 'm, ', 'm. ', 'mac', 'mad', 'mag', 'mai', 'mak', 'mal', 'man', 'mar', 'mas', 'mat', 'may', 'mba', 'mbe', 'mbi', 'mbl', 'me ', 'me,', 'me.', 'mea', 'med', 'mee', 'mel', 'mem', 'men', 'mer', 'mes', 'met', 'mic', 'mid', 'mig', 'mil', 'min', 'mir', 'mis', 'mit', 'mma', 'mme', 'mmi', 'mmo', 'mmu', 'mod', 'mom', 'mon', 'mor', 'mos', 'mot', 'mou', 'mov', 'mpa', 'mpe', 'mpi', 'mpl', 'mpo', 'mpr', 'mpt', 'ms ', 'mse', 'muc', 'mul', 'mun', 'mur', 'mus', 'my ', 'mys', 'n (', 'n 1', 'n A', 'n B', 'n C', 'n D', 'n E', 'n F', 'n G', 'n H', 'n I', 'n J', 'n K', 'n L', 'n M', 'n N', 'n P', 'n R', 'n S', 'n T', 'n W', 'n a', 'n b', 'n c', 'n d', 'n e', 'n f', 'n g', 'n h', 'n i', 'n k', 'n l', 'n m', 'n n', 'n o', 'n p', 'n r', 'n s', 'n t', 'n u', 'n v', 'n w', 'n y', \"n's\", \"n't\", 'n, ', 'n. ', 'na ', 'na,', 'nab', 'nag', 'nal', 'nam', 'nan', 'nap', 'nar', 'nat', 'nce', 'nch', 'nci', 'ncl', 'nco', 'ncr', 'nct', 'ncy', 'nd ', 'nd,', 'nd.', 'nda', 'nde', 'ndi', 'ndl', 'ndo', 'ndr', 'nds', 'ndu', 'ne ', 'ne,', 'ne.', 'nea', 'nec', 'ned', 'nee', 'nei', 'nel', 'nem', 'nen', 'ner', 'nes', 'net', 'nev', 'new', 'nex', 'ney', 'nfe', 'nfi', 'nfl', 'nfo', 'nfr', 'ng ', 'ng,', 'ng.', 'nga', 'nge', 'ngi', 'ngl', 'ngr', 'ngs', 'ngt', 'ngu', 'nha', 'nia', 'nic', 'nie', 'nif', 'nig', 'nim', 'nin', 'nio', 'nis', 'nit', 'niv', 'niz', 'nju', 'nk ', 'nki', 'nks', 'nli', 'nly', 'nme', 'nna', 'nne', 'nni', 'nno', 'no ', 'noc', 'non', 'nor', 'not', 'nou', 'nov', 'now', 'ns ', 'ns,', 'ns.', 'nsc', 'nse', 'nsh', 'nsi', 'nsp', 'nst', 'nsu', 'nt ', 'nt,', 'nt.', 'nta', 'nte', 'nth', 'nti', 'ntl', 'nto', 'ntr', 'nts', 'ntu', 'nty', 'nue', 'num', 'nve', 'nvi', 'nvo', 'nwh', 'ny ', 'o A', 'o B', 'o C', 'o E', 'o L', 'o M', 'o P', 'o S', 'o a', 'o b', 'o c', 'o d', 'o e', 'o f', 'o g', 'o h', 'o i', 'o j', 'o k', 'o l', 'o m', 'o n', 'o o', 'o p', 'o r', 'o s', 'o t', 'o u', 'o v', 'o w', 'o, ', 'o. ', 'oac', 'oad', 'oar', 'oat', 'ob ', 'obe', 'obl', 'obs', 'oca', 'occ', 'oce', 'oci', 'ock', 'oct', 'ocu', 'od ', 'od,', 'od.', 'ode', 'odi', 'ods', 'odu', 'ody', 'oes', 'of ', 'off', 'oft', 'oge', 'ogi', 'ogn', 'ogr', 'ogy', 'ohn', 'oic', 'oid', 'oin', 'ois', 'ok ', 'ok,', 'oke', 'oki', 'oks', 'ol ', 'ol,', 'ola', 'old', 'ole', 'oli', 'oll', 'olo', 'olu', 'olv', 'om ', 'om,', 'oma', 'omb', 'ome', 'omi', 'omm', 'omp', 'on ', \"on'\", 'on,', 'on.', 'ona', 'onc', 'ond', 'one', 'onf', 'ong', 'oni', 'onl', 'onn', 'ono', 'ons', 'ont', 'onv', 'ony', 'oo ', 'ood', 'ook', 'ool', 'oom', 'oon', 'oor', 'oos', 'oot', 'op ', 'ope', 'oph', 'opi', 'opl', 'opo', 'opp', 'ops', 'opt', 'opu', 'or ', 'or,', 'or.', 'ora', 'orc', 'ord', 'ore', 'org', 'ori', 'ork', 'orl', 'orm', 'orn', 'orp', 'orr', 'ors', 'ort', 'ory', 'os ', 'ose', 'osi', 'osp', 'oss', 'ost', 'ot ', 'ota', 'ote', 'oth', 'oti', 'oto', 'ots', 'ott', 'oub', 'oug', 'oul', 'oun', 'oup', 'our', 'ous', 'out', 'ove', 'ovi', 'ow ', 'ow,', 'owa', 'owe', 'owi', 'owl', 'own', 'ows', 'oy ', 'oya', 'oye', 'oys', 'p a', 'p b', 'p f', 'p h', 'p i', 'p o', 'p s', 'p t', 'p w', 'p, ', 'p. ', 'pac', 'pai', 'pan', 'par', 'pas', 'pat', 'pay', 'pe ', 'pea', 'pec', 'ped', 'pel', 'pen', 'peo', 'per', 'pes', 'pet', 'pha', 'phe', 'phi', 'pho', 'phy', 'pic', 'pie', 'pil', 'pin', 'pir', 'pit', 'pla', 'ple', 'pli', 'plo', 'ply', 'poi', 'pol', 'pon', 'pop', 'por', 'pos', 'pot', 'pow', 'ppa', 'ppe', 'ppi', 'ppl', 'ppo', 'ppr', 'pra', 'pre', 'pri', 'pro', 'ps ', 'pse', 'pt ', 'pta', 'pte', 'pti', 'pts', 'ptu', 'pub', 'pul', 'pur', 'put', 'py ', 'qua', 'que', 'qui', 'r (', 'r A', 'r B', 'r C', 'r D', 'r E', 'r G', 'r H', 'r J', 'r L', 'r M', 'r P', 'r R', 'r S', 'r T', 'r a', 'r b', 'r c', 'r d', 'r e', 'r f', 'r g', 'r h', 'r i', 'r k', 'r l', 'r m', 'r n', 'r o', 'r p', 'r r', 'r s', 'r t', 'r u', 'r v', 'r w', \"r's\", 'r, ', 'r-o', 'r. ', 'ra ', 'rab', 'rac', 'rad', 'rag', 'rai', 'ral', 'ram', 'ran', 'rap', 'rar', 'ras', 'rat', 'rav', 'raw', 'ray', 'rce', 'rch', 'rd ', 'rd,', 'rd.', 'rde', 'rdi', 'rds', 're ', 're,', 're.', 'rea', 'reb', 'rec', 'red', 'ree', 'ref', 'reg', 'rei', 'rel', 'rem', 'ren', 'rep', 'req', 'rer', 'res', 'ret', 'rev', 'rew', 'rfu', 'rga', 'rge', 'rgi', 'ria', 'rib', 'ric', 'rid', 'rie', 'rif', 'rig', 'ril', 'rim', 'rin', 'rio', 'rip', 'ris', 'rit', 'riv', 'rk ', 'rk,', 'rke', 'rki', 'rks', 'rl ', 'rld', 'rle', 'rli', 'rly', 'rm ', 'rma', 'rme', 'rmi', 'rms', 'rmy', 'rn ', 'rna', 'rne', 'rni', 'rnm', 'rns', 'roa', 'rob', 'roc', 'rod', 'rof', 'rog', 'rok', 'rol', 'rom', 'ron', 'roo', 'rop', 'ror', 'ros', 'rot', 'rou', 'rov', 'row', 'roy', 'rpo', 'rpr', 'rra', 'rre', 'rri', 'rro', 'rry', 'rs ', 'rs,', 'rs.', 'rsa', 'rse', 'rsh', 'rsi', 'rso', 'rst', 'rsu', 'rt ', 'rt,', 'rt.', 'rta', 'rte', 'rth', 'rti', 'rtl', 'rts', 'rtu', 'rty', 'ruc', 'rue', 'rug', 'rui', 'rul', 'run', 'rup', 'rus', 'rut', 'rva', 'rve', 'rvi', 'rwa', 'ry ', 'ry,', 'ry.', 'ryi', 'ryo', 'ryt', 's \"', 's (', 's A', 's B', 's C', 's D', 's E', 's F', 's G', 's H', 's J', 's L', 's M', 's P', 's R', 's S', 's T', 's a', 's b', 's c', 's d', 's e', 's f', 's g', 's h', 's i', 's j', 's k', 's l', 's m', 's n', 's o', 's p', 's q', 's r', 's s', 's t', 's u', 's v', 's w', 's y', 's\" ', \"s' \", 's) ', 's, ', 's. ', 's: ', 's; ', 'sac', 'saf', 'sag', 'sai', 'sal', 'sam', 'san', 'sap', 'sar', 'sas', 'sat', 'sav', 'say', 'sba', 'sca', 'sce', 'sch', 'sci', 'sco', 'scr', 'scu', 'se ', 'se,', 'se.', 'sea', 'sec', 'sed', 'see', 'sel', 'sem', 'sen', 'sep', 'seq', 'ser', 'ses', 'set', 'sev', 'sex', 'sfu', 'sh ', 'sha', 'she', 'shi', 'sho', 'sia', 'sib', 'sic', 'sid', 'sig', 'sil', 'sim', 'sin', 'sio', 'sis', 'sit', 'siv', 'sk ', 'ske', 'ski', 'sks', 'sla', 'sle', 'sli', 'sly', 'sma', \"sn'\", 'sne', 'so ', 'soc', 'sol', 'som', 'son', 'soo', 'sor', 'sou', 'spa', 'spe', 'spi', 'spo', 'spr', 'ss ', 'ss,', 'ss.', 'ssa', 'sse', 'ssf', 'ssi', 'sso', 'ssu', 'st ', 'st,', 'st.', 'sta', 'ste', 'sti', 'stl', 'sto', 'str', 'sts', 'stu', 'sty', 'sua', 'sub', 'suc', 'sue', 'suf', 'sug', 'sui', 'sul', 'sum', 'sup', 'sur', 'sus', 'swe', 'sy ', 't A', 't B', 't C', 't D', 't E', 't F', 't G', 't H', 't J', 't L', 't M', 't P', 't R', 't S', 't T', 't a', 't b', 't c', 't d', 't e', 't f', 't g', 't h', 't i', 't k', 't l', 't m', 't n', 't o', 't p', 't r', 't s', 't t', 't u', 't v', 't w', \"t's\", 't, ', 't. ', 'ta ', 'tab', 'tac', 'tag', 'tai', 'tak', 'tal', 'tan', 'tar', 'tas', 'tat', 'tay', 'tch', 'te ', 'te,', 'te.', 'tea', 'tec', 'ted', 'tee', 'tel', 'tem', 'ten', 'tep', 'ter', 'tes', 'th ', 'th,', 'th.', 'tha', 'the', 'thi', 'tho', 'thr', 'ths', 'thu', 'thy', 'tia', 'tic', 'tie', 'tif', 'tig', 'til', 'tim', 'tin', 'tio', 'tir', 'tis', 'tit', 'tiv', 'tle', 'tly', 'tme', 'tne', 'to ', 'tog', 'tol', 'tom', 'ton', 'too', 'top', 'tor', 'tou', 'tow', 'tra', 'tre', 'tri', 'tro', 'tru', 'try', 'ts ', 'ts,', 'ts.', 'tse', 'tsi', 'tta', 'tte', 'tti', 'ttl', 'ttr', 'tua', 'tud', 'tun', 'tur', 'tut', 'twe', 'twi', 'two', 'ty ', 'ty,', 'ty.', 'ual', 'uar', 'uat', 'ubl', 'ubs', 'ucc', 'uce', 'uch', 'uck', 'uct', 'udd', 'ude', 'udi', 'ue ', 'ued', 'uel', 'uen', 'ues', 'uff', 'uge', 'ugg', 'ugh', 'uic', 'uil', 'uin', 'uir', 'uis', 'uit', 'ul ', 'ula', 'uld', 'ule', 'ull', 'ult', 'um ', 'uma', 'umb', 'ume', 'umm', 'ump', 'un ', 'una', 'unc', 'und', 'une', 'ung', 'uni', 'unk', 'unn', 'uns', 'unt', 'up ', 'upe', 'upo', 'upp', 'upt', 'ur ', 'ura', 'urc', 'urd', 'ure', 'urg', 'uri', 'urn', 'urp', 'urr', 'urs', 'urt', 'urv', 'ury', 'us ', 'us,', 'us.', 'usa', 'usb', 'use', 'ush', 'usi', 'usl', 'usp', 'uss', 'ust', 'ut ', 'ut,', 'uta', 'ute', 'uth', 'uti', 'uts', 'utt', 'utu', 'val', 'van', 'var', 'vas', 'vat', 've ', 've,', 've.', 'vea', 'ved', 'vel', 'ven', 'ver', 'ves', 'vic', 'vid', 'vie', 'vil', 'vin', 'vio', 'vis', 'vit', 'viv', 'voi', 'vol', 'vor', 'w a', 'w c', 'w d', 'w f', 'w h', 'w i', 'w m', 'w o', 'w s', 'w t', 'w w', 'w, ', 'w. ', 'wai', 'wak', 'wal', 'wan', 'war', 'was', 'wat', 'way', 'wea', 'wed', 'wee', 'wel', 'wen', 'wer', 'wev', 'wha', 'whe', 'whi', 'who', 'wif', 'wil', 'win', 'wis', 'wit', 'wle', 'wly', 'wn ', 'wn,', 'wn.', 'wne', 'wo ', 'wom', 'won', 'woo', 'wor', 'wou', 'wri', 'ws ', 'xce', 'xis', 'xpe', 'xpl', 'xt ', 'xtr', 'y (', 'y A', 'y B', 'y C', 'y D', 'y M', 'y S', 'y a', 'y b', 'y c', 'y d', 'y e', 'y f', 'y g', 'y h', 'y i', 'y k', 'y l', 'y m', 'y n', 'y o', 'y p', 'y r', 'y s', 'y t', 'y u', 'y v', 'y w', \"y's\", 'y, ', 'y. ', 'yal', 'yea', 'yed', 'yer', 'yet', 'yin', 'yon', 'you', 'ys ', 'ysi', 'yst', 'yth', 'ze ', 'zed', 'zes', 'zin', '’s ']\n",
      "char_n_grams_4 [' (th', ' (wh', ' Aft', ' All', ' Alt', ' Ame', ' And', ' As ', ' At ', ' Bri', ' But', ' Car', ' Cha', ' Chi', ' Chr', ' Col', ' Com', ' Con', ' Des', ' Dur', ' Ear', ' Eng', ' Eve', ' Fin', ' For', ' Fra', ' Fre', ' Gra', ' Gre', ' Har', ' He ', ' Her', ' His', ' How', ' In ', ' It ', ' Joh', ' Kin', ' Lat', ' Lon', ' Man', ' Mar', ' Mea', ' Mor', ' New', ' On ', ' One', ' Par', ' Pri', ' She', ' Sta', ' The', ' Thi', ' Tho', ' Thr', ' To ', ' Uni', ' War', ' Whe', ' Whi', ' Wil', ' Wit', ' Wor', ' a \"', ' a b', ' a c', ' a d', ' a f', ' a g', ' a h', ' a j', ' a l', ' a m', ' a n', ' a p', ' a r', ' a s', ' a t', ' a v', ' a w', ' a y', ' aba', ' abl', ' abo', ' acc', ' acr', ' act', ' add', ' adm', ' adv', ' aff', ' aft', ' aga', ' age', ' agr', ' ali', ' all', ' alm', ' alo', ' alr', ' als', ' alt', ' amo', ' an ', ' anc', ' and', ' ang', ' ann', ' ano', ' ant', ' any', ' app', ' are', ' arm', ' aro', ' arr', ' art', ' as ', ' ask', ' ass', ' at ', ' att', ' aut', ' awa', ' bac', ' ban', ' bar', ' bas', ' bat', ' be ', ' bea', ' bec', ' bee', ' bef', ' beg', ' beh', ' bei', ' bel', ' bes', ' bet', ' bir', ' bla', ' blo', ' boa', ' bod', ' boo', ' bor', ' bot', ' boy', ' bra', ' bre', ' bri', ' bro', ' bui', ' bur', ' bus', ' but', ' by ', ' cal', ' cam', ' can', ' cap', ' car', ' cas', ' cat', ' cau', ' cen', ' cer', ' cha', ' che', ' chi', ' cho', ' cit', ' cla', ' cle', ' cli', ' clo', ' col', ' com', ' con', ' cor', ' cou', ' cov', ' cra', ' cre', ' cri', ' cro', ' cru', ' cul', ' cur', ' dan', ' dar', ' dau', ' day', ' dea', ' dec', ' dee', ' def', ' del', ' dem', ' dep', ' des', ' det', ' dev', ' did', ' die', ' dif', ' dir', ' dis', ' div', ' do ', ' doc', ' doe', ' don', ' dow', ' dra', ' dre', ' dri', ' dro', ' dru', ' due', ' dur', ' eac', ' ear', ' eff', ' ele', ' emb', ' emp', ' enc', ' end', ' ene', ' eng', ' eno', ' ent', ' esc', ' est', ' eve', ' evi', ' exa', ' exc', ' exi', ' exp', ' ext', ' eye', ' fac', ' fai', ' fal', ' fam', ' far', ' fat', ' fea', ' fee', ' fel', ' few', ' fig', ' fil', ' fin', ' fir', ' fla', ' fle', ' flo', ' fol', ' foo', ' for', ' fou', ' fre', ' fri', ' fro', ' ful', ' fun', ' fur', ' fut', ' gen', ' get', ' gir', ' giv', ' go ', ' goe', ' goi', ' goo', ' gov', ' gra', ' gre', ' gro', ' gua', ' gui', ' had', ' hal', ' han', ' hap', ' har', ' has', ' hav', ' he ', ' hea', ' hel', ' her', ' hid', ' hig', ' him', ' his', ' hol', ' hom', ' hop', ' hor', ' hos', ' hou', ' how', ' hum', ' hun', ' hus', ' ide', ' if ', ' ill', ' imm', ' imp', ' in ', ' inc', ' ind', ' inf', ' inh', ' inj', ' ins', ' int', ' inv', ' is ', ' it ', ' it,', ' it.', ' its', ' job', ' joi', ' jou', ' jus', ' kee', ' kid', ' kil', ' kin', ' kno', ' lan', ' lar', ' las', ' lat', ' law', ' lea', ' led', ' lef', ' leg', ' les', ' let', ' lif', ' lig', ' lik', ' lin', ' lit', ' liv', ' loc', ' lon', ' loo', ' los', ' lov', ' mad', ' mag', ' mai', ' mak', ' man', ' mar', ' mas', ' mat', ' may', ' mea', ' med', ' mee', ' mem', ' men', ' mer', ' met', ' mid', ' mig', ' mil', ' min', ' mis', ' mod', ' mom', ' mon', ' mor', ' mos', ' mot', ' mou', ' mov', ' muc', ' mur', ' mus', ' mys', ' nam', ' nar', ' nat', ' nea', ' nee', ' nei', ' nev', ' new', ' nex', ' nig', ' no ', ' nor', ' not', ' nov', ' now', ' num', ' obs', ' occ', ' of ', ' off', ' old', ' on ', ' onc', ' one', ' onl', ' ope', ' opp', ' or ', ' ord', ' ori', ' oth', ' out', ' ove', ' own', ' pai', ' par', ' pas', ' pat', ' pea', ' peo', ' per', ' pic', ' pla', ' plo', ' poi', ' pol', ' pop', ' por', ' pos', ' pow', ' pra', ' pre', ' pri', ' pro', ' pub', ' pur', ' put', ' que', ' qui', ' rac', ' rai', ' ran', ' rat', ' rea', ' rec', ' ref', ' reg', ' rel', ' rem', ' ren', ' rep', ' req', ' res', ' ret', ' rev', ' rid', ' rig', ' ris', ' roo', ' rul', ' run', ' saf', ' sai', ' sam', ' sav', ' say', ' sca', ' sce', ' sch', ' sea', ' sec', ' see', ' sel', ' sen', ' ser', ' set', ' sev', ' sex', ' sha', ' she', ' shi', ' sho', ' sid', ' sig', ' sim', ' sin', ' sis', ' sit', ' ski', ' sla', ' sle', ' sma', ' so ', ' soc', ' sol', ' som', ' son', ' soo', ' sou', ' spa', ' spe', ' spi', ' spo', ' sta', ' ste', ' sti', ' sto', ' str', ' stu', ' sub', ' suc', ' suf', ' sug', ' sui', ' sum', ' sup', ' sur', ' sus', ' tak', ' tal', ' tea', ' tel', ' ten', ' ter', ' tha', ' the', ' thi', ' tho', ' thr', ' thu', ' tim', ' to ', ' tog', ' tol', ' too', ' tor', ' tow', ' tra', ' tre', ' tri', ' tro', ' tru', ' try', ' tur', ' two', ' una', ' unc', ' und', ' uni', ' uns', ' unt', ' up ', ' upo', ' use', ' usi', ' var', ' ver', ' vic', ' vil', ' vio', ' vis', ' wai', ' wal', ' wan', ' war', ' was', ' wat', ' way', ' wea', ' wee', ' wel', ' wer', ' wha', ' whe', ' whi', ' who', ' wif', ' wil', ' win', ' wis', ' wit', ' wom', ' won', ' wor', ' wou', ' wri', ' yea', ' you', \"'s a\", \"'s b\", \"'s c\", \"'s d\", \"'s e\", \"'s f\", \"'s g\", \"'s h\", \"'s i\", \"'s l\", \"'s m\", \"'s n\", \"'s o\", \"'s p\", \"'s r\", \"'s s\", \"'s t\", \"'s w\", '(the', ') an', '), a', ', Ma', ', a ', ', af', ', al', ', an', ', ar', ', as', ', at', ', be', ', bu', ', ca', ', co', ', de', ', di', ', ev', ', fi', ', fo', ', ha', ', he', ', hi', ', ho', ', in', ', is', ', it', ', le', ', ma', ', no', ', on', ', or', ', pr', ', re', ', se', ', sh', ', so', ', st', ', su', ', th', ', to', ', un', ', wa', ', wh', ', wi', '-old', '-yea', '. A ', '. Af', '. Al', '. An', '. As', '. At', '. Be', '. Bu', '. Co', '. De', '. Du', '. Ev', '. Fi', '. Fo', '. Ha', '. He', '. Hi', '. Ho', '. In', '. It', '. La', '. Ma', '. Me', '. No', '. On', '. Re', '. Sh', '. So', '. Th', '. To', '. Un', '. Wh', '. Wi', '; th', 'Afte', 'Alth', 'Amer', 'As t', 'At t', 'But ', 'Chri', 'Duri', 'Eart', 'Engl', 'Even', 'Fran', 'He a', 'He i', 'He s', 'He t', 'Her ', 'His ', 'Howe', 'In a', 'In t', 'It i', 'John', 'King', 'Late', 'Lond', 'Mean', 'New ', 'On t', 'One ', 'She ', 'The ', 'Ther', 'They', 'This', 'When', 'Whil', 'With', 'a an', 'a ba', 'a be', 'a bo', 'a br', 'a ca', 'a ch', 'a cl', 'a co', 'a cr', 'a da', 'a de', 'a di', 'a fa', 'a fe', 'a fi', 'a fo', 'a fr', 'a gi', 'a gr', 'a ha', 'a he', 'a hi', 'a ho', 'a in', 'a is', 'a la', 'a le', 'a li', 'a lo', 'a ma', 'a me', 'a mi', 'a mo', 'a ne', 'a pa', 'a pe', 'a pl', 'a po', 'a pr', 'a ra', 'a re', 'a ro', 'a se', 'a sh', 'a si', 'a sm', 'a so', 'a sp', 'a st', 'a su', 'a te', 'a th', 'a to', 'a tr', 'a vi', 'a wa', 'a we', 'a wh', 'a wi', 'a wo', 'a yo', \"a's \", 'a, a', 'a, t', 'a, w', 'a. T', 'aban', 'abil', 'able', 'ably', 'abou', 'acce', 'acci', 'acco', 'ace ', 'ace,', 'ace.', 'aced', 'aces', 'ach ', 'ache', 'achi', 'ack ', 'acke', 'acks', 'acro', 'act ', 'acte', 'acti', 'acts', 'actu', 'ad a', 'ad b', 'ad o', 'ad s', 'ad t', 'ad, ', 'ad. ', 'ade ', 'ader', 'ades', 'adin', 'admi', 'ads ', 'adva', 'adve', 'ady ', 'afte', 'agai', 'age ', 'age,', 'age.', 'aged', 'agen', 'ager', 'ages', 'agic', 'agin', 'agon', 'agre', 'aid ', 'ail ', 'ails', 'aims', 'ain ', 'ain,', 'ain.', 'aine', 'aini', 'ains', 'aint', 'air ', 'aise', 'ake ', 'aken', 'akes', 'akin', 'aks ', 'al a', 'al b', 'al c', 'al d', 'al e', 'al f', 'al h', 'al i', 'al l', 'al m', 'al o', 'al p', 'al r', 'al s', 'al t', 'al w', 'al, ', 'al. ', 'ale ', 'aled', 'alin', 'alis', 'alit', 'aliz', 'alk ', 'all ', 'alle', 'alli', 'allo', 'alls', 'ally', 'almo', 'alon', 'alre', 'als ', 'also', 'alth', 'ame ', 'amed', 'ames', 'amil', 'amin', 'amon', 'amou', 'an A', 'an a', 'an b', 'an c', 'an d', 'an e', 'an f', 'an h', 'an i', 'an l', 'an m', 'an n', 'an o', 'an p', 'an r', 'an s', 'an t', 'an u', 'an w', \"an's\", 'an, ', 'an. ', 'anag', 'ance', 'anci', 'and ', 'and,', 'and.', 'anda', 'ande', 'andi', 'ando', 'ands', 'ane ', 'ange', 'anis', 'anne', 'anno', 'anot', 'ans ', 'ant ', 'ant,', 'anta', 'ante', 'anti', 'antl', 'ants', 'anwh', 'any ', 'apar', 'ape ', 'apes', 'appa', 'appe', 'appr', 'aptu', 'ar a', 'ar i', 'ar o', 'ar t', 'ar w', 'ar, ', 'ar-o', 'ar. ', 'arac', 'aran', 'arat', 'arch', 'ard ', 'ard,', 'arde', 'ardi', 'ards', 'are ', 'ared', 'aren', 'ares', 'arge', 'arin', 'ario', 'aris', 'ark ', 'arli', 'arly', 'arn ', 'arni', 'arns', 'arou', 'arra', 'arre', 'arri', 'arry', 'ars ', 'ars,', 'ars.', 'art ', 'arte', 'arth', 'arti', 'arts', 'arty', 'ary ', 'as a', 'as b', 'as c', 'as d', 'as e', 'as f', 'as g', 'as h', 'as i', 'as l', 'as m', 'as n', 'as o', 'as p', 'as r', 'as s', 'as t', 'as w', 'as, ', 'ase ', 'ased', 'asin', 'ask ', 'asks', 'ason', 'ass ', 'assa', 'asse', 'assi', 'assu', 'ast ', 'aste', 'at C', 'at M', 'at S', 'at a', 'at b', 'at c', 'at d', 'at e', 'at f', 'at h', 'at i', 'at l', 'at m', 'at n', 'at o', 'at p', 'at s', 'at t', 'at w', 'at, ', 'atch', 'ate ', 'ate,', 'ate.', 'ated', 'atel', 'aten', 'ater', 'ates', 'ath ', 'ath,', 'ath.', 'athe', 'atic', 'atin', 'atio', 'ativ', 'ator', 'ats ', 'atta', 'atte', 'attl', 'atur', 'augh', 'ause', 'auth', 'auti', 'ave ', 'avel', 'aves', 'avin', 'away', 'ay a', 'ay b', 'ay f', 'ay h', 'ay i', 'ay o', 'ay t', 'ay w', 'ay, ', 'ay. ', 'ayin', 'ays ', 'back', 'band', 'base', 'batt', 'be a', 'be s', 'be t', 'beau', 'beca', 'beco', 'bed ', 'been', 'befo', 'begi', 'behi', 'bein', 'beli', 'belo', 'ber ', 'bers', 'bes ', 'best', 'betw', 'bili', 'birt', 'bjec', 'blac', 'ble ', 'blem', 'bles', 'blic', 'blin', 'blis', 'bloo', 'bly ', 'boar', 'body', 'book', 'born', 'both', 'bout', 'boy ', 'brea', 'brin', 'brot', 'brou', 'buil', 'busi', 'but ', 'by a', 'by h', 'by s', 'by t', 'cal ', 'call', 'came', 'camp', 'can ', 'cann', 'cape', 'capt', 'care', 'carr', 'case', 'cast', 'cate', 'cati', 'caus', 'ccep', 'cces', 'ccid', 'ccom', 'ce a', 'ce b', 'ce f', 'ce h', 'ce i', 'ce o', 'ce s', 'ce t', 'ce w', 'ce, ', 'ce. ', 'ced ', 'ceed', 'ceiv', 'cene', 'cent', 'cept', 'cern', 'cert', 'ces ', 'cess', 'ch a', 'ch c', 'ch f', 'ch h', 'ch i', 'ch o', 'ch s', 'ch t', 'ch w', 'ch, ', 'ch. ', 'chan', 'char', 'chas', 'ched', 'cher', 'ches', 'chie', 'chil', 'chin', 'choo', 'cial', 'cide', 'cien', 'ciet', 'cing', 'ciou', 'city', 'ck a', 'ck i', 'ck o', 'ck t', 'ck, ', 'ck. ', 'cked', 'ckin', 'ckly', 'cks ', 'clai', 'clas', 'cle ', 'clea', 'clos', 'clud', 'cogn', 'coll', 'colo', 'come', 'comi', 'comm', 'comp', 'conc', 'cond', 'conf', 'conn', 'cons', 'cont', 'conv', 'cord', 'coul', 'coun', 'cour', 'cove', 'crea', 'cret', 'crib', 'crim', 'cros', 'ct a', 'ct o', 'ct t', 'ct, ', 'cted', 'cter', 'ctic', 'ctin', 'ctio', 'ctiv', 'ctor', 'cts ', 'ctua', 'ctur', 'cula', 'cult', 'curr', 'cuse', 'cuss', 'd Ma', 'd St', 'd a ', 'd ab', 'd ac', 'd af', 'd al', 'd an', 'd ar', 'd as', 'd at', 'd ba', 'd be', 'd bo', 'd br', 'd bu', 'd by', 'd ca', 'd ch', 'd co', 'd de', 'd di', 'd do', 'd en', 'd ev', 'd ex', 'd fa', 'd fi', 'd fo', 'd fr', 'd gi', 'd go', 'd ha', 'd he', 'd hi', 'd ho', 'd in', 'd is', 'd it', 'd ki', 'd la', 'd le', 'd li', 'd lo', 'd ma', 'd me', 'd mo', 'd ne', 'd no', 'd of', 'd on', 'd ou', 'd pa', 'd pe', 'd pl', 'd po', 'd pr', 'd re', 'd sa', 'd se', 'd sh', 'd si', 'd so', 'd sp', 'd st', 'd su', 'd ta', 'd te', 'd th', 'd to', 'd tr', 'd un', 'd up', 'd wa', 'd wh', 'd wi', 'd wo', \"d's \", 'd, a', 'd, b', 'd, h', 'd, i', 'd, s', 'd, t', 'd, w', 'd. A', 'd. H', 'd. I', 'd. S', 'd. T', 'dang', 'daug', 'day ', 'day,', 'days', 'dden', 'ddle', 'de a', 'de i', 'de o', 'de t', 'de, ', 'de. ', 'dead', 'deal', 'deat', 'deci', 'ded ', 'deep', 'defe', 'deli', 'den ', 'denc', 'dent', 'der ', 'der,', 'der.', 'dere', 'deri', 'ders', 'des ', 'desc', 'desi', 'desp', 'dest', 'deta', 'dete', 'deve', 'dge ', 'diat', 'dica', 'did ', 'died', 'dies', 'diff', 'ding', 'dire', 'disa', 'disc', 'disp', 'dist', 'diti', 'divi', 'dle ', 'dly ', 'does', 'don ', 'done', 'down', 'draw', 'drea', 'dren', 'driv', 'ds a', 'ds h', 'ds i', 'ds o', 'ds t', 'ds w', 'ds, ', 'ds. ', 'duce', 'duct', 'due ', 'duri', 'dvan', 'dven', 'e Ca', 'e Ch', 'e Co', 'e Gr', 'e Ma', 'e Un', 'e a ', 'e ab', 'e ac', 'e ad', 'e af', 'e ag', 'e al', 'e an', 'e ap', 'e ar', 'e as', 'e at', 'e au', 'e ba', 'e be', 'e bi', 'e bl', 'e bo', 'e br', 'e bu', 'e by', 'e ca', 'e ce', 'e ch', 'e ci', 'e cl', 'e co', 'e cr', 'e cu', 'e da', 'e de', 'e di', 'e do', 'e dr', 'e du', 'e ea', 'e el', 'e en', 'e es', 'e ev', 'e ex', 'e fa', 'e fe', 'e fi', 'e fl', 'e fo', 'e fr', 'e fu', 'e ga', 'e ge', 'e gi', 'e go', 'e gr', 'e gu', 'e ha', 'e he', 'e hi', 'e ho', 'e hu', 'e id', 'e im', 'e in', 'e is', 'e it', 'e jo', 'e ki', 'e kn', 'e la', 'e le', 'e li', 'e lo', 'e ma', 'e me', 'e mi', 'e mo', 'e mu', 'e na', 'e ne', 'e ni', 'e no', 'e of', 'e on', 'e op', 'e or', 'e ot', 'e ou', 'e ov', 'e pa', 'e pe', 'e pi', 'e pl', 'e po', 'e pr', 'e pu', 'e qu', 'e ra', 'e re', 'e ri', 'e ro', 'e ru', 'e sa', 'e sc', 'e se', 'e sh', 'e si', 'e so', 'e sp', 'e st', 'e su', 'e ta', 'e te', 'e th', 'e ti', 'e to', 'e tr', 'e tw', 'e un', 'e up', 'e us', 'e va', 'e ve', 'e vi', 'e wa', 'e we', 'e wh', 'e wi', 'e wo', 'e wr', 'e ye', 'e yo', \"e's \", 'e, a', 'e, b', 'e, h', 'e, i', 'e, s', 'e, t', 'e, w', 'e. A', 'e. B', 'e. D', 'e. H', 'e. I', 'e. M', 'e. O', 'e. S', 'e. T', 'e. W', 'each', 'ead ', 'ead,', 'ead.', 'eade', 'eadi', 'eads', 'eady', 'eak ', 'eaks', 'eal ', 'eale', 'eali', 'eals', 'ealt', 'eam ', 'ean ', 'eans', 'eanw', 'ear ', 'ear-', 'earc', 'eare', 'eari', 'earl', 'earn', 'ears', 'eart', 'ease', 'easi', 'easo', 'east', 'eat ', 'eate', 'eath', 'eati', 'eats', 'eatu', 'eaut', 'eave', 'eavi', 'ecau', 'ecei', 'ecen', 'ecia', 'ecid', 'ecog', 'ecom', 'econ', 'ecov', 'ecre', 'ect ', 'ecte', 'ecti', 'ects', 'ed M', 'ed S', 'ed a', 'ed b', 'ed c', 'ed d', 'ed e', 'ed f', 'ed h', 'ed i', 'ed l', 'ed m', 'ed o', 'ed p', 'ed r', 'ed s', 'ed t', 'ed u', 'ed w', 'ed, ', 'ed. ', 'edge', 'edia', 'edic', 'edit', 'edly', 'eds ', 'educ', 'ee t', 'eed ', 'eeds', 'eein', 'eek ', 'eeks', 'eeli', 'eels', 'eems', 'een ', 'eep ', 'ees ', 'eet ', 'eeti', 'eets', 'efer', 'effe', 'efor', 'eft ', 'eful', 'efus', 'egin', 'ehin', 'eigh', 'eing', 'eir ', 'eith', 'eive', 'el a', 'el i', 'el t', 'el, ', 'el. ', 'elat', 'eld ', 'elea', 'eles', 'elf ', 'elf,', 'elf.', 'elie', 'elig', 'elin', 'ell ', 'elle', 'elli', 'ello', 'ells', 'elop', 'elp ', 'els ', 'elve', 'ely ', 'ely,', 'em a', 'em i', 'em t', 'em, ', 'em. ', 'emai', 'eman', 'embe', 'emen', 'emin', 'emon', 'emor', 'empl', 'empt', 'ems ', 'emse', 'en a', 'en b', 'en c', 'en d', 'en e', 'en f', 'en g', 'en h', 'en i', 'en l', 'en m', 'en o', 'en p', 'en r', 'en s', 'en t', 'en w', 'en, ', 'en. ', 'ence', 'ench', 'enco', 'end ', 'end,', 'ende', 'endi', 'ends', 'ened', 'enem', 'ener', 'enge', 'enin', 'enly', 'enou', 'ens ', 'ense', 'ensi', 'ent ', 'ent,', 'ent.', 'enta', 'ente', 'enti', 'entl', 'entr', 'ents', 'entu', 'eopl', 'epar', 'epor', 'epre', 'ept ', 'epti', 'eque', 'er C', 'er M', 'er S', 'er a', 'er b', 'er c', 'er d', 'er e', 'er f', 'er g', 'er h', 'er i', 'er l', 'er m', 'er n', 'er o', 'er p', 'er r', 'er s', 'er t', 'er u', 'er w', \"er's\", 'er, ', 'er. ', 'eral', 'erat', 'erce', 'ere ', 'ere,', 'ered', 'eren', 'eres', 'erfu', 'erge', 'eria', 'eric', 'erie', 'erin', 'erio', 'erly', 'erma', 'ermi', 'ern ', 'erna', 'erne', 'erou', 'erri', 'ers ', 'ers,', 'ers.', 'erse', 'ersi', 'erso', 'erst', 'ert ', 'erta', 'erva', 'erve', 'ervi', 'ery ', 'eryo', 'es a', 'es b', 'es c', 'es d', 'es e', 'es f', 'es h', 'es i', 'es l', 'es m', 'es n', 'es o', 'es p', 'es r', 'es s', 'es t', 'es u', 'es w', 'es, ', 'es. ', 'esca', 'escr', 'escu', 'ese ', 'esen', 'eser', 'esid', 'espe', 'espi', 'espo', 'ess ', 'ess,', 'ess.', 'essa', 'esse', 'essf', 'essi', 'esso', 'est ', 'esta', 'este', 'esti', 'esto', 'estr', 'ests', 'esul', 'et a', 'et h', 'et i', 'et o', 'et t', 'et, ', 'et. ', 'etai', 'eter', 'ethe', 'ethi', 'etic', 'etin', 'ets ', 'ette', 'etti', 'etur', 'etwe', 'ety ', 'eve ', 'evea', 'evel', 'even', 'ever', 'eves', 'evil', 'evio', 'evol', 'ews ', 'exce', 'exis', 'expe', 'expl', 'ext ', 'extr', 'ey a', 'ey b', 'ey c', 'ey d', 'ey e', 'ey f', 'ey h', 'ey i', 'ey l', 'ey m', 'ey r', 'ey s', 'ey t', 'ey w', 'ey, ', 'ey. ', 'f a ', 'f an', 'f be', 'f co', 'f he', 'f hi', 'f in', 'f it', 'f ma', 'f re', 'f se', 'f th', 'f to', 'f wh', 'face', 'fact', 'fail', 'fair', 'fall', 'fami', 'far ', 'fath', 'fe a', 'fe i', 'fe o', 'fe, ', 'fe. ', 'fear', 'feat', 'fect', 'feel', 'fell', 'fer ', 'fere', 'fers', 'fess', 'few ', 'ff t', 'ffec', 'ffer', 'ffic', 'ffor', 'fice', 'fici', 'fied', 'figh', 'figu', 'fina', 'find', 'fire', 'firs', 'flee', 'foll', 'for ', 'forc', 'fore', 'forg', 'form', 'fort', 'foun', 'four', 'free', 'frie', 'from', 'fron', 'fter', 'ful ', 'full', 'furt', 'fuse', 'futu', 'g a ', 'g ab', 'g an', 'g as', 'g at', 'g be', 'g co', 'g de', 'g fo', 'g fr', 'g he', 'g hi', 'g in', 'g is', 'g it', 'g ma', 'g mo', 'g of', 'g on', 'g ou', 'g re', 'g se', 'g so', 'g st', 'g th', 'g to', 'g up', 'g wh', 'g wi', 'g wo', 'g, a', 'g, t', 'g. T', 'gain', 'gard', 'gate', 'gati', 'ge a', 'ge f', 'ge i', 'ge o', 'ge t', 'ge, ', 'ge. ', 'ged ', 'gene', 'gent', 'ger ', 'gers', 'ges ', 'gest', 'get ', 'geth', 'gets', 'gges', 'ggle', 'gh a', 'gh h', 'gh s', 'gh t', 'ghou', 'ght ', 'ght,', 'ght.', 'ghte', 'ghts', 'gica', 'gin ', 'gina', 'ging', 'gins', 'girl', 'give', 'gle ', 'gly ', 'go t', 'goes', 'goin', 'good', 'gove', 'gran', 'grea', 'gree', 'grou', 'grow', 'gs a', 'gs t', 'gs, ', 'guar', 'gue ', 'gues', 'gure', 'h a ', 'h an', 'h as', 'h co', 'h fo', 'h ha', 'h he', 'h hi', 'h in', 'h is', 'h it', 'h of', 'h on', 'h ot', 'h re', 'h sh', 'h th', 'h to', 'h wh', 'h wi', \"h's \", 'h, a', 'h. T', 'had ', 'half', 'hall', 'han ', 'hanc', 'hand', 'hang', 'happ', 'hara', 'hard', 'hare', 'has ', 'hat ', 'have', 'havi', 'he \"', 'he A', 'he B', 'he C', 'he D', 'he E', 'he F', 'he G', 'he H', 'he I', 'he K', 'he L', 'he M', 'he N', 'he P', 'he R', 'he S', 'he T', 'he U', 'he W', 'he a', 'he b', 'he c', 'he d', 'he e', 'he f', 'he g', 'he h', 'he i', 'he j', 'he k', 'he l', 'he m', 'he n', 'he o', 'he p', 'he r', 'he s', 'he t', 'he u', 'he v', 'he w', 'he y', 'head', 'heal', 'hear', 'hed ', 'heir', 'help', 'hem ', 'hem,', 'hem.', 'hems', 'hen ', 'her ', \"her'\", 'her,', 'her.', 'here', 'heri', 'hers', 'hes ', 'hese', 'hey ', 'hich', 'high', 'hild', 'hile', 'him ', 'him,', 'him.', 'hims', 'hin ', 'hind', 'hing', 'hink', 'hip ', 'hird', 'his ', 'his,', 'hist', 'hite', 'ho a', 'ho c', 'ho d', 'ho h', 'ho i', 'ho l', 'ho s', 'ho t', 'ho w', 'hold', 'hole', 'hom ', 'home', 'hood', 'hool', 'hope', 'hort', 'hose', 'host', 'houg', 'houl', 'hous', 'hout', 'how ', 'howe', 'hows', 'hrea', 'hree', 'hris', 'hrou', 'hrow', 'ht a', 'ht b', 'ht i', 'ht o', 'ht t', 'ht, ', 'ht. ', 'hter', 'hts ', 'huma', 'husb', 'hysi', 'ia a', 'ia, ', 'ia. ', 'iage', 'ial ', 'iall', 'ian ', 'ians', 'iate', 'ibes', 'ible', 'ic a', 'ic s', 'ic, ', 'ical', 'ican', 'icat', 'ice ', 'ice,', 'ice.', 'ices', 'ich ', 'icia', 'icid', 'icio', 'ick ', 'icke', 'ickl', 'ics ', 'icti', 'icul', 'id t', 'idde', 'iddl', 'ide ', 'ide.', 'idea', 'ided', 'iden', 'ider', 'ides', 'idin', 'ie, ', 'ied ', 'ief ', 'ield', 'ienc', 'iend', 'ient', 'ier ', 'ies ', 'ies,', 'ies.', 'iety', 'ieve', 'ife ', 'ife,', 'ife.', 'iffe', 'ific', 'ifie', 'iful', 'igat', 'igh ', 'ight', 'igin', 'ign ', 'igur', 'ike ', 'il t', 'ild ', 'ildr', 'ile ', 'ile,', 'iled', 'ilit', 'ill ', 'illa', 'ille', 'illi', 'ills', 'ils ', 'ily ', 'ily,', 'im a', 'im b', 'im f', 'im i', 'im o', 'im t', 'im w', 'im, ', 'im. ', 'imat', 'ime ', 'ime,', 'ime.', 'imes', 'imin', 'imme', 'impl', 'impo', 'impr', 'ims ', 'imse', 'in 1', 'in A', 'in C', 'in L', 'in M', 'in S', 'in a', 'in b', 'in c', 'in d', 'in e', 'in f', 'in h', 'in i', 'in l', 'in m', 'in o', 'in p', 'in r', 'in s', 'in t', 'in w', 'in, ', 'in. ', 'inal', 'inat', 'ince', 'inci', 'incl', 'incr', 'ind ', 'inde', 'indi', 'inds', 'ine ', 'ine,', 'ined', 'ines', 'info', 'ing ', 'ing,', 'ing.', 'inge', 'ingl', 'ings', 'inin', 'inis', 'init', 'ink ', 'inki', 'inks', 'inne', 'inni', 'ins ', 'insi', 'inst', 'int ', 'inte', 'into', 'intr', 'inue', 'inve', 'invi', 'invo', 'ion ', 'ion,', 'ion.', 'iona', 'ione', 'ions', 'ior ', 'ious', 'ip t', 'ip w', 'ip, ', 'ip. ', 'ips ', 'ir a', 'ir c', 'ir d', 'ir f', 'ir h', 'ir l', 'ir m', 'ir o', 'ir p', 'ir r', 'ir s', 'ir t', 'ir w', 'ird ', 'ire ', 'irec', 'ired', 'ires', 'irl ', 'irst', 'irth', 'is a', 'is b', 'is c', 'is d', 'is e', 'is f', 'is g', 'is h', 'is i', 'is k', 'is l', 'is m', 'is n', 'is o', 'is p', 'is r', 'is s', 'is t', 'is u', 'is v', 'is w', 'is, ', 'is. ', 'isap', 'isco', 'iscu', 'ise ', 'ised', 'ises', 'ish ', 'ishe', 'isin', 'isio', 'isit', 'ison', 'issi', 'ist ', 'ist,', 'ista', 'iste', 'isti', 'isto', 'istr', 'ists', 'it a', 'it b', 'it h', 'it i', 'it o', 'it s', 'it t', 'it w', 'it, ', 'it. ', 'ital', 'itar', 'itat', 'itch', 'ite ', 'ited', 'iter', 'ites', 'ith ', 'ithe', 'ithi', 'itho', 'itia', 'itic', 'itie', 'itin', 'itio', 'itle', 'itor', 'its ', 'itte', 'ittl', 'itua', 'ity ', 'ity,', 'ity.', 'ival', 'ivat', 'ive ', 'ive,', 'ive.', 'ived', 'ivel', 'iven', 'iver', 'ives', 'ivil', 'ivin', 'ize ', 'ized', 'izes', 'ject', 'join', 'jour', 'just', 'k an', 'k fo', 'k in', 'k is', 'k of', 'k on', 'k th', 'k to', \"k's \", 'k, a', 'ke a', 'ke h', 'ke o', 'ke t', 'ked ', 'keep', 'ken ', 'ker ', 'kes ', 'kill', 'kind', 'king', 'kly ', 'know', 'ks a', 'ks h', 'ks i', 'ks o', 'ks t', 'ks, ', 'l an', 'l as', 'l be', 'l ca', 'l ch', 'l co', 'l de', 'l di', 'l en', 'l fo', 'l fr', 'l ha', 'l he', 'l hi', 'l in', 'l is', 'l li', 'l ma', 'l no', 'l of', 'l pr', 'l re', 'l se', 'l st', 'l th', 'l to', 'l wa', 'l wh', 'l wi', \"l's \", 'l, a', 'l, t', 'l, w', 'l. A', 'l. T', 'lace', 'lack', 'lage', 'laim', 'lain', 'lan ', 'land', 'lane', 'lans', 'lant', 'lar ', 'larg', 'lass', 'last', 'late', 'lati', 'lay ', 'ld a', 'ld b', 'ld f', 'ld h', 'ld i', 'ld n', 'ld o', 'ld s', 'ld t', 'ld w', 'ld, ', 'ld. ', 'lder', 'ldin', 'ldre', 'lds ', 'le a', 'le b', 'le c', 'le d', 'le f', 'le h', 'le i', 'le o', 'le p', 'le s', 'le t', 'le w', 'le, ', 'le. ', 'lead', 'lear', 'leas', 'leav', 'lect', 'led ', 'ledg', 'leep', 'left', 'lege', 'len ', 'lent', 'ler ', 'les ', 'less', 'let ', 'lete', 'lett', 'lf a', 'lf i', 'lf t', 'lf, ', 'lf. ', 'lian', 'lic ', 'lica', 'lice', 'lier', 'lies', 'liev', 'life', 'ligh', 'like', 'line', 'ling', 'lion', 'lish', 'list', 'lita', 'lite', 'liti', 'litt', 'lity', 'live', 'livi', 'lize', 'll a', 'll b', 'll c', 'll d', 'll f', 'll h', 'll i', 'll n', 'll o', 'll p', 'll r', 'll s', 'll t', 'll w', 'll, ', 'll. ', 'llag', 'llec', 'lled', 'lleg', 'llen', 'ller', 'llia', 'llin', 'llio', 'llow', 'lls ', 'lly ', 'lly,', 'lmos', 'loca', 'lock', 'logi', 'logy', 'lone', 'long', 'lood', 'look', 'lose', 'lost', 'lot ', 'love', 'low ', 'lowe', 'lowi', 'lows', 'lrea', 'ls a', 'ls h', 'ls i', 'ls o', 'ls t', 'ls, ', 'ls. ', 'lso ', 'lter', 'ltho', 'lude', 'ludi', 'lusi', 'lved', 'lves', 'lway', 'ly a', 'ly b', 'ly c', 'ly d', 'ly e', 'ly f', 'ly g', 'ly h', 'ly i', 'ly l', 'ly m', 'ly o', 'ly p', 'ly r', 'ly s', 'ly t', 'ly u', 'ly w', 'ly, ', 'ly. ', 'm a ', 'm an', 'm as', 'm be', 'm fo', 'm fr', 'm he', 'm hi', 'm in', 'm is', 'm of', 'm on', 'm th', 'm to', 'm wh', 'm wi', 'm, a', 'm. A', 'm. H', 'm. T', 'made', 'mage', 'magi', 'main', 'make', 'maki', 'mall', 'man ', 'man,', 'man.', 'mana', 'mand', 'mani', 'mans', 'many', 'mark', 'marr', 'mast', 'mate', 'mati', 'may ', 'mber', 'mble', 'me a', 'me b', 'me f', 'me h', 'me i', 'me o', 'me p', 'me s', 'me t', 'me w', 'me, ', 'me. ', 'mean', 'med ', 'medi', 'meet', 'memb', 'memo', 'men ', 'ment', 'mer ', 'meri', 'mes ', 'meth', 'migh', 'mili', 'mily', 'mina', 'mind', 'mine', 'ming', 'mini', 'mise', 'miss', 'mist', 'mitt', 'mman', 'mmed', 'mmer', 'mmit', 'mmon', 'mmun', 'mode', 'mon ', 'mone', 'mong', 'mons', 'mont', 'more', 'most', 'moth', 'moti', 'moun', 'mous', 'move', 'mpan', 'mper', 'mple', 'mpli', 'mpor', 'mpt ', 'mpts', 'ms a', 'ms o', 'ms t', 'msel', 'much', 'muni', 'murd', 'must', 'myst', 'n Ma', 'n a ', 'n ab', 'n ac', 'n ad', 'n af', 'n ag', 'n al', 'n an', 'n ar', 'n as', 'n at', 'n ba', 'n be', 'n bo', 'n by', 'n ca', 'n ch', 'n co', 'n de', 'n di', 'n do', 'n en', 'n ev', 'n ex', 'n fa', 'n fi', 'n fo', 'n fr', 'n go', 'n ha', 'n he', 'n hi', 'n ho', 'n im', 'n in', 'n is', 'n it', 'n le', 'n li', 'n lo', 'n ma', 'n me', 'n mo', 'n na', 'n of', 'n ol', 'n on', 'n or', 'n ou', 'n pa', 'n po', 'n pr', 'n re', 'n se', 'n sh', 'n so', 'n sp', 'n st', 'n su', 'n ta', 'n te', 'n th', 'n to', 'n tr', 'n un', 'n wa', 'n wh', 'n wi', 'n wo', \"n's \", \"n't \", 'n, a', 'n, b', 'n, h', 'n, i', 'n, s', 'n, t', 'n, w', 'n. A', 'n. H', 'n. I', 'n. S', 'n. T', 'n. W', 'na, ', 'nabl', 'nage', 'nal ', 'nali', 'nall', 'name', 'nant', 'narr', 'nate', 'nati', 'natu', 'nce ', 'nce,', 'nce.', 'nced', 'ncer', 'nces', 'nch ', 'ncie', 'nclu', 'ncou', 'ncre', 'nd A', 'nd B', 'nd C', 'nd D', 'nd E', 'nd G', 'nd H', 'nd J', 'nd L', 'nd M', 'nd P', 'nd R', 'nd S', 'nd T', 'nd a', 'nd b', 'nd c', 'nd d', 'nd e', 'nd f', 'nd g', 'nd h', 'nd i', 'nd k', 'nd l', 'nd m', 'nd n', 'nd o', 'nd p', 'nd r', 'nd s', 'nd t', 'nd u', 'nd w', 'nd, ', 'nd. ', 'nded', 'nder', 'ndin', 'ndon', 'nds ', 'nds,', 'nds.', 'ne a', 'ne b', 'ne c', 'ne d', 'ne f', 'ne h', 'ne i', 'ne o', 'ne s', 'ne t', 'ne w', 'ne, ', 'ne. ', 'near', 'nect', 'ned ', 'need', 'nent', 'ner ', 'nera', 'ners', 'nes ', 'ness', 'neve', 'new ', 'news', 'next', 'ney ', 'nfor', 'nfro', 'ng S', 'ng a', 'ng b', 'ng c', 'ng d', 'ng e', 'ng f', 'ng g', 'ng h', 'ng i', 'ng l', 'ng m', 'ng n', 'ng o', 'ng p', 'ng r', 'ng s', 'ng t', 'ng u', 'ng w', 'ng, ', 'ng. ', 'nge ', 'nged', 'nger', 'nges', 'ngin', 'ngle', 'ngly', 'ngs ', 'nica', 'nigh', 'ning', 'nion', 'nish', 'nist', 'nite', 'niti', 'nity', 'nive', 'nize', 'nkin', 'nks ', 'nly ', 'nmen', 'nnec', 'nner', 'nnin', 'nnot', 'nor ', 'not ', 'note', 'noth', 'noti', 'noug', 'nove', 'now ', 'nowl', 'nown', 'nows', 'ns a', 'ns b', 'ns f', 'ns h', 'ns i', 'ns o', 'ns t', 'ns w', 'ns, ', 'ns. ', 'nse ', 'nshi', 'nsid', 'nsis', 'nst ', 'nsta', 'nste', 'nstr', 'nt a', 'nt b', 'nt c', 'nt d', 'nt f', 'nt h', 'nt i', 'nt o', 'nt p', 'nt s', 'nt t', 'nt w', 'nt, ', 'nt. ', 'ntac', 'ntai', 'ntal', 'nted', 'ntel', 'nten', 'nter', 'ntia', 'ntic', 'ntil', 'ntin', 'ntio', 'ntir', 'ntit', 'ntly', 'nto ', 'ntra', 'ntri', 'ntro', 'ntry', 'nts ', 'nts,', 'nts.', 'ntua', 'ntur', 'nues', 'numb', 'nver', 'nves', 'nvin', 'nvit', 'nvol', 'nwhi', 'ny o', 'o a ', 'o ac', 'o al', 'o an', 'o ar', 'o as', 'o at', 'o be', 'o br', 'o ca', 'o ch', 'o co', 'o de', 'o di', 'o do', 'o en', 'o es', 'o ex', 'o fa', 'o fi', 'o fo', 'o ge', 'o gi', 'o go', 'o ha', 'o he', 'o hi', 'o in', 'o is', 'o ke', 'o ki', 'o le', 'o li', 'o lo', 'o ma', 'o me', 'o mo', 'o no', 'o of', 'o on', 'o pa', 'o pr', 'o pu', 'o re', 'o sa', 'o se', 'o sh', 'o so', 'o sp', 'o st', 'o su', 'o ta', 'o te', 'o th', 'o to', 'o tr', 'o un', 'o us', 'o wa', 'o wh', 'o wi', 'o wo', 'o, a', 'oach', 'oard', 'oble', 'obse', 'ocal', 'ocat', 'occu', 'ocia', 'ocie', 'ock ', 'ocke', 'octo', 'od a', 'od, ', 'od. ', 'oduc', 'ody ', 'oes ', 'oesn', 'of A', 'of B', 'of C', 'of D', 'of E', 'of G', 'of H', 'of J', 'of L', 'of M', 'of P', 'of R', 'of S', 'of T', 'of a', 'of b', 'of c', 'of d', 'of e', 'of f', 'of g', 'of h', 'of i', 'of l', 'of m', 'of o', 'of p', 'of r', 'of s', 'of t', 'of w', 'off ', 'offe', 'offi', 'oget', 'ogni', 'ogra', 'oice', 'oin ', 'oing', 'oint', 'ok i', 'ok, ', 'oken', 'okin', 'oks ', 'ol, ', 'old ', 'olde', 'oldi', 'ole ', 'olen', 'olic', 'olit', 'olla', 'olle', 'ollo', 'olog', 'olut', 'olve', 'om a', 'om h', 'om o', 'om s', 'om t', 'om w', 'om, ', 'oman', 'ome ', 'ome,', 'ome.', 'omen', 'omes', 'omet', 'omin', 'omis', 'omma', 'ommi', 'ommu', 'ompa', 'ompl', 'on a', 'on b', 'on c', 'on d', 'on e', 'on f', 'on h', 'on i', 'on l', 'on m', 'on o', 'on p', 'on r', 'on s', 'on t', 'on w', \"on's\", 'on, ', 'on. ', 'onal', 'once', 'oncl', 'ond ', 'onde', 'ondo', 'one ', 'one,', 'one.', 'oned', 'ones', 'oney', 'onfi', 'onfr', 'ong ', 'onge', 'onis', 'only', 'onne', 'ons ', 'ons,', 'ons.', 'onse', 'onsh', 'onsi', 'onst', 'ont ', 'onta', 'onte', 'onth', 'onti', 'ontr', 'onve', 'onvi', 'ood ', 'ook ', 'ook,', 'ooki', 'ooks', 'ool ', 'oom ', 'oon ', 'oor ', 'ope ', 'open', 'oper', 'ople', 'oppo', 'ops ', 'opul', 'or a', 'or b', 'or c', 'or d', 'or e', 'or f', 'or h', 'or i', 'or m', 'or o', 'or p', 'or r', 'or s', 'or t', 'or w', 'or, ', 'or. ', 'orat', 'orce', 'ord ', 'orde', 'ordi', 'ords', 'ore ', 'ored', 'ores', 'orga', 'orge', 'orie', 'orig', 'orit', 'ork ', 'orki', 'orks', 'orld', 'orm ', 'orma', 'orme', 'orms', 'orn ', 'orni', 'orri', 'ors ', 'orse', 'ort ', 'orta', 'orte', 'orth', 'orti', 'orts', 'ortu', 'ory ', 'ory,', 'ory.', 'ose ', 'osed', 'oses', 'osin', 'osit', 'oss ', 'osse', 'ossi', 'ost ', 'ot a', 'ot b', 'ot h', 'ot i', 'ot o', 'ot r', 'ot s', 'ot t', 'ot w', 'otag', 'ote ', 'otec', 'oth ', 'othe', 'othi', 'otic', 'otio', 'ots ', 'oubl', 'ough', 'ould', 'ounc', 'ound', 'oung', 'ount', 'oup ', 'our ', 'ourn', 'ours', 'ourt', 'ous ', 'ouse', 'ousl', 'out ', 'outh', 'outs', 'ove ', 'oved', 'ovel', 'over', 'oves', 'ovid', 'ovin', 'ow a', 'ow h', 'ow i', 'ow t', 'ow, ', 'owar', 'owed', 'ower', 'owev', 'owin', 'owle', 'own ', 'own,', 'own.', 'owne', 'ows ', 'oyed', 'oys ', 'p an', 'p hi', 'p in', 'p of', 'p th', 'p to', 'p wi', 'pain', 'pair', 'pani', 'pany', 'para', 'pare', 'part', 'pass', 'past', 'path', 'peak', 'pear', 'peci', 'pect', 'ped ', 'pen ', 'pend', 'pene', 'pens', 'peop', 'per ', 'pera', 'perf', 'peri', 'pers', 'pes ', 'phys', 'ping', 'pire', 'piri', 'pita', 'pite', 'plac', 'plai', 'plan', 'play', 'ple ', 'plet', 'plic', 'plot', 'ply ', 'poin', 'poli', 'pon ', 'pons', 'popu', 'port', 'pose', 'posi', 'poss', 'powe', 'ppar', 'ppea', 'pped', 'ppen', 'ppin', 'ppor', 'ppos', 'ppro', 'prep', 'pres', 'pret', 'prev', 'prin', 'pris', 'proa', 'prob', 'proc', 'prod', 'prof', 'prom', 'prop', 'prot', 'prov', 'ps a', 'ps t', 'pt t', 'pted', 'pter', 'ptio', 'pts ', 'ptur', 'publ', 'pula', 'purs', 'put ', 'quen', 'ques', 'quic', 'quir', 'r a ', 'r al', 'r an', 'r ar', 'r as', 'r at', 'r ba', 'r be', 'r bo', 'r br', 'r by', 'r ca', 'r ch', 'r co', 'r da', 'r de', 'r di', 'r en', 'r ex', 'r fa', 'r fi', 'r fo', 'r fr', 'r ha', 'r he', 'r hi', 'r ho', 'r hu', 'r in', 'r is', 'r la', 'r le', 'r li', 'r lo', 'r ma', 'r me', 'r mi', 'r mo', 'r na', 'r ne', 'r no', 'r of', 'r on', 'r ow', 'r pa', 'r pe', 'r pl', 'r po', 'r pr', 'r re', 'r se', 'r sh', 'r si', 'r so', 'r st', 'r su', 'r te', 'r th', 'r to', 'r tr', 'r un', 'r wa', 'r wh', 'r wi', 'r wo', \"r's \", 'r, M', 'r, a', 'r, b', 'r, h', 'r, i', 'r, s', 'r, t', 'r, w', 'r-ol', 'r. A', 'r. H', 'r. I', 'r. S', 'r. T', 'r. W', 'rabl', 'race', 'rack', 'ract', 'rade', 'radi', 'rage', 'rain', 'rais', 'ral ', 'rall', 'ranc', 'rand', 'rang', 'rans', 'rant', 'rate', 'rath', 'rati', 'rato', 'rave', 'rce ', 'rced', 'rces', 'rch ', 'rchi', 'rd a', 'rd t', 'rd, ', 'rd. ', 'rder', 'rdin', 'rds ', 're a', 're b', 're c', 're d', 're e', 're f', 're g', 're h', 're i', 're l', 're m', 're n', 're o', 're p', 're r', 're s', 're t', 're w', 're, ', 're. ', 'reac', 'read', 'reak', 'real', 'ream', 'reas', 'reat', 'rece', 'reco', 'rect', 'red ', 'red,', 'red.', 'redi', 'ree ', 'reed', 'rees', 'reet', 'refe', 'refu', 'rega', 'rela', 'rele', 'reli', 'rely', 'rema', 'reme', 'remo', 'ren ', 'renc', 'rent', 'repa', 'repo', 'requ', 'res ', 'resc', 'rese', 'resi', 'reso', 'resp', 'ress', 'rest', 'resu', 'ret ', 'retu', 'reve', 'revi', 'rew ', 'rful', 'rgan', 'rge ', 'riag', 'rial', 'rian', 'ribe', 'rica', 'rick', 'ried', 'rief', 'rien', 'ries', 'rifi', 'righ', 'rigi', 'rime', 'rinc', 'ring', 'rior', 'riou', 'rise', 'riso', 'rist', 'rite', 'riti', 'ritt', 'rity', 'riva', 'rive', 'rk, ', 'rkin', 'rks ', 'rld ', 'rlie', 'rly ', 'rman', 'rmat', 'rmed', 'rmer', 'rmin', 'rms ', 'rmy ', 'rn t', 'rnal', 'rnat', 'rned', 'rney', 'rnin', 'rns ', 'roac', 'robl', 'roce', 'rodu', 'roke', 'rol ', 'rom ', 'romi', 'rong', 'ront', 'room', 'rope', 'ross', 'rote', 'roth', 'roub', 'roug', 'roun', 'roup', 'rous', 'rove', 'rovi', 'row ', 'rown', 'rows', 'roy ', 'rpri', 'rran', 'rrat', 'rren', 'rres', 'rria', 'rrie', 'rriv', 'rror', 'rry ', 'rs a', 'rs b', 'rs f', 'rs h', 'rs i', 'rs l', 'rs o', 'rs s', 'rs t', 'rs w', 'rs, ', 'rs. ', 'rse ', 'rsel', 'rson', 'rst ', 'rsta', 'rt a', 'rt o', 'rt t', 'rt, ', 'rt. ', 'rtai', 'rted', 'rter', 'rth ', 'rthe', 'rtic', 'rtin', 'rtly', 'rts ', 'rtun', 'rty ', 'ruct', 'rue ', 'rugg', 'rule', 'run ', 'runs', 'rupt', 'rust', 'ruth', 'rviv', 'rwar', 'ry a', 'ry b', 'ry c', 'ry d', 'ry f', 'ry h', 'ry i', 'ry o', 'ry s', 'ry t', 'ry w', 'ry, ', 'ry. ', 'ryin', 'ryon', 's Ma', 's a ', 's ab', 's ac', 's ad', 's af', 's ag', 's al', 's an', 's ap', 's ar', 's as', 's at', 's aw', 's ba', 's be', 's bo', 's br', 's bu', 's by', 's ca', 's ch', 's cl', 's co', 's cr', 's da', 's de', 's di', 's do', 's dr', 's du', 's ea', 's en', 's es', 's ev', 's ex', 's fa', 's fe', 's fi', 's fo', 's fr', 's fu', 's gi', 's go', 's gr', 's ha', 's he', 's hi', 's ho', 's im', 's in', 's is', 's it', 's ju', 's ki', 's kn', 's la', 's le', 's li', 's lo', 's ma', 's me', 's mi', 's mo', 's mu', 's na', 's ne', 's no', 's of', 's ol', 's on', 's or', 's ou', 's ov', 's ow', 's pa', 's pe', 's pl', 's po', 's pr', 's pu', 's qu', 's ra', 's re', 's ro', 's sa', 's se', 's sh', 's si', 's so', 's sp', 's st', 's su', 's ta', 's te', 's th', 's ti', 's to', 's tr', 's tw', 's un', 's up', 's us', 's ve', 's vi', 's wa', 's we', 's wh', 's wi', 's wo', 's, a', 's, b', 's, c', 's, h', 's, i', 's, o', 's, s', 's, t', 's, w', 's. A', 's. B', 's. D', 's. H', 's. I', 's. M', 's. O', 's. S', 's. T', 's. W', 'safe', 'sage', 'same', 'sapp', 'sati', 'save', 'says', 'sban', 'scap', 'scen', 'scho', 'scie', 'scov', 'scri', 'scue', 'scus', 'se a', 'se c', 'se f', 'se h', 'se i', 'se o', 'se s', 'se t', 'se w', 'se, ', 'se. ', 'sear', 'seco', 'secr', 'secu', 'sed ', 'see ', 'seek', 'seem', 'seen', 'sees', 'self', 'selv', 'send', 'sens', 'sent', 'sequ', 'seri', 'serv', 'ses ', 'ses,', 'sess', 'set ', 'sets', 'sett', 'seve', 'sful', 'sh t', 'shar', 'she ', 'shed', 'shes', 'shin', 'ship', 'shor', 'shou', 'show', 'sibl', 'sica', 'side', 'sign', 'sinc', 'sine', 'sing', 'sion', 'sist', 'sit ', 'site', 'siti', 'sits', 'situ', 'sive', 'sks ', 'slan', 'slee', 'sly ', 'smal', \"sn't\", 'so a', 'so h', 'so s', 'so t', 'soci', 'sold', 'solv', 'some', 'son ', 'son,', 'son.', 'sona', 'sone', 'sons', 'soon', 'spea', 'spec', 'spen', 'sper', 'spir', 'spit', 'spon', 'ss a', 'ss i', 'ss o', 'ss t', 'ss, ', 'ss. ', 'ssag', 'ssed', 'sses', 'ssfu', 'ssib', 'ssin', 'ssio', 'ssis', 'st a', 'st b', 'st c', 'st d', 'st e', 'st f', 'st h', 'st i', 'st l', 'st m', 'st o', 'st p', 'st r', 'st s', 'st t', 'st w', 'st, ', 'st. ', 'stab', 'stan', 'star', 'stat', 'stay', 'stea', 'sted', 'stem', 'sten', 'ster', 'stic', 'stig', 'stil', 'stin', 'stio', 'stit', 'ston', 'stop', 'stor', 'stra', 'stre', 'stri', 'stro', 'stru', 'sts ', 'stud', 'sual', 'succ', 'such', 'suff', 'sugg', 'suit', 'sult', 'sume', 'summ', 'supe', 'supp', 'sure', 'surp', 'surr', 'surv', 'susp', 't Ma', 't a ', 't ag', 't al', 't an', 't ar', 't as', 't at', 't ba', 't be', 't by', 't ca', 't ch', 't co', 't da', 't de', 't di', 't do', 't en', 't ev', 't ex', 't fa', 't fi', 't fo', 't fr', 't ha', 't he', 't hi', 't ho', 't in', 't is', 't it', 't le', 't li', 't lo', 't ma', 't me', 't mo', 't no', 't of', 't on', 't ou', 't pa', 't pe', 't po', 't pr', 't re', 't se', 't sh', 't so', 't st', 't su', 't ta', 't te', 't th', 't ti', 't to', 't tr', 't un', 't up', 't wa', 't wh', 't wi', 't wo', \"t's \", 't, a', 't, b', 't, h', 't, i', 't, s', 't, t', 't, w', 't. A', 't. H', 't. I', 't. S', 't. T', 't. W', 'tabl', 'tack', 'tact', 'tage', 'tail', 'tain', 'take', 'taki', 'tal ', 'tali', 'talk', 'tall', 'tanc', 'tand', 'tant', 'tart', 'tary', 'tate', 'tati', 'tay ', 'tch ', 'tche', 'te a', 'te f', 'te h', 'te i', 'te o', 'te s', 'te t', 'te w', 'te, ', 'te. ', 'teac', 'tead', 'tect', 'ted ', 'ted,', 'ted.', 'teen', 'tell', 'tely', 'temp', 'ten ', 'tenc', 'tend', 'tens', 'tent', 'ter ', 'ter,', 'ter.', 'tera', 'tere', 'teri', 'term', 'tern', 'terr', 'ters', 'tes ', 'test', 'th A', 'th C', 'th M', 'th a', 'th b', 'th c', 'th f', 'th h', 'th i', 'th m', 'th o', 'th p', 'th s', 'th t', 'th w', 'th, ', 'th. ', 'than', 'that', 'the ', 'thei', 'them', 'then', 'ther', 'thes', 'they', 'thin', 'thir', 'this', 'thor', 'thos', 'thou', 'thre', 'thro', 'ths ', 'thus', 'thy ', 'tial', 'tic ', 'tica', 'tice', 'tici', 'ticu', 'ties', 'tifi', 'tifu', 'tiga', 'til ', 'till', 'tima', 'time', 'tine', 'ting', 'tinu', 'tion', 'tire', 'tist', 'titu', 'tive', 'tle ', 'tles', 'tly ', 'tmen', 'to A', 'to B', 'to C', 'to E', 'to L', 'to M', 'to S', 'to a', 'to b', 'to c', 'to d', 'to e', 'to f', 'to g', 'to h', 'to i', 'to k', 'to l', 'to m', 'to o', 'to p', 'to r', 'to s', 'to t', 'to u', 'to v', 'to w', 'toge', 'told', 'ton ', 'too ', 'top ', 'tor ', 'tor,', 'tore', 'tori', 'tors', 'tory', 'towa', 'town', 'trac', 'trad', 'trai', 'tran', 'trap', 'trat', 'trav', 'trea', 'tree', 'tres', 'tria', 'tric', 'trie', 'trip', 'trod', 'trol', 'tron', 'trou', 'troy', 'truc', 'true', 'trug', 'try ', 'tryi', 'ts a', 'ts b', 'ts c', 'ts f', 'ts h', 'ts i', 'ts o', 'ts s', 'ts t', 'ts w', 'ts, ', 'ts. ', 'tsid', 'ttac', 'tted', 'ttem', 'tten', 'tter', 'ttin', 'ttle', 'tual', 'tuat', 'tude', 'tura', 'ture', 'turn', 'twee', 'two ', 'ty a', 'ty i', 'ty o', 'ty t', 'ty w', 'ty, ', 'ty. ', 'ual ', 'uall', 'uard', 'uati', 'uble', 'ubli', 'ucce', 'uced', 'uch ', 'ucti', 'udde', 'ude ', 'udin', 'ue t', 'ued ', 'uenc', 'uent', 'ues ', 'uest', 'uffe', 'ugge', 'uggl', 'ugh ', 'ught', 'uick', 'uild', 'uilt', 'ular', 'ulat', 'uld ', 'ull ', 'ully', 'ult ', 'ulti', 'uman', 'umbe', 'unab', 'unce', 'unco', 'und ', 'unde', 'unds', 'ung ', 'unge', 'unit', 'uns ', 'unt ', 'unte', 'unti', 'untr', 'up a', 'up i', 'up o', 'up t', 'up w', 'uper', 'upon', 'uppo', 'ural', 'urde', 'ure ', 'ure,', 'ure.', 'ured', 'ures', 'urin', 'urn ', 'urne', 'urni', 'urns', 'urpr', 'urre', 'urse', 'ursu', 'urth', 'urvi', 'us a', 'us c', 'us i', 'us o', 'us s', 'us t', 'us, ', 'us. ', 'usba', 'use ', 'use,', 'use.', 'used', 'uses', 'usin', 'usio', 'usly', 'uspe', 'ussi', 'ust ', 'ustr', 'ut a', 'ut b', 'ut d', 'ut f', 'ut h', 'ut i', 'ut n', 'ut o', 'ut s', 'ut t', 'ut w', 'ut, ', 'ute ', 'uth ', 'utho', 'utio', 'uts ', 'utsi', 'utur', 'val ', 'vant', 'vari', 'vate', 've a', 've b', 've c', 've d', 've f', 've h', 've i', 've o', 've s', 've t', 've w', 've, ', 've. ', 'veal', 'ved ', 'vel ', 'velo', 'vels', 'vely', 'ven ', 'veng', 'vent', 'ver ', 'ver,', 'ver.', 'vera', 'vere', 'veri', 'vern', 'vers', 'vert', 'very', 'ves ', 'ves,', 'ves.', 'vest', 'vice', 'vict', 'vide', 'view', 'vil ', 'vill', 'vinc', 'ving', 'viol', 'viou', 'visi', 'vive', 'volu', 'volv', 'w an', 'w of', 'w th', 'w to', 'wait', 'wake', 'walk', 'want', 'war ', 'ward', 'ware', 'warn', 'was ', 'watc', 'wate', 'way ', 'way,', 'way.', 'ways', 'weal', 'wed ', 'week', 'ween', 'well', 'went', 'wer ', 'were', 'wers', 'weve', 'what', 'when', 'wher', 'whic', 'whil', 'who ', 'whom', 'whos', 'wife', 'will', 'wing', 'wish', 'with', 'wled', 'wn a', 'wn i', 'wn o', 'wn t', 'wn, ', 'wn. ', 'woma', 'wome', 'word', 'work', 'worl', 'woul', 'writ', 'ws h', 'ws t', 'xist', 'xpec', 'xper', 'xpla', 'xplo', 'y a ', 'y ab', 'y ac', 'y af', 'y al', 'y an', 'y ar', 'y as', 'y at', 'y ba', 'y be', 'y bu', 'y by', 'y ca', 'y ch', 'y co', 'y de', 'y di', 'y do', 'y en', 'y ex', 'y fa', 'y fi', 'y fo', 'y fr', 'y go', 'y ha', 'y he', 'y hi', 'y ho', 'y in', 'y is', 'y le', 'y li', 'y lo', 'y ma', 'y me', 'y mo', 'y no', 'y of', 'y on', 'y pa', 'y pr', 'y re', 'y se', 'y sh', 'y so', 'y st', 'y su', 'y ta', 'y te', 'y th', 'y to', 'y tr', 'y un', 'y wa', 'y we', 'y wh', 'y wi', 'y wo', \"y's \", 'y, a', 'y, b', 'y, h', 'y, i', 'y, s', 'y, t', 'y, w', 'y. A', 'y. H', 'y. I', 'y. S', 'y. T', 'year', 'yed ', 'ying', 'yone', 'youn', 'ys a', 'ys t', 'ysic', 'yste', 'ythi', 'zed ', 'zes ', 'zing']\n",
      "pos_n_grams_1 ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
      "pos_n_grams_2 ['CC CD', 'CC DT', 'CC IN', 'CC JJ', 'CC MD', 'CC NN', 'CC NNP', 'CC NNS', 'CC PRP', 'CC PRP$', 'CC RB', 'CC TO', 'CC VB', 'CC VBD', 'CC VBG', 'CC VBN', 'CC VBP', 'CC VBZ', 'CC WRB', 'CD CC', 'CD CD', 'CD DT', 'CD IN', 'CD JJ', 'CD NN', 'CD NNP', 'CD NNS', 'DT CD', 'DT DT', 'DT IN', 'DT JJ', 'DT JJR', 'DT JJS', 'DT NN', 'DT NNP', 'DT NNPS', 'DT NNS', 'DT PRP', 'DT RB', 'DT VBG', 'DT VBN', 'DT VBP', 'DT VBZ', 'EX VBZ', 'FW NN', 'IN CC', 'IN CD', 'IN DT', 'IN EX', 'IN IN', 'IN JJ', 'IN NN', 'IN NNP', 'IN NNS', 'IN PDT', 'IN PRP', 'IN PRP$', 'IN RB', 'IN TO', 'IN VBG', 'IN VBN', 'IN WDT', 'IN WP', 'IN WRB', 'JJ CC', 'JJ CD', 'JJ DT', 'JJ IN', 'JJ JJ', 'JJ NN', 'JJ NNP', 'JJ NNS', 'JJ PRP', 'JJ RB', 'JJ TO', 'JJ VBG', 'JJ VBN', 'JJ VBZ', 'JJ WRB', 'JJR IN', 'JJR NN', 'JJR NNS', 'JJS IN', 'JJS NN', 'MD RB', 'MD VB', 'NN CC', 'NN CD', 'NN DT', 'NN EX', 'NN IN', 'NN JJ', 'NN MD', 'NN NN', 'NN NNP', 'NN NNS', 'NN PRP', 'NN PRP$', 'NN RB', 'NN TO', 'NN VB', 'NN VBD', 'NN VBG', 'NN VBN', 'NN VBP', 'NN VBZ', 'NN WDT', 'NN WP', 'NN WRB', 'NNP CC', 'NNP CD', 'NNP DT', 'NNP FW', 'NNP IN', 'NNP JJ', 'NNP MD', 'NNP NN', 'NNP NNP', 'NNP NNPS', 'NNP NNS', 'NNP PRP', 'NNP PRP$', 'NNP RB', 'NNP TO', 'NNP VB', 'NNP VBD', 'NNP VBG', 'NNP VBN', 'NNP VBP', 'NNP VBZ', 'NNP WDT', 'NNP WP', 'NNP WRB', 'NNS CC', 'NNS CD', 'NNS DT', 'NNS IN', 'NNS JJ', 'NNS MD', 'NNS NN', 'NNS NNP', 'NNS NNS', 'NNS PRP', 'NNS PRP$', 'NNS RB', 'NNS TO', 'NNS VBD', 'NNS VBG', 'NNS VBN', 'NNS VBP', 'NNS VBZ', 'NNS WDT', 'NNS WP', 'NNS WRB', 'PDT DT', 'PRP CC', 'PRP DT', 'PRP IN', 'PRP JJ', 'PRP MD', 'PRP NN', 'PRP NNP', 'PRP NNS', 'PRP PRP', 'PRP RB', 'PRP RP', 'PRP TO', 'PRP VB', 'PRP VBD', 'PRP VBG', 'PRP VBP', 'PRP VBZ', 'PRP$ CC', 'PRP$ DT', 'PRP$ IN', 'PRP$ JJ', 'PRP$ JJS', 'PRP$ NN', 'PRP$ NNP', 'PRP$ NNS', 'PRP$ RB', 'PRP$ TO', 'PRP$ VBG', 'RB CC', 'RB CD', 'RB DT', 'RB IN', 'RB JJ', 'RB NN', 'RB NNP', 'RB NNS', 'RB PRP', 'RB PRP$', 'RB RB', 'RB RBR', 'RB TO', 'RB VB', 'RB VBD', 'RB VBG', 'RB VBN', 'RB VBP', 'RB VBZ', 'RB WRB', 'RBR JJ', 'RBS JJ', 'RP CC', 'RP DT', 'RP IN', 'RP NNP', 'RP PRP$', 'RP RB', 'RP TO', 'RP VBG', 'TO DT', 'TO JJ', 'TO NN', 'TO NNP', 'TO NNS', 'TO PRP', 'TO PRP$', 'TO RB', 'TO VB', 'TO VBG', 'VB CC', 'VB CD', 'VB DT', 'VB IN', 'VB JJ', 'VB NN', 'VB NNP', 'VB NNS', 'VB PRP', 'VB PRP$', 'VB RB', 'VB RP', 'VB TO', 'VB VB', 'VB VBG', 'VB VBN', 'VB WP', 'VB WRB', 'VBD CC', 'VBD CD', 'VBD DT', 'VBD IN', 'VBD JJ', 'VBD NN', 'VBD NNP', 'VBD NNS', 'VBD PRP', 'VBD PRP$', 'VBD RB', 'VBD RP', 'VBD TO', 'VBD VBG', 'VBD VBN', 'VBG CC', 'VBG DT', 'VBG IN', 'VBG JJ', 'VBG NN', 'VBG NNP', 'VBG NNS', 'VBG PRP', 'VBG PRP$', 'VBG RB', 'VBG RP', 'VBG TO', 'VBG VBN', 'VBN CC', 'VBN DT', 'VBN IN', 'VBN JJ', 'VBN NN', 'VBN NNP', 'VBN NNS', 'VBN PRP', 'VBN PRP$', 'VBN RB', 'VBN RP', 'VBN TO', 'VBN VBG', 'VBN VBN', 'VBN WRB', 'VBP DT', 'VBP IN', 'VBP JJ', 'VBP NN', 'VBP NNP', 'VBP NNS', 'VBP PRP', 'VBP PRP$', 'VBP RB', 'VBP RP', 'VBP TO', 'VBP VBG', 'VBP VBN', 'VBZ CC', 'VBZ CD', 'VBZ DT', 'VBZ IN', 'VBZ JJ', 'VBZ NN', 'VBZ NNP', 'VBZ NNS', 'VBZ PRP', 'VBZ PRP$', 'VBZ RB', 'VBZ RP', 'VBZ TO', 'VBZ VB', 'VBZ VBG', 'VBZ VBN', 'VBZ WRB', 'WDT DT', 'WDT MD', 'WDT NNP', 'WDT PRP', 'WDT VBD', 'WDT VBP', 'WDT VBZ', 'WP MD', 'WP NNP', 'WP PRP', 'WP RB', 'WP VBD', 'WP VBP', 'WP VBZ', 'WP$ NN', 'WRB DT', 'WRB NNP', 'WRB PRP', 'WRB PRP$']\n",
      "pos_n_grams_3 ['CC DT JJ', 'CC DT NN', 'CC DT NNP', 'CC DT NNS', 'CC IN DT', 'CC IN NNP', 'CC IN PRP', 'CC JJ NN', 'CC JJ NNP', 'CC JJ NNS', 'CC NN CC', 'CC NN IN', 'CC NN NN', 'CC NN NNP', 'CC NN NNS', 'CC NN TO', 'CC NNP CC', 'CC NNP DT', 'CC NNP IN', 'CC NNP NN', 'CC NNP NNP', 'CC NNP NNS', 'CC NNP RB', 'CC NNP VBD', 'CC NNP VBP', 'CC NNP VBZ', 'CC NNS IN', 'CC NNS NNP', 'CC NNS TO', 'CC NNS VBP', 'CC PRP VBP', 'CC PRP VBZ', 'CC PRP$ JJ', 'CC PRP$ NN', 'CC PRP$ NNS', 'CC RB DT', 'CC RB IN', 'CC RB JJ', 'CC RB NNS', 'CC RB VB', 'CC RB VBZ', 'CC TO VB', 'CC VB DT', 'CC VB IN', 'CC VB PRP', 'CC VB PRP$', 'CC VBG DT', 'CC VBN IN', 'CC VBZ DT', 'CC VBZ IN', 'CC VBZ NNP', 'CC VBZ PRP', 'CC VBZ PRP$', 'CC VBZ RB', 'CC VBZ TO', 'CC VBZ VBN', 'CD IN DT', 'CD IN NNP', 'CD IN PRP$', 'CD JJ NNS', 'CD NN IN', 'CD NN JJ', 'CD NN NN', 'CD NNS IN', 'CD NNS RB', 'CD NNS VBP', 'DT CD NN', 'DT CD NNS', 'DT IN DT', 'DT JJ CC', 'DT JJ CD', 'DT JJ IN', 'DT JJ JJ', 'DT JJ NN', 'DT JJ NNP', 'DT JJ NNS', 'DT JJR NN', 'DT JJS NN', 'DT NN CC', 'DT NN CD', 'DT NN DT', 'DT NN IN', 'DT NN JJ', 'DT NN MD', 'DT NN NN', 'DT NN NNP', 'DT NN NNS', 'DT NN PRP', 'DT NN PRP$', 'DT NN RB', 'DT NN TO', 'DT NN VBD', 'DT NN VBG', 'DT NN VBN', 'DT NN VBP', 'DT NN VBZ', 'DT NN WDT', 'DT NN WP', 'DT NN WRB', 'DT NNP CC', 'DT NNP DT', 'DT NNP IN', 'DT NNP JJ', 'DT NNP NN', 'DT NNP NNP', 'DT NNP NNPS', 'DT NNP NNS', 'DT NNP PRP', 'DT NNP RB', 'DT NNP VBD', 'DT NNP VBZ', 'DT NNS CC', 'DT NNS DT', 'DT NNS IN', 'DT NNS NNP', 'DT NNS PRP', 'DT NNS RB', 'DT NNS TO', 'DT NNS VBD', 'DT NNS VBG', 'DT NNS VBP', 'DT RB JJ', 'DT RB VBN', 'DT VBG NN', 'DT VBN NN', 'EX VBZ DT', 'IN CD IN', 'IN CD JJ', 'IN CD NN', 'IN CD NNS', 'IN DT CD', 'IN DT IN', 'IN DT JJ', 'IN DT NN', 'IN DT NNP', 'IN DT NNS', 'IN DT RB', 'IN DT VBG', 'IN IN DT', 'IN IN NN', 'IN IN NNP', 'IN IN PRP', 'IN IN PRP$', 'IN JJ CC', 'IN JJ IN', 'IN JJ JJ', 'IN JJ NN', 'IN JJ NNP', 'IN JJ NNS', 'IN NN CC', 'IN NN DT', 'IN NN IN', 'IN NN NN', 'IN NN NNP', 'IN NN NNS', 'IN NN PRP', 'IN NN RB', 'IN NN TO', 'IN NN VBG', 'IN NN VBN', 'IN NN VBZ', 'IN NNP CC', 'IN NNP CD', 'IN NNP DT', 'IN NNP FW', 'IN NNP IN', 'IN NNP JJ', 'IN NNP MD', 'IN NNP NN', 'IN NNP NNP', 'IN NNP NNS', 'IN NNP PRP', 'IN NNP RB', 'IN NNP TO', 'IN NNP VBD', 'IN NNP VBG', 'IN NNP VBP', 'IN NNP VBZ', 'IN NNP WP', 'IN NNP WRB', 'IN NNS CC', 'IN NNS DT', 'IN NNS IN', 'IN NNS NNP', 'IN NNS VBP', 'IN PRP CC', 'IN PRP DT', 'IN PRP IN', 'IN PRP MD', 'IN PRP NNP', 'IN PRP PRP', 'IN PRP RB', 'IN PRP VBD', 'IN PRP VBP', 'IN PRP VBZ', 'IN PRP$ JJ', 'IN PRP$ NN', 'IN PRP$ NNP', 'IN PRP$ NNS', 'IN RB JJ', 'IN RB VBG', 'IN TO VB', 'IN VBG DT', 'IN VBG IN', 'IN VBG JJ', 'IN VBG NN', 'IN VBG NNP', 'IN VBG NNS', 'IN VBG PRP', 'IN VBG PRP$', 'IN VBG RB', 'IN VBG TO', 'IN VBG VBN', 'IN WDT PRP', 'JJ CC DT', 'JJ CC JJ', 'JJ CC RB', 'JJ CC VBZ', 'JJ DT NN', 'JJ IN DT', 'JJ IN JJ', 'JJ IN NN', 'JJ IN NNP', 'JJ IN PRP', 'JJ IN PRP$', 'JJ IN VBG', 'JJ JJ NN', 'JJ JJ NNP', 'JJ JJ NNS', 'JJ NN CC', 'JJ NN DT', 'JJ NN IN', 'JJ NN JJ', 'JJ NN NN', 'JJ NN NNP', 'JJ NN NNS', 'JJ NN PRP', 'JJ NN RB', 'JJ NN TO', 'JJ NN VBD', 'JJ NN VBG', 'JJ NN VBN', 'JJ NN VBZ', 'JJ NN WDT', 'JJ NN WP', 'JJ NN WRB', 'JJ NNP CC', 'JJ NNP IN', 'JJ NNP NN', 'JJ NNP NNP', 'JJ NNP NNS', 'JJ NNP VBZ', 'JJ NNS CC', 'JJ NNS DT', 'JJ NNS IN', 'JJ NNS JJ', 'JJ NNS NNP', 'JJ NNS PRP', 'JJ NNS RB', 'JJ NNS TO', 'JJ NNS VBG', 'JJ NNS VBN', 'JJ NNS VBP', 'JJ NNS WDT', 'JJ NNS WP', 'JJ PRP VBZ', 'JJ TO DT', 'JJ TO VB', 'MD RB VB', 'MD VB DT', 'MD VB IN', 'MD VB JJ', 'MD VB NN', 'MD VB NNP', 'MD VB PRP', 'MD VB PRP$', 'MD VB RB', 'MD VB TO', 'MD VB VBN', 'NN CC DT', 'NN CC IN', 'NN CC JJ', 'NN CC NN', 'NN CC NNP', 'NN CC NNS', 'NN CC PRP', 'NN CC PRP$', 'NN CC RB', 'NN CC VB', 'NN CC VBD', 'NN CC VBG', 'NN CC VBZ', 'NN CD NN', 'NN DT JJ', 'NN DT NN', 'NN DT NNP', 'NN DT NNS', 'NN IN CD', 'NN IN DT', 'NN IN IN', 'NN IN JJ', 'NN IN NN', 'NN IN NNP', 'NN IN NNS', 'NN IN PRP', 'NN IN PRP$', 'NN IN RB', 'NN IN VBG', 'NN IN WDT', 'NN JJ IN', 'NN JJ NN', 'NN JJ NNP', 'NN JJ NNS', 'NN JJ TO', 'NN MD VB', 'NN NN CC', 'NN NN DT', 'NN NN IN', 'NN NN JJ', 'NN NN NN', 'NN NN NNP', 'NN NN NNS', 'NN NN PRP', 'NN NN RB', 'NN NN TO', 'NN NN VBD', 'NN NN VBG', 'NN NN VBN', 'NN NN VBZ', 'NN NN WDT', 'NN NN WP', 'NN NN WRB', 'NN NNP CC', 'NN NNP DT', 'NN NNP IN', 'NN NNP NN', 'NN NNP NNP', 'NN NNP NNS', 'NN NNP PRP', 'NN NNP RB', 'NN NNP VBD', 'NN NNP VBZ', 'NN NNP WP', 'NN NNS CC', 'NN NNS DT', 'NN NNS IN', 'NN NNS NNP', 'NN NNS PRP', 'NN NNS RB', 'NN NNS TO', 'NN NNS VBP', 'NN PRP MD', 'NN PRP RB', 'NN PRP VBD', 'NN PRP VBP', 'NN PRP VBZ', 'NN PRP$ NN', 'NN RB DT', 'NN RB IN', 'NN RB JJ', 'NN RB NNP', 'NN RB PRP', 'NN RB RB', 'NN RB TO', 'NN RB VBG', 'NN RB VBZ', 'NN TO DT', 'NN TO NN', 'NN TO NNP', 'NN TO PRP$', 'NN TO VB', 'NN VBD DT', 'NN VBD IN', 'NN VBD JJ', 'NN VBD NN', 'NN VBD NNP', 'NN VBD RB', 'NN VBD TO', 'NN VBD VBN', 'NN VBG DT', 'NN VBG IN', 'NN VBG NN', 'NN VBG NNP', 'NN VBG PRP', 'NN VBG PRP$', 'NN VBG TO', 'NN VBN IN', 'NN VBN NNP', 'NN VBP VBN', 'NN VBZ DT', 'NN VBZ IN', 'NN VBZ JJ', 'NN VBZ NN', 'NN VBZ NNP', 'NN VBZ PRP', 'NN VBZ PRP$', 'NN VBZ RB', 'NN VBZ RP', 'NN VBZ TO', 'NN VBZ VBG', 'NN VBZ VBN', 'NN WDT MD', 'NN WDT NNP', 'NN WDT VBD', 'NN WDT VBZ', 'NN WP VBD', 'NN WP VBZ', 'NN WRB DT', 'NN WRB NNP', 'NN WRB PRP', 'NNP CC DT', 'NNP CC IN', 'NNP CC JJ', 'NNP CC NN', 'NNP CC NNP', 'NNP CC NNS', 'NNP CC PRP', 'NNP CC PRP$', 'NNP CC RB', 'NNP CC VB', 'NNP CC VBD', 'NNP CC VBZ', 'NNP DT JJ', 'NNP DT NN', 'NNP DT NNP', 'NNP DT NNS', 'NNP FW NN', 'NNP IN CD', 'NNP IN DT', 'NNP IN IN', 'NNP IN JJ', 'NNP IN NN', 'NNP IN NNP', 'NNP IN PRP', 'NNP IN PRP$', 'NNP IN VBG', 'NNP JJ NN', 'NNP JJ NNS', 'NNP MD VB', 'NNP NN CC', 'NNP NN DT', 'NNP NN IN', 'NNP NN JJ', 'NNP NN NN', 'NNP NN NNP', 'NNP NN NNS', 'NNP NN PRP', 'NNP NN RB', 'NNP NN TO', 'NNP NN VBD', 'NNP NN VBG', 'NNP NN VBZ', 'NNP NNP CC', 'NNP NNP CD', 'NNP NNP DT', 'NNP NNP IN', 'NNP NNP JJ', 'NNP NNP NN', 'NNP NNP NNP', 'NNP NNP NNS', 'NNP NNP PRP', 'NNP NNP RB', 'NNP NNP TO', 'NNP NNP VBD', 'NNP NNP VBG', 'NNP NNP VBP', 'NNP NNP VBZ', 'NNP NNP WP', 'NNP NNP WRB', 'NNP NNS CC', 'NNP NNS IN', 'NNP NNS NNP', 'NNP NNS TO', 'NNP NNS VBP', 'NNP PRP RB', 'NNP PRP VBD', 'NNP PRP VBP', 'NNP PRP VBZ', 'NNP PRP$ NN', 'NNP RB DT', 'NNP RB IN', 'NNP RB NNP', 'NNP RB RB', 'NNP RB VBD', 'NNP RB VBZ', 'NNP TO DT', 'NNP TO VB', 'NNP VBD DT', 'NNP VBD IN', 'NNP VBD JJ', 'NNP VBD NN', 'NNP VBD NNP', 'NNP VBD RB', 'NNP VBD TO', 'NNP VBD VBN', 'NNP VBG DT', 'NNP VBG IN', 'NNP VBN IN', 'NNP VBP DT', 'NNP VBP IN', 'NNP VBP JJ', 'NNP VBP NN', 'NNP VBP RB', 'NNP VBP TO', 'NNP VBP VBN', 'NNP VBZ DT', 'NNP VBZ IN', 'NNP VBZ JJ', 'NNP VBZ NN', 'NNP VBZ NNP', 'NNP VBZ PRP', 'NNP VBZ PRP$', 'NNP VBZ RB', 'NNP VBZ RP', 'NNP VBZ TO', 'NNP VBZ VBG', 'NNP VBZ VBN', 'NNP WDT VBZ', 'NNP WP VBD', 'NNP WP VBZ', 'NNP WRB NNP', 'NNP WRB PRP', 'NNS CC DT', 'NNS CC IN', 'NNS CC JJ', 'NNS CC NN', 'NNS CC NNP', 'NNS CC NNS', 'NNS CC PRP', 'NNS CC RB', 'NNS CC VB', 'NNS CC VBG', 'NNS CC VBZ', 'NNS DT JJ', 'NNS DT NN', 'NNS DT NNP', 'NNS DT NNS', 'NNS IN CD', 'NNS IN DT', 'NNS IN IN', 'NNS IN JJ', 'NNS IN NN', 'NNS IN NNP', 'NNS IN NNS', 'NNS IN PRP', 'NNS IN PRP$', 'NNS IN VBG', 'NNS JJ IN', 'NNS MD VB', 'NNS NN IN', 'NNS NNP CC', 'NNS NNP IN', 'NNS NNP NNP', 'NNS NNP RB', 'NNS NNP VBZ', 'NNS PRP VBD', 'NNS PRP VBP', 'NNS PRP VBZ', 'NNS PRP$ NN', 'NNS RB DT', 'NNS RB IN', 'NNS RB NNP', 'NNS RB RB', 'NNS RB VBP', 'NNS TO DT', 'NNS TO NNP', 'NNS TO VB', 'NNS VBG DT', 'NNS VBG IN', 'NNS VBN IN', 'NNS VBP DT', 'NNS VBP IN', 'NNS VBP JJ', 'NNS VBP NNP', 'NNS VBP RB', 'NNS VBP TO', 'NNS VBP VBG', 'NNS VBP VBN', 'NNS WDT VBP', 'NNS WP VBP', 'PDT DT NN', 'PRP CC NNP', 'PRP DT JJ', 'PRP DT NN', 'PRP IN DT', 'PRP IN NN', 'PRP IN NNP', 'PRP IN PRP', 'PRP IN PRP$', 'PRP IN VBG', 'PRP MD RB', 'PRP MD VB', 'PRP NNP VBZ', 'PRP PRP VBZ', 'PRP RB IN', 'PRP RB VBD', 'PRP RB VBP', 'PRP RB VBZ', 'PRP TO DT', 'PRP TO VB', 'PRP VBD DT', 'PRP VBD IN', 'PRP VBD JJ', 'PRP VBD NNP', 'PRP VBD PRP', 'PRP VBD RB', 'PRP VBD TO', 'PRP VBD VBN', 'PRP VBP DT', 'PRP VBP IN', 'PRP VBP JJ', 'PRP VBP NNP', 'PRP VBP PRP', 'PRP VBP RB', 'PRP VBP TO', 'PRP VBP VBN', 'PRP VBZ CC', 'PRP VBZ DT', 'PRP VBZ IN', 'PRP VBZ JJ', 'PRP VBZ NN', 'PRP VBZ NNP', 'PRP VBZ PRP', 'PRP VBZ PRP$', 'PRP VBZ RB', 'PRP VBZ RP', 'PRP VBZ TO', 'PRP VBZ VBG', 'PRP VBZ VBN', 'PRP$ JJ JJ', 'PRP$ JJ NN', 'PRP$ JJ NNP', 'PRP$ JJ NNS', 'PRP$ NN CC', 'PRP$ NN DT', 'PRP$ NN IN', 'PRP$ NN JJ', 'PRP$ NN NN', 'PRP$ NN NNP', 'PRP$ NN NNS', 'PRP$ NN PRP', 'PRP$ NN RB', 'PRP$ NN TO', 'PRP$ NN VBD', 'PRP$ NN VBG', 'PRP$ NN VBZ', 'PRP$ NNS CC', 'PRP$ NNS IN', 'PRP$ NNS NNP', 'PRP$ NNS RB', 'PRP$ NNS TO', 'PRP$ NNS VBP', 'PRP$ TO VB', 'RB CC RB', 'RB DT JJ', 'RB DT NN', 'RB DT NNP', 'RB IN DT', 'RB IN JJ', 'RB IN NN', 'RB IN NNP', 'RB IN PRP', 'RB IN PRP$', 'RB IN VBG', 'RB JJ CC', 'RB JJ IN', 'RB JJ NN', 'RB JJ NNP', 'RB JJ NNS', 'RB JJ TO', 'RB NN IN', 'RB NNP NNP', 'RB NNP VBZ', 'RB PRP VBP', 'RB PRP VBZ', 'RB PRP$ NN', 'RB RB IN', 'RB RB JJ', 'RB RB VBN', 'RB TO DT', 'RB TO NNP', 'RB TO VB', 'RB VB DT', 'RB VB IN', 'RB VB PRP', 'RB VBD DT', 'RB VBD IN', 'RB VBG DT', 'RB VBG IN', 'RB VBN CC', 'RB VBN DT', 'RB VBN IN', 'RB VBN NN', 'RB VBN NNP', 'RB VBN TO', 'RB VBZ DT', 'RB VBZ IN', 'RB VBZ NNP', 'RB VBZ PRP', 'RB VBZ PRP$', 'RB VBZ TO', 'RP DT NN', 'RP IN DT', 'RP IN NNP', 'RP TO VB', 'TO DT JJ', 'TO DT NN', 'TO DT NNP', 'TO DT NNS', 'TO NN IN', 'TO NNP CC', 'TO NNP IN', 'TO NNP NN', 'TO NNP NNP', 'TO PRP$ JJ', 'TO PRP$ NN', 'TO VB CC', 'TO VB DT', 'TO VB IN', 'TO VB JJ', 'TO VB NN', 'TO VB NNP', 'TO VB NNS', 'TO VB PRP', 'TO VB PRP$', 'TO VB RB', 'TO VB RP', 'TO VB TO', 'TO VB VBG', 'TO VB VBN', 'VB CC VB', 'VB DT JJ', 'VB DT NN', 'VB DT NNP', 'VB DT NNS', 'VB IN DT', 'VB IN NN', 'VB IN NNP', 'VB IN PRP', 'VB IN PRP$', 'VB JJ IN', 'VB JJ NN', 'VB JJ NNS', 'VB NN CC', 'VB NN IN', 'VB NNP CC', 'VB NNP IN', 'VB NNP JJ', 'VB NNP NNP', 'VB NNP VBZ', 'VB NNS IN', 'VB PRP CC', 'VB PRP DT', 'VB PRP IN', 'VB PRP RB', 'VB PRP TO', 'VB PRP VBZ', 'VB PRP$ JJ', 'VB PRP$ NN', 'VB PRP$ NNS', 'VB RB IN', 'VB RP DT', 'VB RP IN', 'VB TO DT', 'VB TO NNP', 'VB TO VB', 'VB VBN IN', 'VB VBN TO', 'VBD DT JJ', 'VBD DT NN', 'VBD DT NNP', 'VBD IN DT', 'VBD IN NN', 'VBD IN NNP', 'VBD IN PRP$', 'VBD JJ NN', 'VBD JJ NNS', 'VBD NN IN', 'VBD NNP NNP', 'VBD PRP IN', 'VBD PRP$ NN', 'VBD RB IN', 'VBD RB JJ', 'VBD RB VBN', 'VBD TO VB', 'VBD VBN DT', 'VBD VBN IN', 'VBD VBN TO', 'VBG DT JJ', 'VBG DT NN', 'VBG DT NNP', 'VBG DT NNS', 'VBG IN DT', 'VBG IN NN', 'VBG IN NNP', 'VBG IN PRP', 'VBG IN PRP$', 'VBG JJ NN', 'VBG JJ NNS', 'VBG NN CC', 'VBG NN IN', 'VBG NNP IN', 'VBG NNP NN', 'VBG NNP NNP', 'VBG NNS IN', 'VBG PRP IN', 'VBG PRP TO', 'VBG PRP$ JJ', 'VBG PRP$ NN', 'VBG PRP$ NNS', 'VBG RB IN', 'VBG TO DT', 'VBG TO NNP', 'VBG TO VB', 'VBG VBN IN', 'VBN CC VBN', 'VBN DT JJ', 'VBN DT NN', 'VBN DT NNP', 'VBN IN CD', 'VBN IN DT', 'VBN IN IN', 'VBN IN JJ', 'VBN IN NN', 'VBN IN NNP', 'VBN IN NNS', 'VBN IN PRP', 'VBN IN PRP$', 'VBN IN VBG', 'VBN NN IN', 'VBN NNP NNP', 'VBN NNP VBZ', 'VBN PRP$ NN', 'VBN RB IN', 'VBN RP IN', 'VBN TO DT', 'VBN TO NNP', 'VBN TO VB', 'VBN VBN IN', 'VBP DT JJ', 'VBP DT NN', 'VBP DT NNP', 'VBP DT NNS', 'VBP IN DT', 'VBP IN NN', 'VBP IN NNP', 'VBP IN PRP', 'VBP JJ IN', 'VBP JJ NN', 'VBP JJ NNS', 'VBP NN IN', 'VBP NNP NNP', 'VBP PRP$ NN', 'VBP RB IN', 'VBP RB JJ', 'VBP RB VBN', 'VBP TO VB', 'VBP VBN IN', 'VBP VBN TO', 'VBZ DT JJ', 'VBZ DT NN', 'VBZ DT NNP', 'VBZ DT NNS', 'VBZ IN DT', 'VBZ IN IN', 'VBZ IN JJ', 'VBZ IN NN', 'VBZ IN NNP', 'VBZ IN PRP', 'VBZ IN PRP$', 'VBZ IN TO', 'VBZ IN VBG', 'VBZ JJ CC', 'VBZ JJ IN', 'VBZ JJ NN', 'VBZ JJ NNP', 'VBZ JJ NNS', 'VBZ JJ TO', 'VBZ NN IN', 'VBZ NN TO', 'VBZ NNP CC', 'VBZ NNP DT', 'VBZ NNP IN', 'VBZ NNP NN', 'VBZ NNP NNP', 'VBZ NNP TO', 'VBZ NNS IN', 'VBZ PRP DT', 'VBZ PRP IN', 'VBZ PRP RB', 'VBZ PRP TO', 'VBZ PRP VBZ', 'VBZ PRP$ IN', 'VBZ PRP$ JJ', 'VBZ PRP$ NN', 'VBZ PRP$ NNS', 'VBZ RB DT', 'VBZ RB IN', 'VBZ RB JJ', 'VBZ RB RB', 'VBZ RB TO', 'VBZ RB VB', 'VBZ RB VBG', 'VBZ RB VBN', 'VBZ RP DT', 'VBZ RP IN', 'VBZ RP TO', 'VBZ TO DT', 'VBZ TO NNP', 'VBZ TO VB', 'VBZ VBG DT', 'VBZ VBG IN', 'VBZ VBG TO', 'VBZ VBN CC', 'VBZ VBN DT', 'VBZ VBN IN', 'VBZ VBN JJ', 'VBZ VBN NNP', 'VBZ VBN PRP', 'VBZ VBN PRP$', 'VBZ VBN RB', 'VBZ VBN RP', 'VBZ VBN TO', 'VBZ VBN VBG', 'VBZ VBN VBN', 'WDT MD VB', 'WDT NNP VBZ', 'WDT PRP VBZ', 'WDT VBZ DT', 'WDT VBZ VBN', 'WP PRP VBZ', 'WP VBD VBN', 'WP VBZ DT', 'WP VBZ IN', 'WP VBZ PRP', 'WP VBZ RB', 'WP VBZ TO', 'WP VBZ VBN', 'WRB DT NN', 'WRB NNP VBZ', 'WRB PRP VBD', 'WRB PRP VBP', 'WRB PRP VBZ']\n"
     ]
    }
   ],
   "source": [
    "for key in selected_features.keys():\n",
    "    print(key,selected_features[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export path calculated features\n",
    "PATH_CALC_FEATURES = './Features/test_calc_features{0}.txt'.format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "extracted_features = FEATURE_SELECTOR_v4.select_features(\n",
    "    # Contents\n",
    "    serie_texts=serie_texts,                          \n",
    "    # Dictionary with the features which should be calculated\n",
    "    features_to_calc=features_to_calc, \n",
    "    # Dictionary with the parameters for tokenizing the contents\n",
    "    token_params_1=token_params_1,              \n",
    "    # Dictionary with the parameters for filtering the n_grams\n",
    "    filter_params=filter_params,\n",
    "    # Threshold for tokens\n",
    "    cv_min_df = 0,                           \n",
    "    # Flag if the features should be selected or extracted\n",
    "    flag_extract_features=True,\n",
    "    # Path to already selected features       \n",
    "    path_to_features=path_selected_features,    \n",
    "    # TFIDF / Text_length\n",
    "    normalization_type='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extracted_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-69ef82a42345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save the calculated features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mextracted_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPATH_CALC_FEATURES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'extracted_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the calculated features\n",
    "extracted_features.to_csv(path_or_buf=PATH_CALC_FEATURES, sep=',', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features=pd.read_csv(filepath_or_buffer='./Features/test_calc_features2018_12_25_23_09.txt', sep=',', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW: we shouldn't get NaN values in the first place\n",
    "# Fill NaN values in features\n",
    "extracted_features.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(extracted_features, df_texts.label, test_size=0.33, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(extracted_features, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_clf 0.5871534449629426\n",
      "MultinomialNB 0.643425748009882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 0.47845182541861103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.7106780126269558\n",
      "GradientBoostingClassifier 0.7878122426571507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier 0.6900905846829536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model0= SVC(gamma='auto')\n",
    "model0.fit(X_train, y_train)\n",
    "print('svm_clf',model0.score(X_test, y_test))\n",
    "\n",
    "model1 = MultinomialNB(alpha=0.001)\n",
    "model1.fit( X_train, y_train )\n",
    "print('MultinomialNB',model1.score(X_test, y_test))\n",
    "\n",
    "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2.fit( X_train, y_train )\n",
    "print('SGDClassifier',model2.score(X_test, y_test))\n",
    "\n",
    "model3 = RandomForestClassifier()\n",
    "model3.fit( X_train, y_train )\n",
    "print('RandomForestClassifier',model3.score(X_test, y_test))\n",
    "\n",
    "model4 = GradientBoostingClassifier()\n",
    "model4.fit( X_train, y_train )\n",
    "print('GradientBoostingClassifier',model4.score(X_test, y_test))\n",
    "\n",
    "model5=MLPClassifier(hidden_layer_sizes=(200,))\n",
    "model5.fit( X_train, y_train )\n",
    "print('MLPClassifier',model5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10628</th>\n",
       "      <th>10629</th>\n",
       "      <th>10630</th>\n",
       "      <th>10631</th>\n",
       "      <th>10632</th>\n",
       "      <th>10633</th>\n",
       "      <th>10634</th>\n",
       "      <th>10635</th>\n",
       "      <th>10636</th>\n",
       "      <th>10637</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "      <td>1.103900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.076206e-16</td>\n",
       "      <td>2.574663e-16</td>\n",
       "      <td>-2.368690e-16</td>\n",
       "      <td>-5.767246e-16</td>\n",
       "      <td>-2.111224e-16</td>\n",
       "      <td>1.609165e-19</td>\n",
       "      <td>8.882589e-17</td>\n",
       "      <td>5.149327e-18</td>\n",
       "      <td>4.119461e-17</td>\n",
       "      <td>-3.572345e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.930998e-17</td>\n",
       "      <td>-1.705715e-17</td>\n",
       "      <td>-7.209058e-17</td>\n",
       "      <td>-4.344745e-17</td>\n",
       "      <td>6.436659e-19</td>\n",
       "      <td>5.278060e-17</td>\n",
       "      <td>-1.158599e-17</td>\n",
       "      <td>4.376928e-17</td>\n",
       "      <td>1.802264e-17</td>\n",
       "      <td>5.149327e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000045e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.920921e+00</td>\n",
       "      <td>-5.967601e+00</td>\n",
       "      <td>-3.348115e+00</td>\n",
       "      <td>-2.646035e+00</td>\n",
       "      <td>-2.035737e+00</td>\n",
       "      <td>-2.227685e-01</td>\n",
       "      <td>-3.032335e-01</td>\n",
       "      <td>-5.100637e-01</td>\n",
       "      <td>-2.794832e-01</td>\n",
       "      <td>-1.955677e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.312851e-01</td>\n",
       "      <td>-2.344928e-01</td>\n",
       "      <td>-2.471872e-01</td>\n",
       "      <td>-2.096703e-01</td>\n",
       "      <td>-3.080339e-01</td>\n",
       "      <td>-2.955312e-01</td>\n",
       "      <td>-3.993123e-01</td>\n",
       "      <td>-3.121379e-01</td>\n",
       "      <td>-2.867378e-01</td>\n",
       "      <td>-4.192929e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.679949e-01</td>\n",
       "      <td>-5.665708e-01</td>\n",
       "      <td>-5.667396e-01</td>\n",
       "      <td>-7.546716e-01</td>\n",
       "      <td>-5.833672e-01</td>\n",
       "      <td>-2.227685e-01</td>\n",
       "      <td>-3.032335e-01</td>\n",
       "      <td>-5.100637e-01</td>\n",
       "      <td>-2.794832e-01</td>\n",
       "      <td>-1.955677e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.312851e-01</td>\n",
       "      <td>-2.344928e-01</td>\n",
       "      <td>-2.471872e-01</td>\n",
       "      <td>-2.096703e-01</td>\n",
       "      <td>-3.080339e-01</td>\n",
       "      <td>-2.955312e-01</td>\n",
       "      <td>-3.993123e-01</td>\n",
       "      <td>-3.121379e-01</td>\n",
       "      <td>-2.867378e-01</td>\n",
       "      <td>-4.192929e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.434806e-01</td>\n",
       "      <td>-7.645394e-02</td>\n",
       "      <td>-1.409876e-01</td>\n",
       "      <td>-1.223148e-01</td>\n",
       "      <td>-1.385598e-01</td>\n",
       "      <td>-2.227685e-01</td>\n",
       "      <td>-3.032335e-01</td>\n",
       "      <td>-5.100637e-01</td>\n",
       "      <td>-2.794832e-01</td>\n",
       "      <td>-1.955677e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.312851e-01</td>\n",
       "      <td>-2.344928e-01</td>\n",
       "      <td>-2.471872e-01</td>\n",
       "      <td>-2.096703e-01</td>\n",
       "      <td>-3.080339e-01</td>\n",
       "      <td>-2.955312e-01</td>\n",
       "      <td>-3.993123e-01</td>\n",
       "      <td>-3.121379e-01</td>\n",
       "      <td>-2.867378e-01</td>\n",
       "      <td>-4.192929e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.840530e-01</td>\n",
       "      <td>4.650777e-01</td>\n",
       "      <td>3.865312e-01</td>\n",
       "      <td>6.456352e-01</td>\n",
       "      <td>3.973758e-01</td>\n",
       "      <td>-2.227685e-01</td>\n",
       "      <td>-3.032335e-01</td>\n",
       "      <td>1.978228e-01</td>\n",
       "      <td>-2.794832e-01</td>\n",
       "      <td>-1.955677e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.312851e-01</td>\n",
       "      <td>-2.344928e-01</td>\n",
       "      <td>-2.471872e-01</td>\n",
       "      <td>-2.096703e-01</td>\n",
       "      <td>-3.080339e-01</td>\n",
       "      <td>-2.955312e-01</td>\n",
       "      <td>-2.201768e-02</td>\n",
       "      <td>-3.121379e-01</td>\n",
       "      <td>-2.867378e-01</td>\n",
       "      <td>1.468682e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.976557e+01</td>\n",
       "      <td>2.732082e+01</td>\n",
       "      <td>2.652905e+01</td>\n",
       "      <td>3.410068e+00</td>\n",
       "      <td>1.300829e+01</td>\n",
       "      <td>2.282587e+01</td>\n",
       "      <td>2.094566e+01</td>\n",
       "      <td>2.293868e+01</td>\n",
       "      <td>3.232037e+01</td>\n",
       "      <td>5.374518e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.299104e+01</td>\n",
       "      <td>2.387181e+01</td>\n",
       "      <td>4.020338e+01</td>\n",
       "      <td>3.181282e+01</td>\n",
       "      <td>3.435760e+01</td>\n",
       "      <td>3.152126e+01</td>\n",
       "      <td>2.165790e+01</td>\n",
       "      <td>2.340699e+01</td>\n",
       "      <td>2.861880e+01</td>\n",
       "      <td>4.389225e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 10638 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4      \\\n",
       "count  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04   \n",
       "mean  -6.076206e-16  2.574663e-16 -2.368690e-16 -5.767246e-16 -2.111224e-16   \n",
       "std    1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00   \n",
       "min   -2.920921e+00 -5.967601e+00 -3.348115e+00 -2.646035e+00 -2.035737e+00   \n",
       "25%   -5.679949e-01 -5.665708e-01 -5.667396e-01 -7.546716e-01 -5.833672e-01   \n",
       "50%   -1.434806e-01 -7.645394e-02 -1.409876e-01 -1.223148e-01 -1.385598e-01   \n",
       "75%    3.840530e-01  4.650777e-01  3.865312e-01  6.456352e-01  3.973758e-01   \n",
       "max    2.976557e+01  2.732082e+01  2.652905e+01  3.410068e+00  1.300829e+01   \n",
       "\n",
       "              5             6             7             8             9      \\\n",
       "count  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04   \n",
       "mean   1.609165e-19  8.882589e-17  5.149327e-18  4.119461e-17 -3.572345e-17   \n",
       "std    1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00   \n",
       "min   -2.227685e-01 -3.032335e-01 -5.100637e-01 -2.794832e-01 -1.955677e-01   \n",
       "25%   -2.227685e-01 -3.032335e-01 -5.100637e-01 -2.794832e-01 -1.955677e-01   \n",
       "50%   -2.227685e-01 -3.032335e-01 -5.100637e-01 -2.794832e-01 -1.955677e-01   \n",
       "75%   -2.227685e-01 -3.032335e-01  1.978228e-01 -2.794832e-01 -1.955677e-01   \n",
       "max    2.282587e+01  2.094566e+01  2.293868e+01  3.232037e+01  5.374518e+01   \n",
       "\n",
       "           ...              10628         10629         10630         10631  \\\n",
       "count      ...       1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04   \n",
       "mean       ...      -1.930998e-17 -1.705715e-17 -7.209058e-17 -4.344745e-17   \n",
       "std        ...       1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00   \n",
       "min        ...      -2.312851e-01 -2.344928e-01 -2.471872e-01 -2.096703e-01   \n",
       "25%        ...      -2.312851e-01 -2.344928e-01 -2.471872e-01 -2.096703e-01   \n",
       "50%        ...      -2.312851e-01 -2.344928e-01 -2.471872e-01 -2.096703e-01   \n",
       "75%        ...      -2.312851e-01 -2.344928e-01 -2.471872e-01 -2.096703e-01   \n",
       "max        ...       4.299104e+01  2.387181e+01  4.020338e+01  3.181282e+01   \n",
       "\n",
       "              10632         10633         10634         10635         10636  \\\n",
       "count  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04  1.103900e+04   \n",
       "mean   6.436659e-19  5.278060e-17 -1.158599e-17  4.376928e-17  1.802264e-17   \n",
       "std    1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00  1.000045e+00   \n",
       "min   -3.080339e-01 -2.955312e-01 -3.993123e-01 -3.121379e-01 -2.867378e-01   \n",
       "25%   -3.080339e-01 -2.955312e-01 -3.993123e-01 -3.121379e-01 -2.867378e-01   \n",
       "50%   -3.080339e-01 -2.955312e-01 -3.993123e-01 -3.121379e-01 -2.867378e-01   \n",
       "75%   -3.080339e-01 -2.955312e-01 -2.201768e-02 -3.121379e-01 -2.867378e-01   \n",
       "max    3.435760e+01  3.152126e+01  2.165790e+01  2.340699e+01  2.861880e+01   \n",
       "\n",
       "              10637  \n",
       "count  1.103900e+04  \n",
       "mean   5.149327e-18  \n",
       "std    1.000045e+00  \n",
       "min   -4.192929e-01  \n",
       "25%   -4.192929e-01  \n",
       "50%   -4.192929e-01  \n",
       "75%    1.468682e-01  \n",
       "max    4.389225e+01  \n",
       "\n",
       "[8 rows x 10638 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "extracted_features_scaled=pd.DataFrame(scaler.fit_transform(extracted_features))\n",
    "extracted_features_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=10, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
    "extracted_features_pca=pca.fit_transform(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(extracted_features_pca, df_texts.label, test_size=0.33, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(extracted_features, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_clf 0.6272303046939336\n",
      "SGDClassifier 0.5237441668954159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.6291517979687071\n",
      "GradientBoostingClassifier 0.6722481471314851\n",
      "MLPClassifier 0.6560527038155366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model0= SVC(gamma='auto')\n",
    "model0.fit(X_train, y_train)\n",
    "print('svm_clf',model0.score(X_test, y_test))\n",
    "\n",
    "#model1 = MultinomialNB(alpha=0.001)\n",
    "#model1.fit( X_train, y_train )\n",
    "#print('MultinomialNB',model1.score(X_test, y_test))\n",
    "\n",
    "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2.fit( X_train, y_train )\n",
    "print('SGDClassifier',model2.score(X_test, y_test))\n",
    "\n",
    "model3 = RandomForestClassifier()\n",
    "model3.fit( X_train, y_train )\n",
    "print('RandomForestClassifier',model3.score(X_test, y_test))\n",
    "\n",
    "model4 = GradientBoostingClassifier()\n",
    "model4.fit( X_train, y_train )\n",
    "print('GradientBoostingClassifier',model4.score(X_test, y_test))\n",
    "\n",
    "model5=MLPClassifier(hidden_layer_sizes=(200,))\n",
    "model5.fit( X_train, y_train )\n",
    "print('MLPClassifier',model5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with the first 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(extracted_features.iloc[:,:4], df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC(gamma='auto')\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6384847653033214\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB 0.5940159209442767\n",
      "SGDClassifier 0.47817732637935767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Marius\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.6148778479275323\n",
      "GradientBoostingClassifier 0.674444139445512\n",
      "MLPClassifier 0.6648366730716443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model1 = MultinomialNB(alpha=0.001)\n",
    "model1.fit( X_train, y_train )\n",
    "print('MultinomialNB',model1.score(X_test, y_test))\n",
    "\n",
    "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2.fit( X_train, y_train )\n",
    "print('SGDClassifier',model2.score(X_test, y_test))\n",
    "\n",
    "model3 = RandomForestClassifier()\n",
    "model3.fit( X_train, y_train )\n",
    "print('RandomForestClassifier',model3.score(X_test, y_test))\n",
    "\n",
    "model4 = GradientBoostingClassifier()\n",
    "model4.fit( X_train, y_train )\n",
    "print('GradientBoostingClassifier',model4.score(X_test, y_test))\n",
    "\n",
    "model5=MLPClassifier(hidden_layer_sizes=(100,))\n",
    "model5.fit( X_train, y_train )\n",
    "print('MLPClassifier',model5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences(s):\n",
    "    words = s.str.split()\n",
    "    words = pd.DataFrame(words.apply(np.array_split, indices_or_sections=4).tolist())\n",
    "    return words.applymap(lambda s: \" \".join(s) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with sequences and all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seqs = sequences(serie_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_seq=pd.DataFrame()\n",
    "for index, col in enumerate(text_seqs.columns):\n",
    "    print(index)\n",
    "    df_features_seq=pd.concat((df_features_seq,FEATURE_SELECTOR_v4.select_features(serie_texts=text_seqs.iloc[:,index],features_to_calc=features_to_calc,token_params_1=token_params_1,filter_params=filter_params,path_to_features='./Features/test_features2018_12_25_21_31.txt',flag_extract_features=True,cv_min_df=0,normalization_type='')),axis=1,ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_features_seq.info())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW: we shouldn't get NaN values in the first place\n",
    "# Fill NaN values in features\n",
    "df_features_seq.fillna(0, inplace=True)\n",
    "df_features_seq.replace([np.inf, -np.inf], np.float64(0),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.any(np.isinf(df_features_seq.values), axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_seq.loc[np.any(np.isinf(df_features_seq.values), axis=0),np.any(np.isinf(df_features_seq.values), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(df_features_seq, df_texts.label, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_seq = SVC(gamma='auto')\n",
    "svm_clf_seq.fit(X_train_seq, y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf_seq.score(X_test_seq, y_test_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model1_seq = MultinomialNB(alpha=0.001)\n",
    "model1_seq.fit( X_train_seq, y_train_seq )\n",
    "print('MultinomialNB',model1_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model2_seq = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2_seq.fit( X_train_seq, y_train_seq )\n",
    "print('SGDClassifier',model2_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model3_seq = RandomForestClassifier()\n",
    "model3_seq.fit( X_train_seq, y_train_seq )\n",
    "print('RandomForestClassifier',model3_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model4_seq = GradientBoostingClassifier()\n",
    "model4_seq.fit( X_train_seq, y_train_seq )\n",
    "print('GradientBoostingClassifier',model4_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model5_seq=MLPClassifier(hidden_layer_sizes=(100,))\n",
    "model5_seq.fit( X_train_seq, y_train_seq )\n",
    "print('MLPClassifier',model5_seq.score(X_test_seq, y_test_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with sequences and 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b2f7525e54e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_features_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_seqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf_features_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFEATURE_SELECTOR_v4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserie_texts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_seqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures_to_calc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_to_calc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken_params_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_params_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilter_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath_to_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_selected_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflag_extract_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_min_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalization_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Marius\\Documents\\Studium\\3_Semester\\Seminar Text Mining\\Repository\\AATM\\FEATURE_SELECTOR_v4.py\u001b[0m in \u001b[0;36mselect_features\u001b[1;34m(serie_texts, features_to_calc, token_params_1, filter_params, path_to_features, flag_extract_features, cv_min_df, normalization_type)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;31m# Add frequencies to feature dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mdf_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_frequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflag_extract_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[0mndims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(self, inplace)\u001b[0m\n\u001b[0;32m   4455\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inplace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4456\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4457\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4458\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4459\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4437\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4439\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4441\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   4426\u001b[0m         \"\"\"\n\u001b[0;32m   4427\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4428\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4429\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   4435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4436\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4096\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4097\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4098\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4103\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4104\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   5067\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5068\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 5069\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   5070\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5071\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   5090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5091\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5092\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5093\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_features_seq=pd.DataFrame()\n",
    "for index, col in enumerate(text_seqs.columns):\n",
    "    df_features_seq=pd.concat((df_features_seq,FEATURE_SELECTOR_v4.select_features(serie_texts=text_seqs.iloc[:,index],features_to_calc=features_to_calc,token_params_1=token_params_1,filter_params=filter_params,path_to_features=path_selected_features,flag_extract_features=True,cv_min_df=0,normalization_type=''))[0:4],axis=1,ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_features_seq.info())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW: we shouldn't get NaN values in the first place\n",
    "# Fill NaN values in features\n",
    "df_features_seq.fillna(0, inplace=True)\n",
    "df_features_seq.replace([np.inf, -np.inf], np.float64(0),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.any(np.isinf(df_features_seq.values), axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_seq.loc[np.any(np.isinf(df_features_seq.values), axis=0),np.any(np.isinf(df_features_seq.values), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(df_features_seq, df_texts.label, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_seq = SVC(gamma='auto')\n",
    "svm_clf_seq.fit(X_train_seq, y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf_seq.score(X_test_seq, y_test_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model1_seq = MultinomialNB(alpha=0.001)\n",
    "model1_seq.fit( X_train_seq, y_train_seq )\n",
    "print('MultinomialNB',model1_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model2_seq = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2_seq.fit( X_train_seq, y_train_seq )\n",
    "print('SGDClassifier',model2_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model3_seq = RandomForestClassifier()\n",
    "model3_seq.fit( X_train_seq, y_train_seq )\n",
    "print('RandomForestClassifier',model3_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model4_seq = GradientBoostingClassifier()\n",
    "model4_seq.fit( X_train_seq, y_train_seq )\n",
    "print('GradientBoostingClassifier',model4_seq.score(X_test_seq, y_test_seq))\n",
    "\n",
    "model5_seq=MLPClassifier(hidden_layer_sizes=(100,))\n",
    "model5_seq.fit( X_train_seq, y_train_seq )\n",
    "print('MLPClassifier',model5_seq.score(X_test_seq, y_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras simple dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "model = Sequential()\n",
    " \n",
    "model.add(Dense(units=500, activation='relu', input_dim=len(df_features_seq.columns))\n",
    "model.add(Dense(units=1, activation='softmax'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_onehot[:-100], y_train[:-100], \n",
    "          epochs=2, batch_size=128, verbose=1, \n",
    "          validation_data=(X_train_onehot[-500:], y_train[-500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-0431c1bbe4df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_seqs_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserie_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# number of sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "text_seqs_lstm = sequences(serie_texts)\n",
    "# number of sequences\n",
    "num_seq=4\n",
    "# number of features\n",
    "num_features=4000\n",
    "\n",
    "#np_features_seq_lstm=np.array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_seq_lstm=text_seqs_lstm.apply(lambda row: FEATURE_SELECTOR_v4.select_features(serie_texts=row,features_to_calc=features_to_calc,token_params_1=token_params_1,filter_params=filter_params,path_to_features=path_selected_features,flag_extract_features=True,cv_min_df=0,normalization_type=''))\n",
    "features_seq_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(100, input_shape=(num_seq, num_features)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(1, num_seq, num_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_params_keras = set_token_params_1 (get_tokens='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts=df_texts.text.apply(lambda row: \" \".join(FEATURE_SELECTOR_v4.tokenize(text=row, **token_params_keras)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        money tell the stori of and is narrat by john ...\n",
       "1        the open sentenc say the pushcart war start on...\n",
       "2        dug a mundan is transport in to the magic land...\n",
       "3        the term gild age commonli given to the era co...\n",
       "4        the resid of tiamat are split into two clan wi...\n",
       "5        india opal buloni is a 10 year old girl who ha...\n",
       "6          to be a jedi is to safeguard peac in the galaxi\n",
       "7        crippl by a freak accid enterpris ha cross ove...\n",
       "8        it is the stori of a young woman name much afr...\n",
       "9        from the hardcov jacket it look as though aust...\n",
       "10       the plot of permut citi follow the live of sev...\n",
       "11       the stori narrat by the gigant but docil half ...\n",
       "12       use altern first person perspect the novel tel...\n",
       "13       the stori focus on nicol gunther perrin a youn...\n",
       "14       the first section describ the choos of talia b...\n",
       "15       versori a literari lectur engag in spread engl...\n",
       "16       astrid magnussen is a twelv year old girl live...\n",
       "17       in 1377 england a 13 year old boy known onli a...\n",
       "18       after have a dark side focus nightmar anakin b...\n",
       "19       the polypontian empir ha conquer much of the k...\n",
       "20       sir gareth is a note corinthian and ha been a ...\n",
       "21       in salem villag massachusett 1692 the doctor a...\n",
       "22       when the galaxi s most skill hunter is ask to ...\n",
       "23       set in the year 2031 heavi weather depict a wo...\n",
       "24       the book s first three movement consist of the...\n",
       "25       event are list here not in chronolog order but...\n",
       "26       stolen as a child and rais in the wood of evil...\n",
       "27       the book open with captain jack aubrey and hi ...\n",
       "28       the golden globe and steel beach take place in...\n",
       "29       extinct take the form of the autobiograph test...\n",
       "                               ...                        \n",
       "11009    the demigod diari contain four new stori with ...\n",
       "11010    the novel begin as sukartono tono a dutch trai...\n",
       "11011    inspector han bärlach at the end of hi career ...\n",
       "11012    who kill zebede open with a direct address to ...\n",
       "11013    the stori begin in the year 1463 dr with a mee...\n",
       "11014    the novella tell the stori of hauk haien alleg...\n",
       "11015    the eponym paradox of democraci that thi colle...\n",
       "11016     the book depict a schoolboy who gain magic power\n",
       "11017    the gzilt a civilis that almost join the cultu...\n",
       "11018    fifti shade of grey follow anastasia ana steel...\n",
       "11019    the novel is split into seven part the first d...\n",
       "11020    the book follow sever charact as they deal wit...\n",
       "11021    the novel is set in the countri side around la...\n",
       "11022    the book is about a feder judg s murder and an...\n",
       "11023    bring up the bodi begin where the previou nove...\n",
       "11024    the book follow nora and patch a teenag girl a...\n",
       "11025    the novel is a frame narr a writer name gerard...\n",
       "11026    the famili focus on three brother from the gao...\n",
       "11027    heaven leigh casteel is a fourteen year old gi...\n",
       "11028    after the event of heaven the first book in th...\n",
       "11029    a novel about anni stonewal the daughter of he...\n",
       "11030    after mickey bolitar move in with hi uncl myro...\n",
       "11031    stephani harrington and her treecat climb quic...\n",
       "11032    the birth of plenti is an histori of the world...\n",
       "11033    set dure the summer of 2004 the novel main plo...\n",
       "11034    class wikit season cover book titl 1 8 the sim...\n",
       "11035    prue mckeel have rescu her brother from the do...\n",
       "11036    the reader first meet rapp while he is do a co...\n",
       "11037    colbert address topic includ wall street campa...\n",
       "11038    makar devushkin and varvara dobroselova are se...\n",
       "Name: text, Length: 11039, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000\n",
    "tokenizer_stem = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer_stem.fit_on_texts(stemmed_texts)\n",
    "sequences_stem = tokenizer_stem.texts_to_sequences(stemmed_texts)\n",
    "data_stem = pad_sequences(sequences_stem, maxlen=1200, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 500   73    1 ...    0    0    0]\n",
      " [   1   64 1232 ...    0    0    0]\n",
      " [5875    3 2824 ...    0    0    0]\n",
      " ...\n",
      " [   1  522   42 ...    0    0    0]\n",
      " [1647 1992  230 ...    0    0    0]\n",
      " [   5 7154   30 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(data_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_stem, X_test_lstm_stem, y_train_lstm_stem, y_test_lstm_stem = train_test_split(data_stem, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=1200))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4437 samples, validate on 2959 samples\n",
      "Epoch 1/5\n",
      "4437/4437 [==============================] - 313s 71ms/step - loss: 0.6943 - acc: 0.4897 - val_loss: 0.6942 - val_acc: 0.4873\n",
      "Epoch 2/5\n",
      "4437/4437 [==============================] - 305s 69ms/step - loss: 0.6936 - acc: 0.4895 - val_loss: 0.6941 - val_acc: 0.4873\n",
      "Epoch 3/5\n",
      "4437/4437 [==============================] - 304s 69ms/step - loss: 0.6935 - acc: 0.4904 - val_loss: 0.6937 - val_acc: 0.4873\n",
      "Epoch 4/5\n",
      "4437/4437 [==============================] - 305s 69ms/step - loss: 0.6937 - acc: 0.4979 - val_loss: 0.6929 - val_acc: 0.5127\n",
      "Epoch 5/5\n",
      "4437/4437 [==============================] - 303s 68ms/step - loss: 0.6934 - acc: 0.5015 - val_loss: 0.6928 - val_acc: 0.5127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x250fc9f9eb8>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_lstm_stem, np.array(y_train_lstm_stem), validation_split=0.4, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df_texts.text)\n",
    "sequences = tokenizer.texts_to_sequences(df_texts.text)\n",
    "data = pad_sequences(sequences, maxlen=1000, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_orig, X_test_lstm_orig, y_train_lstm_orig, y_test_lstm_orig = train_test_split(data, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "model_orig = Sequential()\n",
    "model_orig.add(Embedding(20000, 200, input_length=1000))\n",
    "model_orig.add(LSTM(200, dropout=0.5, recurrent_dropout=0.5))\n",
    "model_orig.add(Dense(1, activation='sigmoid'))\n",
    "model_orig.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5177 samples, validate on 2219 samples\n",
      "Epoch 1/10\n",
      "5177/5177 [==============================] - 951s 184ms/step - loss: 0.6752 - acc: 0.5774 - val_loss: 0.6824 - val_acc: 0.4899\n",
      "Epoch 2/10\n",
      "5177/5177 [==============================] - 981s 189ms/step - loss: 0.6605 - acc: 0.5926 - val_loss: 0.6615 - val_acc: 0.6048\n",
      "Epoch 3/10\n",
      "1856/5177 [=========>....................] - ETA: 9:39 - loss: 0.6360 - acc: 0.6056"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-b6979f3aea81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_lstm_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_lstm_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_orig.fit(X_train_lstm_orig, np.array(y_train_lstm_orig), validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4437 samples, validate on 2959 samples\n",
      "Epoch 1/40\n",
      "4437/4437 [==============================] - 311s 70ms/step - loss: 0.4153 - acc: 0.7951 - val_loss: 0.5990 - val_acc: 0.7665\n",
      "Epoch 2/40\n",
      "4437/4437 [==============================] - 304s 69ms/step - loss: 0.3529 - acc: 0.8332 - val_loss: 0.6471 - val_acc: 0.7638\n",
      "Epoch 3/40\n",
      "4437/4437 [==============================] - 305s 69ms/step - loss: 0.3386 - acc: 0.8425 - val_loss: 0.6215 - val_acc: 0.7682\n",
      "Epoch 4/40\n",
      "4437/4437 [==============================] - 316s 71ms/step - loss: 0.3318 - acc: 0.8576 - val_loss: 0.7351 - val_acc: 0.6871\n",
      "Epoch 5/40\n",
      "4437/4437 [==============================] - 315s 71ms/step - loss: 0.2976 - acc: 0.8787 - val_loss: 0.6246 - val_acc: 0.7651\n",
      "Epoch 6/40\n",
      "1312/4437 [=======>......................] - ETA: 3:13 - loss: 0.2704 - acc: 0.8918"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-8bae3fcf05a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_lstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_lstm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_lstm, np.array(y_train_lstm), validation_split=0.4, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_original_word_tokens_grouped_by_texts = df_texts.text.apply(lambda row: nltk.pos_tag(FEATURE_SELECTOR_v4.tokenize(text=row, **token_params_keras)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(money, NN), (tell, VB), (the, DT), (stori, N...\n",
       "1        [(the, DT), (open, JJ), (sentenc, NNS), (say, ...\n",
       "2        [(dug, VB), (a, DT), (mundan, NN), (is, VBZ), ...\n",
       "3        [(the, DT), (term, NN), (gild, JJ), (age, NN),...\n",
       "4        [(the, DT), (resid, NN), (of, IN), (tiamat, NN...\n",
       "5        [(india, NN), (opal, JJ), (buloni, NN), (is, V...\n",
       "6        [(to, TO), (be, VB), (a, DT), (jedi, NN), (is,...\n",
       "7        [(crippl, NN), (by, IN), (a, DT), (freak, JJ),...\n",
       "8        [(it, PRP), (is, VBZ), (the, DT), (stori, NN),...\n",
       "9        [(from, IN), (the, DT), (hardcov, NN), (jacket...\n",
       "10       [(the, DT), (plot, NN), (of, IN), (permut, NN)...\n",
       "11       [(the, DT), (stori, JJ), (narrat, NN), (by, IN...\n",
       "12       [(use, NN), (altern, JJ), (first, JJ), (person...\n",
       "13       [(the, DT), (stori, JJ), (focus, NN), (on, IN)...\n",
       "14       [(the, DT), (first, JJ), (section, NN), (descr...\n",
       "15       [(versori, FW), (a, DT), (literari, NN), (lect...\n",
       "16       [(astrid, JJ), (magnussen, NN), (is, VBZ), (a,...\n",
       "17       [(in, IN), (1377, CD), (england, VBP), (a, DT)...\n",
       "18       [(after, IN), (have, VBP), (a, DT), (dark, JJ)...\n",
       "19       [(the, DT), (polypontian, JJ), (empir, NN), (h...\n",
       "20       [(sir, NN), (gareth, NN), (is, VBZ), (a, DT), ...\n",
       "21       [(in, IN), (salem, JJ), (villag, NN), (massach...\n",
       "22       [(when, WRB), (the, DT), (galaxi, NN), (s, VBZ...\n",
       "23       [(set, VBN), (in, IN), (the, DT), (year, NN), ...\n",
       "24       [(the, DT), (book, NN), (s, NN), (first, RB), ...\n",
       "25       [(event, NN), (are, VBP), (list, NN), (here, R...\n",
       "26       [(stolen, VBN), (as, IN), (a, DT), (child, NN)...\n",
       "27       [(the, DT), (book, NN), (open, JJ), (with, IN)...\n",
       "28       [(the, DT), (golden, JJ), (globe, NN), (and, C...\n",
       "29       [(extinct, JJ), (take, VB), (the, DT), (form, ...\n",
       "                               ...                        \n",
       "11009    [(the, DT), (demigod, NN), (diari, VBZ), (cont...\n",
       "11010    [(the, DT), (novel, JJ), (begin, NN), (as, IN)...\n",
       "11011    [(inspector, NN), (han, NN), (bärlach, NN), (a...\n",
       "11012    [(who, WP), (kill, VBP), (zebede, VB), (open, ...\n",
       "11013    [(the, DT), (stori, JJ), (begin, NN), (in, IN)...\n",
       "11014    [(the, DT), (novella, NN), (tell, VB), (the, D...\n",
       "11015    [(the, DT), (eponym, JJ), (paradox, NN), (of, ...\n",
       "11016    [(the, DT), (book, NN), (depict, VBZ), (a, DT)...\n",
       "11017    [(the, DT), (gzilt, NN), (a, DT), (civilis, NN...\n",
       "11018    [(fifti, JJ), (shade, NN), (of, IN), (grey, NN...\n",
       "11019    [(the, DT), (novel, NN), (is, VBZ), (split, VB...\n",
       "11020    [(the, DT), (book, NN), (follow, VBP), (sever,...\n",
       "11021    [(the, DT), (novel, NN), (is, VBZ), (set, VBN)...\n",
       "11022    [(the, DT), (book, NN), (is, VBZ), (about, IN)...\n",
       "11023    [(bring, VBG), (up, RP), (the, DT), (bodi, NN)...\n",
       "11024    [(the, DT), (book, NN), (follow, JJ), (nora, N...\n",
       "11025    [(the, DT), (novel, NN), (is, VBZ), (a, DT), (...\n",
       "11026    [(the, DT), (famili, JJ), (focus, NN), (on, IN...\n",
       "11027    [(heaven, RB), (leigh, JJ), (casteel, NN), (is...\n",
       "11028    [(after, IN), (the, DT), (event, NN), (of, IN)...\n",
       "11029    [(a, DT), (novel, NN), (about, IN), (anni, NN)...\n",
       "11030    [(after, IN), (mickey, JJ), (bolitar, NN), (mo...\n",
       "11031    [(stephani, JJ), (harrington, NN), (and, CC), ...\n",
       "11032    [(the, DT), (birth, NN), (of, IN), (plenti, NN...\n",
       "11033    [(set, VBN), (dure, NN), (the, DT), (summer, N...\n",
       "11034    [(class, NN), (wikit, NN), (season, NN), (cove...\n",
       "11035    [(prue, NN), (mckeel, NN), (have, VBP), (rescu...\n",
       "11036    [(the, DT), (reader, NN), (first, JJ), (meet, ...\n",
       "11037    [(colbert, NN), (address, NN), (topic, NN), (i...\n",
       "11038    [(makar, NN), (devushkin, NN), (and, CC), (var...\n",
       "Name: text, Length: 11039, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_original_word_tokens_grouped_by_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_texts=series_original_word_tokens_grouped_by_texts.apply(lambda row: \" \".join([w[1] for w in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NN VB DT NN IN CC VBZ VBN IN NN PRP DT NN NN I...\n",
       "1        DT JJ NNS VBP DT JJ NN NN IN DT NN IN NN CD CD...\n",
       "2        VB DT NN VBZ RB IN TO DT JJ NN IN NN WRB PRP V...\n",
       "3         DT NN JJ NN NN VBN TO DT NN NN IN DT NN IN NN NN\n",
       "4        DT NN IN NN VBP VBN IN CD JJ NN WP VBZ NN NN C...\n",
       "5        NN JJ NN VBZ DT CD NN JJ NN WP VBZ RB VB TO DT...\n",
       "6                        TO VB DT NN VBZ TO VB NN IN DT NN\n",
       "7        NN IN DT JJ NN NN NN NN IN IN DT JJ NNS CC IN ...\n",
       "8        PRP VBZ DT NN IN DT JJ NN NN JJ NN CC PRP$ NN ...\n",
       "9             IN DT NN NN PRP VBD IN IN NN MD VB VBN IN NN\n",
       "10       DT NN IN NN NN VBP DT NN IN NN NN IN DT JJ NN ...\n",
       "11       DT JJ NN IN DT NN CC JJ NN JJ JJ NN JJ JJ NN I...\n",
       "12       NN JJ JJ NN VBP DT NN VBP DT NN IN NN NN JJ CD...\n",
       "13       DT JJ NN IN NN NN VBD DT JJ NN IN JJ JJ NN NN ...\n",
       "14       DT JJ NN VBZ DT NN IN NN IN DT NN NN CC VB NN ...\n",
       "15       FW DT NN NN NN IN JJ JJ NN IN NN TO VB NN IN N...\n",
       "16                JJ NN VBZ DT JJ NN JJ NN VBP IN JJ NN NN\n",
       "17       IN CD VBP DT CD NN JJ NN VBN RB IN JJ JJ NN VB...\n",
       "18                 IN VBP DT JJ NN NN NN JJ NN TO VB NN NN\n",
       "19       DT JJ NN NN NN RB IN DT JJ NN IN JJ NN RB IN T...\n",
       "20       NN NN VBZ DT NN NN CC NN VBN DT NN NN RB VBD J...\n",
       "21       IN JJ NN NN CD DT NN CC NN NN NN NN IN DT NN I...\n",
       "22       WRB DT NN VBZ RBS JJ NN VBZ RB TO VB NN NN TO ...\n",
       "23       VBN IN DT NN CD NN NN NN DT NN WRB NN NN IN DT...\n",
       "24       DT NN NN RB CD NN NN IN DT NN NN CC NN NN IN C...\n",
       "25       NN VBP NN RB RB IN NN NN CC IN DT NN PRP VBD J...\n",
       "26       VBN IN DT NN CC NN IN DT NN IN NN IN NN TO DT ...\n",
       "27       DT NN JJ IN JJ NN NN CC NN VBP NN CC VB JJ NN ...\n",
       "28       DT JJ NN CC NN NNS VBP NN IN DT NNS JJ TO CC V...\n",
       "29       JJ VB DT NN IN DT JJ NN IN JJ NN VBD DT JJ JJ ...\n",
       "                               ...                        \n",
       "11009    DT NN VBZ VBP CD JJ NN IN JJ NN NN IN JJ NN NN...\n",
       "11010    DT JJ NN IN NN IN DT JJ NN NN CC JJ NN NN NN N...\n",
       "11011    NN NN NN IN DT NN IN JJ NN CC NN IN NN VBZ VBN...\n",
       "11012     WP VBP VB JJ IN DT JJ NN TO DT NN IN DT JJ JJ NN\n",
       "11013    DT JJ NN IN DT NN CD NN IN DT NN IN DT NNP NN ...\n",
       "11014    DT NN VB DT NN IN JJ NN NN NN TO DT NN IN DT N...\n",
       "11015    DT JJ NN IN NN IN JJ NN IN JJ NN IN VBZ DT JJ ...\n",
       "11016                         DT NN VBZ DT NN WP VBP JJ NN\n",
       "11017    DT NN DT NN WDT RB VBP DT NN CD CD NN VBD DT N...\n",
       "11018    JJ NN IN NN JJ NN NN NN DT CD NN JJ NN JJ WP V...\n",
       "11019    DT NN VBZ VBN IN CD NN DT JJ NN DT NN IN DT NN...\n",
       "11020    DT NN VBP RB NN IN PRP VBP IN DT JJ NN IN DT N...\n",
       "11021    DT NN VBZ VBN IN DT JJ NN IN NN NN CC NN NN NN...\n",
       "11022    DT NN VBZ IN DT NN NN NN NN CC DT JJ NN WP VBZ...\n",
       "11023                      VBG RP DT NN NN WRB DT NN JJ NN\n",
       "11024    DT NN JJ NNS CC VB DT NN NN CC DT NN WDT VBP V...\n",
       "11025    DT NN VBZ DT NN VBZ DT NN NN NN VBP DT NN IN J...\n",
       "11026    DT JJ NN IN CD NN IN DT NN NN NN NN CC NN CC P...\n",
       "11027    RB JJ NN VBZ DT JJ NN JJ NN VBP IN NN IN PRP$ ...\n",
       "11028    IN DT NN IN NN DT JJ NN IN DT NN VBD JJ NN VB ...\n",
       "11029    DT NN IN NN VBP DT NN IN JJ NN NN NN CC JJ NN ...\n",
       "11030    IN JJ NN NN IN IN NN JJ NN NN VBD JJ NN NN NN ...\n",
       "11031    JJ NN CC PRP$ NN NN NN MD VB VB JJ NN VBP DT N...\n",
       "11032           DT NN IN NN VBZ DT NN IN DT NN NN IN JJ NN\n",
       "11033     VBN NN DT NN IN CD DT NN JJ NN NN IN NN CC JJ NN\n",
       "11034    NN NN NN NN NN VBD CD CD DT NN DT NN NN TO PRP...\n",
       "11035    NN NN VBP VBN PRP$ NN IN DT NN NN IN DT NN IN ...\n",
       "11036    DT NN JJ NN NN IN PRP VBZ VB DT JJ NN IN NN CC...\n",
       "11037    NN NN NN VBP JJ NN NN NN NN NN NN IN DT NN NN ...\n",
       "11038    NN NN CC NN NNS VBP JJ NN RB RB CC JJ NN IN DT...\n",
       "Name: text, Length: 11039, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  3, ...,  0,  0,  0],\n",
       "       [ 3,  4, 15, ...,  0,  0,  0],\n",
       "       [ 5,  3,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 3,  1,  4, ...,  0,  0,  0],\n",
       "       [ 1,  1,  1, ...,  0,  0,  0],\n",
       "       [ 1,  1,  6, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(pos_texts)\n",
    "sequences = tokenizer.texts_to_sequences(pos_texts)\n",
    "pos_data = pad_sequences(sequences, maxlen=800, padding='post')\n",
    "pos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm_pos, X_test_lstm_pos, y_train_lstm_pos, y_test_lstm_pos = train_test_split(pos_data, df_texts.label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "model_pos = Sequential()\n",
    "model_pos.add(Embedding(20000, 200, input_length=800))\n",
    "model_pos.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_pos.add(Dense(1, activation='sigmoid'))\n",
    "model_pos.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5916 samples, validate on 1480 samples\n",
      "Epoch 1/20\n",
      "5916/5916 [==============================] - 401s 68ms/step - loss: 0.6938 - acc: 0.5145 - val_loss: 0.6932 - val_acc: 0.5027\n",
      "Epoch 2/20\n",
      "5916/5916 [==============================] - 389s 66ms/step - loss: 0.6938 - acc: 0.4909 - val_loss: 0.6934 - val_acc: 0.4973\n",
      "Epoch 3/20\n",
      "5916/5916 [==============================] - 389s 66ms/step - loss: 0.6937 - acc: 0.5041 - val_loss: 0.6932 - val_acc: 0.4973\n",
      "Epoch 4/20\n",
      "5916/5916 [==============================] - 388s 66ms/step - loss: 0.6936 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.5027\n",
      "Epoch 5/20\n",
      "5916/5916 [==============================] - 389s 66ms/step - loss: 0.6936 - acc: 0.4912 - val_loss: 0.6931 - val_acc: 0.4973\n",
      "Epoch 6/20\n",
      "5916/5916 [==============================] - 390s 66ms/step - loss: 0.6932 - acc: 0.5056 - val_loss: 0.6933 - val_acc: 0.4973\n",
      "Epoch 7/20\n",
      "5916/5916 [==============================] - 389s 66ms/step - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4973\n",
      "Epoch 8/20\n",
      "5916/5916 [==============================] - 389s 66ms/step - loss: 0.6935 - acc: 0.5017 - val_loss: 0.6934 - val_acc: 0.5027\n",
      "Epoch 9/20\n",
      "5916/5916 [==============================] - 371s 63ms/step - loss: 0.6934 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.5027\n",
      "Epoch 10/20\n",
      "5916/5916 [==============================] - 369s 62ms/step - loss: 0.6931 - acc: 0.5066 - val_loss: 0.6933 - val_acc: 0.5027\n",
      "Epoch 11/20\n",
      "5916/5916 [==============================] - 367s 62ms/step - loss: 0.6934 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.4973\n",
      "Epoch 12/20\n",
      "5916/5916 [==============================] - 362s 61ms/step - loss: 0.6934 - acc: 0.5044 - val_loss: 0.6932 - val_acc: 0.5027\n",
      "Epoch 13/20\n",
      "5916/5916 [==============================] - 363s 61ms/step - loss: 0.6935 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.5027\n",
      "Epoch 14/20\n",
      "5916/5916 [==============================] - 361s 61ms/step - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5027\n",
      "Epoch 15/20\n",
      "5916/5916 [==============================] - 361s 61ms/step - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6938 - val_acc: 0.4973\n",
      "Epoch 16/20\n",
      "5916/5916 [==============================] - 361s 61ms/step - loss: 0.6933 - acc: 0.4953 - val_loss: 0.6932 - val_acc: 0.4973\n",
      "Epoch 17/20\n",
      "5916/5916 [==============================] - 363s 61ms/step - loss: 0.6934 - acc: 0.4968 - val_loss: 0.6932 - val_acc: 0.4973\n",
      "Epoch 18/20\n",
      "5916/5916 [==============================] - 360s 61ms/step - loss: 0.6935 - acc: 0.4887 - val_loss: 0.6931 - val_acc: 0.5027\n",
      "Epoch 19/20\n",
      "5916/5916 [==============================] - 361s 61ms/step - loss: 0.6933 - acc: 0.5007 - val_loss: 0.6933 - val_acc: 0.5027\n",
      "Epoch 20/20\n",
      "5916/5916 [==============================] - 362s 61ms/step - loss: 0.6934 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x250ac170ac8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pos.fit(X_train_lstm_pos, np.array(y_train_lstm_pos), validation_split=0.2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
